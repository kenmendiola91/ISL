#LyX 1.6.4.1 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble
\rhead{ISL}
\chead{}
\lhead{Knowledge Vault}


\usepackage{fancyvrb}
\end_preamble
\use_default_options true
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 1.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip 1sp
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title

\series bold
Introduction to Statistical Learning
\end_layout

\begin_layout Author
Robert Chang
\end_layout

\begin_layout Abstract
This document aims to summarize all the insights & new discoveries that
 I otherwise would not have found on my own, throughtout the course of reading
 ISL and taking the Stanford online course along with Professor Trevor Hastie
 and Professor Robert Tibshirani (Our own professors from Stanford Statistics!)
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
In the past six months or so (From September 2013 to March 2014), I have
 been quite dedicated, over the weekends, to push myself to learn new materials
 in order to deepen my technical knowledge in the area of statistics and
 computer science.
\begin_inset Foot
status open

\begin_layout Plain Layout
Coursera | Data Analysis | Jeff Leek, Coursera | Python: Learn to program,
 Machine Learning for Hackers, Coursera | Scala, Learning from Data | Caltech
 | Edx, and more ...
 etc
\end_layout

\end_inset

 During this time, I have completed quite a few Coursera and Edx courses.
 As satisfying as they were, there are occasions where I feel I did not
 quite internalize the learnings, and as a result, applying them to my daily
 work, despite the various efforts I put in during the time of learning.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
I raised this issue to myself in the 2013 Yearly review, but I never really
 take measures to iterate and experiment on combating this shortfall.
 In the midst of transition to the Coursera Data Science Specialization
 Track that will start on 4/7/2014, I decided to use this two week gap to
 experiment new ways of 'securing my learning results'.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
The most obvious and immediate experiment I could try, is to load my knowledge
 into the knowledge vault.
 The purpose is to systematically document the insights, so if I were ever
 to come back and re-study the materials, I would have a much easier time
 to refresh and catch up.
 I have done this successfully for Linear Algebra (using materials from
 Gilbert String's MIT Linear Algebra + Stanford Math 104 course notes).
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
To summarize, the challenge here is not to write a knowledge vault document,
 but to assess whether a knowledge vault document is an efficient way for
 upholding the things that I have learned!
\begin_inset Foot
status open

\begin_layout Plain Layout
Therefore, don't rush yourself to finish all the chapter summaries, because
 that defeats the purpose of knowledge vault documentation.
\end_layout

\end_inset


\end_layout

\begin_layout Section*
The Course
\end_layout

\begin_layout Standard
The course 
\color blue

\begin_inset CommandInset href
LatexCommand href
name "Introduction to Statistical Learning with R"
target "https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/info"

\end_inset


\color inherit
 (ISLR) is offered by Stanford online, using the Stanford Edx platform.
 This course attracts me immediately! First of all, it is taught by the
 two authors who wrote ELS, and I have personally taken classes from them.
 They are truly the experts.
 Second, the 
\color blue

\begin_inset CommandInset href
LatexCommand href
name "Textbook ISLR"
target "http://www-bcf.usc.edu/~gareth/ISL/"

\end_inset


\color inherit
 is superb, it hides away the mathematical details, and gives a very good
 introduction to all the major topics.
 It also has done a great job in talking about the trade-offs between models
 (under what conditions should you use what), and there are labs in the
 end of each chapter for readers to practice.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
There are toal of 10 chapters, and my studying format was the following:
\end_layout

\begin_layout Itemize
In each week, go through the segment videos, including the R labs
\end_layout

\begin_layout Itemize
Immediately after each segment, summarize the learnings in word, and put
 them under Google Calendar
\end_layout

\begin_layout Itemize
Go through the R labs, and try to follow each of the steps as much as I
 can
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard

\color black
Not a bad approach.
 But the reality is, since this is an introductory course, I am already
 somewhat familiar with the major concepts.
 As a result, the deliberate practice and its gain is not going to be learning
 this new algorithm, but much deeper.
 The real gain is going to be where I internalize:
\end_layout

\begin_layout Itemize

\color blue
New ways of looking at and understanding the stories of algorithms that
 I already know
\end_layout

\begin_layout Itemize

\color blue
Learning new models (Generalized Additive Models, LDA, QDA ...
 etc)
\end_layout

\begin_layout Itemize

\color blue
For a given situation, which model is going to be more appropriate and why?
\end_layout

\begin_layout Itemize

\color blue
What are the practical concerns when using the model? Tuning? Scaling? ...
 etc
\end_layout

\begin_layout Itemize

\color blue
Other new insights
\end_layout

\begin_layout Standard

\color blue
These are new insights, so they need to be pointed out OVER and OVER again
 before I 
\series bold
internalize
\series default
 them, before they become something I can cite without effort, and use them
 in practice.
\end_layout

\begin_layout Section*
The Contents
\begin_inset Foot
status open

\begin_layout Plain Layout
See slide 24/29 of Introduction PDF for 
\begin_inset Quotes eld
\end_inset

Philosophy
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
There are 10 chapters, which covers the topics below:
\end_layout

\begin_layout Enumerate
Introduction
\end_layout

\begin_layout Enumerate
Statistical Learning
\end_layout

\begin_layout Enumerate
Linear regression
\end_layout

\begin_layout Enumerate
Classification
\end_layout

\begin_layout Enumerate
Resampling Methods
\end_layout

\begin_layout Enumerate
Linear Model Selection and Regularization
\end_layout

\begin_layout Enumerate
Moving Beyond Linearity
\end_layout

\begin_layout Enumerate
Tree-Based Methods
\end_layout

\begin_layout Enumerate
Support Vector Machine
\end_layout

\begin_layout Enumerate
Unsupervised Learning
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
ISLR is based on the following four premises (more details can be found
 in textbook page 8):
\end_layout

\begin_layout Itemize
Many statistical learning methods are relevant and useful in a wide range
 of academic and non-academic disciplines, beyond just the statistical sciences
 -- 
\color blue
Learn new methods that are widely applicable, internalized the story on
 models you already know
\end_layout

\begin_layout Itemize
Statistical learning should not be viewed as a series of black boxes: 
\begin_inset Quotes eld
\end_inset

Without understanding all of the cogs inside the box, or the interaction
 between those cogs, it is impossible to select the best box.
 Hence, we have attempted to carefully describe the model, intuition, assumption
s, and trade-offs behind each of the methods that we consider
\begin_inset Quotes erd
\end_inset

 -- 
\color blue
Know the trade-offs between different models in different situation
\end_layout

\begin_layout Itemize
While is is important know what job is performed by each cog, it is not
 necessary to have the skills to construct the machine inside the box! --
 
\color blue
You don't need to know the mathematical details, but know the main story
 each algorithm presents
\end_layout

\begin_layout Itemize
We presume that the reader is interested in applying statistical learning
 methods to real-world problems -- 
\color blue
Learn the in practical know-hows
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Chapter 2: Statistical Learning
\end_layout

\begin_layout Standard
Typically, we have some response variable 
\begin_inset Formula $Y$
\end_inset

 and some predictors 
\begin_inset Formula $X's$
\end_inset

, our goal is to:
\end_layout

\begin_layout Itemize
Make prediction of 
\begin_inset Formula $Y$
\end_inset

 at a new point 
\begin_inset Formula $X=x$
\end_inset


\end_layout

\begin_layout Itemize
We can understand which components of 
\begin_inset Formula $X=(X_{1},X_{2},\dots,X_{p})$
\end_inset

 are important in explaining 
\begin_inset Formula $Y$
\end_inset

, and which are irrelevant
\end_layout

\begin_deeper
\begin_layout Itemize
e.g.
 Do statistical inference on the 
\begin_inset Formula $\beta$
\end_inset

 coefficients of a linear model to test if the null hypothesis 
\begin_inset Formula $\beta=0$
\end_inset

 is correct 
\end_layout

\end_deeper
\begin_layout Itemize
Depending on the complexity of 
\begin_inset Formula $f$
\end_inset

, we may be able to understand how each component 
\begin_inset Formula $X_{j}$
\end_inset

 of 
\begin_inset Formula $X$
\end_inset

 affects 
\begin_inset Formula $Y$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
e.g.
 Look at the magnitude of the coefficients: for a one unit increase in 
\begin_inset Formula $X_{j}$
\end_inset

, how does 
\begin_inset Formula $Y$
\end_inset

 change?
\end_layout

\end_deeper
\begin_layout Standard
Building a model 
\begin_inset Formula $f(X)$
\end_inset

 to study 
\begin_inset Formula $Y$
\end_inset

 would help us answer the above questions!
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Subsection*
Is there an ideal 
\begin_inset Formula $f(X)$
\end_inset

?
\end_layout

\begin_layout Standard
A good choice is to pick 
\begin_inset Formula $f(x)=E(Y|X=x)$
\end_inset

, which is called the 
\series bold
regression function
\series default
.
 It has the nice property that it is the function 
\begin_inset Formula $g$
\end_inset

 such that
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\min_{g}E[(Y-g(X))^{2}|X=x]\]

\end_inset


\end_layout

\begin_layout Standard
In other word, it is the function that minimize the square error against
 the response variable 
\begin_inset Formula $Y$
\end_inset

.
 
\end_layout

\begin_layout Subsubsection*
Prediction
\end_layout

\begin_layout Standard
In the context of prediction, the reality is, 
\end_layout

\begin_layout Itemize
we do not know 
\begin_inset Formula $f(x)=E[Y|X=x]$
\end_inset

, so we have use 
\begin_inset Formula $\hat{f}(x)$
\end_inset

 to estimate 
\begin_inset Formula $f(x)$
\end_inset


\end_layout

\begin_layout Itemize
Even if we can do a perfect job in estimating 
\begin_inset Formula $f(x)$
\end_inset

 using 
\begin_inset Formula $\hat{f}(x)$
\end_inset

, we still cannot perfectly predict 
\begin_inset Formula $Y$
\end_inset

, since 
\begin_inset Formula $Y=f(x)+\epsilon$
\end_inset

, i.e.
 at each 
\begin_inset Formula $X=x$
\end_inset

 there is typically a distribution of possible 
\begin_inset Formula $Y$
\end_inset

 values.
 The 
\begin_inset Formula $\epsilon$
\end_inset

 is called the irreducible error
\end_layout

\begin_layout Standard
In fact, looking at 
\begin_inset Formula $E[(Y-\hat{f}(X))^{2}|X=x]=E[(Y-\hat{Y})^{2}|X=x]$
\end_inset

 (not 
\begin_inset Formula $E[(Y-f(X))^{2}|X=x]$
\end_inset

), we have:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
E\left[\left(Y-\hat{f}\left(X\right)\right)^{2}\vert X=x\right]=\underbrace{{\left[f(x)-\hat{f}\left(x\right)\right]^{2}}}_{\text{reducible}}+\underbrace{Var(\epsilon)}_{Irreducible}\]

\end_inset


\end_layout

\begin_layout Standard
Fitting a better model or finding a better 
\begin_inset Formula $\hat{f}(x)$
\end_inset

 that approximate 
\begin_inset Formula $f(x)$
\end_inset

 would reduce the 
\series bold
reducible
\series default
 part of the error, but the 
\series bold
irreducible
\series default
 part of the error depends on the natural variance of 
\begin_inset Formula $\epsilon$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
Inference
\end_layout

\begin_layout Standard
We are often interested in understanding the way that 
\begin_inset Formula $Y$
\end_inset

 is affected as 
\begin_inset Formula $X_{1},\dots X_{p}$
\end_inset

 changes.
 In this situation we wish to estimate 
\begin_inset Formula $f$
\end_inset

, but our goal is not necessarily to make predictions for 
\begin_inset Formula $Y$
\end_inset

.
 We instead want to understand the relationship between 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

.
 Here are some of the typical questions one would ask:
\end_layout

\begin_layout Itemize

\shape italic
Which predictors are associated with the response?
\shape default
 It is often the case that only a small fraction of available predictors
 are associated with 
\begin_inset Formula $Y$
\end_inset

, identifying the few important predictors among a large set of possible
 variables can be very useful.
\end_layout

\begin_layout Itemize

\shape italic
What is the relationship between the response and each predictor? 
\shape default
How does 
\begin_inset Formula $Y$
\end_inset

 changes as 
\begin_inset Formula $X$
\end_inset

 change
\end_layout

\begin_layout Itemize

\shape italic
Can the relationship between 
\begin_inset Formula $Y$
\end_inset

 and each predictor be adequately summarized using a linear equation, or
 is the relationship more complicated? 
\end_layout

\begin_layout Subsubsection*
Prediction or Inference?
\end_layout

\begin_layout Standard
This really depends, many times we only care about prediction, there are
 times where we only care about inference, but sometimes we care a combination
 of two.
 This really comes with experience (Read the Advertising example that illustrate
 the point in textbook starting page 20).
\end_layout

\begin_layout Subsubsection*
Trade-offs
\end_layout

\begin_layout Itemize

\series bold
Prediction accuracy V.S.
 Interpretability
\end_layout

\begin_deeper
\begin_layout Standard
Generally, linear models will give us better interpretability, but it might
 not be complex enough to do a good job in prediction.
 Other times, we might have a very complex model that does a great job in
 prediction, but is very difficult to interpret.
 
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Good fit V.S.
 Over-fit V.S.
 Under-fit
\end_layout

\begin_deeper
\begin_layout Itemize
How dowe know when the fit is just right? Test error & using the method
 of Cross Validation 
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Parsimony V.S.
 Black-Box
\end_layout

\begin_deeper
\begin_layout Itemize
We often prefer a simpler model involving fewer variables over a black-box
 predictor involving them all (Ocam's razor principle)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/flexibility_interpretability.png
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Assessing Model Accuracy
\end_layout

\begin_layout Standard
Suppose we fit a model 
\begin_inset Formula $\hat{f}(x)$
\end_inset

 to some training data 
\begin_inset Formula $Tr=\left\{ x_{i},y_{i}\right\} _{1}^{N}$
\end_inset

, and we wish to see how well it performs
\end_layout

\begin_layout Itemize
We could compute the average squared prediction error over 
\series bold
Tr:
\begin_inset Formula \[
MSE_{Tr}=Ave_{i\in Tr}\left[y_{i}-\hat{f}(x_{i})\right]^{2}\]

\end_inset


\end_layout

\begin_layout Itemize
But training error is biased toward overfit models.
 Instead, we should assess the model using test data 
\begin_inset Formula $TE=\left\{ x_{i},y_{i}\right\} _{1}^{N}$
\end_inset

 
\begin_inset Formula \[
MSE_{TE}=Ave_{i\in TE}\left[y_{i}-\hat{f}(x_{i})\right]^{2}\]

\end_inset


\end_layout

\begin_layout Standard
Check out the typical scenarios in 
\color blue

\begin_inset CommandInset href
LatexCommand href
name "Figure 2.9 - 2.11"
target "https://class.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/statistical_learning.pdf"

\end_inset

:
\end_layout

\begin_layout Itemize
In Figure 2.9, the true 
\begin_inset Formula $f$
\end_inset

 is somwhat nonlinear
\end_layout

\begin_deeper
\begin_layout Itemize
A linear model is too simple, and have high training & test error (underfitting)
\end_layout

\begin_layout Itemize
A highly nonlinear model is too complex, so it has very low training error,
 but very high test error (overfitting)
\end_layout

\begin_layout Itemize
Another one that is just right, probably determined by CV.
\end_layout

\end_deeper
\begin_layout Itemize
In Figure 2.10, the true 
\begin_inset Formula $f$
\end_inset

 is linear
\end_layout

\begin_deeper
\begin_layout Itemize
A linear model is quite appropriate, so it has low training and low test
 error
\end_layout

\begin_layout Itemize
A nonlinear model is too complex, so it has low training error, but high
 test error (overfitting)
\end_layout

\begin_layout Itemize
Anothre one that is just right, notice it is close to the linear model
\end_layout

\end_deeper
\begin_layout Itemize
In Figure 2.11,the true 
\begin_inset Formula $f$
\end_inset

 is highly nonlinear
\end_layout

\begin_deeper
\begin_layout Itemize
A linear model is too simple, so it has high training and test error (underfitti
ng)
\end_layout

\begin_layout Itemize
A nonlinear model is appropriate, so it has low training and test error
 
\end_layout

\begin_layout Itemize
Another one that is just right, notice it is closer to the complex model.
\end_layout

\end_deeper
\begin_layout Subsubsection*
Bias and Variance Trade-off
\end_layout

\begin_layout Standard
Suppose we have fit a model 
\begin_inset Formula $\hat{f}(x)$
\end_inset

 to some training data 
\series bold
Tr, 
\series default
and let 
\begin_inset Formula $(x_{0},y_{0})$
\end_inset

 be a test observation drawn from the population.
 If the true model is 
\begin_inset Formula $Y=f(X)+\epsilon$
\end_inset

 with 
\begin_inset Formula $f(X)=E(Y|X=x)$
\end_inset

, then we have
\end_layout

\begin_layout Standard
\begin_inset Formula \[
E\left(y_{0}-\hat{f}(x_{0})\right)^{2}=Var(\hat{f}(x_{0}))+[Bias(\hat{f}(x_{0}))]^{2}+Var(\epsilon)\]

\end_inset


\end_layout

\begin_layout Standard
The expectation averages over the variability of 
\begin_inset Formula $y_{0}$
\end_inset

 as well as the variability of
\series bold
 
\begin_inset Formula $\mathbf{Tr}=\left\{ x_{i},y_{i}\right\} _{1}^{N}$
\end_inset

.
 
\series default
Here, the notation 
\begin_inset Formula $E\left(y_{0}-\hat{f}(x_{0})\right)^{2}$
\end_inset

defines the 
\shape italic
\color black
expected test MSE, 
\shape default
\color inherit
and refers to the average test MSE that we would obtain if we repeatedly
 estimated 
\begin_inset Formula $f$
\end_inset

 using a large number of different training sets, and tested each at 
\begin_inset Formula $x_{0}$
\end_inset

.
 The overall expected test MSE can be computed by averaging 
\begin_inset Formula $E\left(y_{0}-\hat{f}(x_{0})\right)^{2}$
\end_inset

 over all possible values of 
\begin_inset Formula $x_{0}$
\end_inset

 in the test set.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
What do we mean by the variance and bias of a statistical learning method?
\end_layout

\begin_layout Itemize

\series bold
Source of Variance
\series default
: It refers to the amount by which 
\begin_inset Formula $\hat{f}$
\end_inset

 would change if we estimated it using a different training data set.
 Since the training data are used to fit the statistical learning method,
 different training data sets will result in a different 
\begin_inset Formula $\hat{f}$
\end_inset

.
 Ideally, the estimate of 
\begin_inset Formula $f$
\end_inset

 should not vary too much between training set replica.
 However, if a method has high variance then small changes in teh training
 data can result in large changes in 
\begin_inset Formula $\hat{f}$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Source of Bias
\series default
: It refers to the error that is introduced by approximating a real-life
 problem, wihch may be extremely complicated, by a much simpler model.
 
\end_layout

\begin_layout Standard
Typically, as the flexibility of 
\begin_inset Formula $\hat{f}$
\end_inset

 increases, the bias decreases, but the variance increases.
 The relative rate of change of these two quantities determines whether
 the test MSE increases or decreases.
 Choosing the flexibility based on average test error amounts to a bias-variance
 trade-off.
 Bias-variance trade-off for the three examples above are presented here:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/bias_variance_tradeoff_decomp.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Classification Problems
\end_layout

\begin_layout Standard
Thus far, our discussion of model accuracy has been focused on the regression
 setting.
 But many of the concepts that we have encountered, such as bias-variance
 trade-off, transfer over to the classification setting with only some modificat
ions due to the fact that 
\begin_inset Formula $y_{i}$
\end_inset

 is no longer numerical.
 Here, the response variable 
\begin_inset Formula $Y$
\end_inset

 is qualitative, but the goal is the same:
\end_layout

\begin_layout Itemize
Build a classifer 
\begin_inset Formula $C(X)$
\end_inset

 that assigns a class label from 
\begin_inset Formula $\mathcal{C}$
\end_inset

 to a future unlabeled observation 
\begin_inset Formula $X$
\end_inset


\end_layout

\begin_layout Itemize
Assess the uncertainty in each classification
\end_layout

\begin_layout Itemize
Understand the roles of the different predictors among 
\begin_inset Formula $X=(X_{1}\dots X_{p})$
\end_inset


\end_layout

\begin_layout Subsubsection*
Assessing Model accuracy
\end_layout

\begin_layout Standard
The most natural metric to assess model accuracy is the mis-classification
 error rate, which amounts to
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
MSE_{Tr} & = & Ave_{i\in Tr}I\left(y_{0}\neq\hat{y_{0}}\right)\\
MSE_{Te} & = & Ave_{i\in Te}I\left(y_{0}\neq\hat{y_{0}}\right)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is simply the proportion of unseen observation where the predicted
 label does not equal to the true labels.
\end_layout

\begin_layout Subsubsection*
The Bayes Classifier
\end_layout

\begin_layout Standard
It can be shown that the test error rate given above is minimized, on average,
 by a very simple classifier that assigns each observation to the most likely
 class, given its predictor values.
 In other word the Bayes optimal classfier at 
\begin_inset Formula $x$
\end_inset

 is 
\end_layout

\begin_layout Standard
\begin_inset Formula \[
C(x)=j\:\text{if}\: p_{j}(x)=max\left\{ p_{1}(x)\dots p_{K}(x)\right\} \]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $p_{j}(x)=P(Y=j|X=x)$
\end_inset

 is the 
\series bold
true
\series default
 conditional probability that 
\begin_inset Formula $Y=j$
\end_inset

, given the value of 
\begin_inset Formula $x$
\end_inset

.
 Of course, in reality, we do not know the true conditional probabilities,
 but this bayes optimal classifier is great when we need to establish the
 lower bound/baseline for testing models on a set of simulated data, and
 compare different model performance with the baseline.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
The bayes error rate at 
\begin_inset Formula $X=x_{0}$
\end_inset

 will be 
\begin_inset Formula $1-max_{j}P(Y=j|X=x_{0})$
\end_inset

.
 In general, the overall Bayes error rate is given by
\end_layout

\begin_layout Standard
\begin_inset Formula \[
1-E\left(max_{j}P(Y=j|X)\right)\]

\end_inset


\end_layout

\begin_layout Standard
where the expectation averages the probability over all possible values
 of 
\begin_inset Formula $X$
\end_inset

.
 The Bayes error rate is analogous to the irreducible error, discussed earlier.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
In the classification setting, SVM builds structured models for 
\begin_inset Formula $C(x)$
\end_inset

, but logistic regression & generalized additive model builds structured
 model to represent 
\begin_inset Formula $p_{k}(x)$
\end_inset

 directly.
\end_layout

\begin_layout Subsection*
Side Topic: 
\begin_inset Formula $K$
\end_inset

- nearest neighbors
\end_layout

\begin_layout Standard
\begin_inset Formula $K-$
\end_inset

nearest neighbors method was used in this chapter to demonstrate the bias-varian
ce trade-offs.
 But it was also used to demonstrated modeling in high dimensional space.
 Namely, the curse of dimensionality, where observation tend to sit at the
 boundaries of the space, and all the observations are quite far apart from
 each other.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Rule of Thumb:
\end_layout

\begin_layout Itemize
Nearest neighbor averaging can be pretty good for small 
\begin_inset Formula $p$
\end_inset

.
 i.e.
 
\begin_inset Formula $p\leq4$
\end_inset

 and large-ish 
\begin_inset Formula $N$
\end_inset


\end_layout

\begin_layout Itemize
It can be very lousy when 
\begin_inset Formula $p$
\end_inset

 is large due to curse of dimensionality
\end_layout

\begin_deeper
\begin_layout Itemize
We need to get a reasonable fraction, around 
\begin_inset Formula $10\%$
\end_inset

 of 
\begin_inset Formula $N$
\end_inset

 values of 
\begin_inset Formula $y_{i}$
\end_inset

 to average to bring down the variance
\end_layout

\begin_layout Itemize
A 
\begin_inset Formula $10\%$
\end_inset

 neighborhood in high dimensions need no longer be local, so we lose the
 spirit of local averaging or majority vote.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Lingering Questions
\end_layout

\begin_layout Standard
In this chapter, we introduced:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
E\left[\left(Y-\hat{f}\left(X\right)\right)^{2}\vert X=x\right] & = & \underbrace{{\left[f(x)-\hat{f}\left(x\right)\right]^{2}}}_{\text{reducible}}+\underbrace{Var(\epsilon)}_{Irreducible}\\
E\left(y_{0}-\hat{f}(x_{0})\right)^{2} & = & Var(\hat{f}(x_{0}))+[Bias(\hat{f}(x_{0}))]^{2}+Var(\epsilon)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
What are the difference of the two?
\end_layout

\begin_layout Itemize
I think for the first quantity, the expectation is over 
\begin_inset Formula $(Y,X)$
\end_inset

, and only after then make a prediction on 
\begin_inset Formula $x$
\end_inset

.
 This means that we have all the data from the population to build 
\begin_inset Formula $\hat{f}$
\end_inset

, so we are not limited by just a snapshot of the training set.
 Also, when assessing the error, we also have the whole 
\begin_inset Formula $(Y,X)$
\end_inset

 population at our disposal.
 
\end_layout

\begin_layout Itemize
In the second setting, we do not have the whole 
\begin_inset Formula $(Y,X)$
\end_inset

 population at our disposal.
 At any given time, we only have a snapshot of it, meaning, the training
 set, 
\begin_inset Formula $\mathbf{Tr}=\left\{ x_{i},y_{i}\right\} _{1}^{N}$
\end_inset

, and we are using that training set to build 
\begin_inset Formula $\hat{f}$
\end_inset

.
 This means that for different training set, we will have different 
\begin_inset Formula $\hat{f}$
\end_inset

, and that's where the 
\begin_inset Formula $Var(\hat{f}$
\end_inset

) comes in.
 
\end_layout

\begin_layout Standard
I think this highlights the difference between 
\series bold
fitting
\series default
 and 
\series bold
learning
\series default
 (i.e.
 
\series bold
making predictions
\series default
), as mentioned in Learning from Data | Caltech Course! In fitting, we used
 up all the data that is ever available, but in learning, we can only learned
 from the limited training set we have at hand, and that's the key that
 resulted in the tension between variance & Bias.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Is my interpretation right?
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Chapter 3: Linear Regression
\end_layout

\begin_layout Standard
Linear regression is the cornerstone of many of the more complex learning
 algorithms we will discussed later, so it's important to study in its own
 right.
 The set of questions I want to focus on are:
\end_layout

\begin_layout Itemize
What are the standard set of questions that we will be able to address using
 Linear Regression?
\end_layout

\begin_layout Itemize
What are the woes of (interpreting) linear regressions
\end_layout

\begin_layout Itemize
Model assumption -- When is it (not) appropriate to use LR?
\end_layout

\begin_layout Itemize
Some additional modeling techniques like qualitative variables, interactions,
 higher order/non-linear terms
\end_layout

\begin_layout Subsection*
What are the standard set of questions that LR can address?
\end_layout

\begin_layout Standard
Very often, when we have a supervised learning problem involving prediction
 and interpretation, we are likely to ask the following questions:
\end_layout

\begin_layout Itemize
Is there a relationship between 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

? (F-statistics can be our friend here: Is there at least one predictors
 that is related to 
\begin_inset Formula $Y$
\end_inset

.
 T-statistics can be used here: Does each of the 
\begin_inset Formula $X$
\end_inset

 relates to 
\begin_inset Formula $Y$
\end_inset

)
\end_layout

\begin_layout Itemize
How strong is the relationship between 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

? (
\begin_inset Formula $R^{2}$
\end_inset

 can be our friend here)
\end_layout

\begin_layout Itemize
Which 
\begin_inset Formula $X's$
\end_inset

 contribute to 
\begin_inset Formula $Y$
\end_inset

? (Subset selection can be helpful here, what are the main effects?)
\end_layout

\begin_layout Itemize
How accurately an we estimate the effect of each 
\begin_inset Formula $X$
\end_inset

 on 
\begin_inset Formula $Y$
\end_inset

? (The 
\begin_inset Formula $\beta's$
\end_inset

 coefficients of the linear model can be helpful here)
\end_layout

\begin_layout Itemize
How accurately can we predict future sales? (MSE on test set)
\end_layout

\begin_layout Itemize
Is the relationship linear? (Modeling checking, ANOVA)
\end_layout

\begin_layout Itemize
Is there synergy among the 
\begin_inset Formula $X$
\end_inset

's? (Adding interaction terms, in addition to main effect?)
\end_layout

\begin_layout Subsubsection*
Is there a relationship between (all) 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

? 
\begin_inset Formula $(F-test)$
\end_inset


\end_layout

\begin_layout Standard
In simple linear regression, since there is only 1 predictor, checking whether
 there is a relationship between 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 amounts to just checking 
\begin_inset Formula $H_{0}:\:\beta_{1}=0$
\end_inset

.
 In the context of multiple linear regression, we need to check:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
H_{0} & : & \beta_{1}=\beta_{2}=\dots=\beta_{p}=0\\
H_{A} & : & \text{at least one of the }\beta's\:\text{is non-zero}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The classicial test is the 
\begin_inset Formula $F$
\end_inset

-test, available in 
\begin_inset Formula $R$
\end_inset

.
 The 
\begin_inset Formula $F$
\end_inset

-statistics is in the form 
\begin_inset Formula $F=\frac{(TSS-RSS)/p}{RSS/(n-p-1)}$
\end_inset

.
 It can be shown that 
\begin_inset Formula $E[RSS/(n-p-1)]=\sigma^{2}$
\end_inset

, and under 
\begin_inset Formula $H_{0}$
\end_inset

, 
\begin_inset Formula $E[(TSS-RSS)/p]=\sigma^{2}$
\end_inset

.
 So when the null is true, 
\begin_inset Formula $F$
\end_inset

-statistics would be close to 1, otherwise, it would be much larger (intuition
 is that the predictors are good, so 
\begin_inset Formula $RSS$
\end_inset

 would be small, 
\begin_inset Formula $TSS-RSS$
\end_inset

 would be big, so 
\begin_inset Formula $F$
\end_inset

-statistic would be large).
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Subsubsection*
Is there a relationship between each of the 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

? 
\begin_inset Formula $(t-test)$
\end_inset


\end_layout

\begin_layout Standard
Sometimes we would want to test if a particular subset of 
\begin_inset Formula $q$
\end_inset

 coefficients are 0s, this correspond to 
\begin_inset Formula $H_{0}:\:\beta_{p-q+1}=\dots=\beta_{p}=0$
\end_inset

, and yet again we use 
\begin_inset Formula $F=\frac{(RSS_{0}-RSS)/p}{RSS/(n-p-1)}$
\end_inset

 where 
\begin_inset Formula $RSS_{0}$
\end_inset

 is the fitted model with all the predictors except those 
\begin_inset Formula $q$
\end_inset

 predictors.
 The idea is the same, instead of comparing the full fit model's performance
 (RSS) with the performance of the full null model (TSS), we replace TSS
 with the appropriate baseline 
\begin_inset Formula $RSS_{0}$
\end_inset

.
 When we run 
\begin_inset Formula $t$
\end_inset

-test on each of the individual predictors, we are impact running this 
\begin_inset Formula $F$
\end_inset

-test with 
\begin_inset Formula $RSS_{0}=$
\end_inset

 the fit of the model with that one single predictor removed 
\begin_inset Formula $(q=1)$
\end_inset

.
 In other words, the 
\begin_inset Formula $t$
\end_inset

-test in multiple regression is measuring the partial effect of each of
 the variables, while holding all else constant.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
In addition to the 
\begin_inset Formula $F$
\end_inset

- statistics view, we can also view the inference as a 
\begin_inset Formula $t$
\end_inset

- test directly.
 Since we assume a underlying linear statistical model in the context of
 linear regression, assuming that we have different replica of the training
 set, we will be able to produce different linear regression lines with
 each training set.
 This means that we can view the coefficients 
\begin_inset Formula $\beta's$
\end_inset

 as random variable, and we can do inference on them.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
By inference, I mean we can get estimated values of 
\begin_inset Formula $\beta$
\end_inset

's using 
\begin_inset Formula $\hat{\beta}$
\end_inset

's.
 We can also get their corresponding 
\begin_inset Formula $SE(\hat{\beta})$
\end_inset

, and get 
\begin_inset Formula $t-$
\end_inset

statistics to do hypothesis testing and compute confidence interval.
 The whole game is so we can decide on the following two competing hypotheses:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
H_{0} & : & \text{There is no relationship between \ensuremath{X\:}and \ensuremath{Y}}\\
H_{1} & : & \text{There is some relationship between \ensuremath{X\:}and \ensuremath{Y}}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection*
How accurately an we estimate the effect of each 
\begin_inset Formula $X$
\end_inset

 on 
\begin_inset Formula $Y$
\end_inset

?
\end_layout

\begin_layout Standard
The best way is to look at the regression coefficients.
 The ideal scenario of running a multiple regression is when the predictors
 are uncorrelated -- a balance design.
 This would allow us to interpret the coefficients as 
\begin_inset Quotes eld
\end_inset

a unit change in 
\begin_inset Formula $X_{j}$
\end_inset

 is associated with a 
\begin_inset Formula $\beta_{j}$
\end_inset

 unit change in 
\begin_inset Formula $Y$
\end_inset

, while all the other variable stay fixed
\begin_inset Quotes erd
\end_inset

.
 Unfortunately, when variables are correlated, we can not make that statement.
 Correlations amonst predictor cause problems:
\end_layout

\begin_layout Itemize
The variance of all coefficients tends to be inflated, sometimes dramatically.
\end_layout

\begin_layout Itemize
Interpretation becomes hazardous -- when 
\begin_inset Formula $X_{j}$
\end_inset

 changes, everything else changes.
\end_layout

\begin_layout Itemize
Claims of causality should be avoided for observational data.
\end_layout

\begin_layout Subsubsection*
Which 
\begin_inset Formula $X$
\end_inset

's contribute to 
\begin_inset Formula $Y$
\end_inset

? (subset selection)
\end_layout

\begin_layout Standard
Usually, the first step in multiple regression is to compute the 
\begin_inset Formula $F$
\end_inset

-statistic to see if at least one of the predictors are related to 
\begin_inset Formula $Y$
\end_inset

.
 If yes, it is only then natural to ask which are the guilty ones! The popular
 methods are all subset selection, forward selection, and backward selection:
\end_layout

\begin_layout Itemize

\series bold
All subset selection
\series default
: Consider all 
\begin_inset Formula $2^{p}$
\end_inset

 model, this is not very feasible when 
\begin_inset Formula $p$
\end_inset

 is large
\end_layout

\begin_layout Itemize

\series bold
Forward selection
\series default
: Begin with the null model - fit 
\begin_inset Formula $p$
\end_inset

 simple linear regression and add to the null model the variable that result
 in the lowest RSS, we then add to that model the variable that result in
 the lowest RSS for the new two-variable model.
 Repeat the process until a stopping criteron is satisfied.
\end_layout

\begin_layout Itemize

\series bold
Backward selection
\series default
: Start with all variables in the model, and remove the variable with the
 largest 
\begin_inset Formula $p$
\end_inset

-value.
 The new 
\begin_inset Formula $(p-1)$
\end_inset

 variable is fit, and the predictor with the largest 
\begin_inset Formula $p$
\end_inset

-value is again removed.
 This continues a stopping criterion is met.
\end_layout

\begin_layout Itemize

\series bold
Mixed selection
\series default
: see textbook 79.
\end_layout

\begin_layout Standard
Backward selection cannot be used if 
\begin_inset Formula $p>n$
\end_inset

, forward selection can always be used.
 Forward selection is a greedy approach, so it might include variables early
 that later become redundant.
 Mixed strategy can remedy this.
 In Chapter 6, we will discuss more systematic criteria for choosing an
 
\begin_inset Quotes eld
\end_inset

optimal
\begin_inset Quotes erd
\end_inset

 member in the path of the models produced by forward or backward stepwise
 selection.
 This include mallow 
\begin_inset Formula $C_{p},\: AIC,\: BIC,\:$
\end_inset

 adjusted 
\begin_inset Formula $R^{2}$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
Given that there is a relationship between 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

, how strong is the relationship? (
\begin_inset Formula $R^{2}$
\end_inset

)
\end_layout

\begin_layout Standard
It talks about 
\begin_inset Formula $R^{2}$
\end_inset

 and 
\begin_inset Formula $RSE$
\end_inset

 (root-mean-square) as the basis here.
 The basic idea is to compare different models (models that use different
 subset of predictors) based on these criteria.
 The problem with 
\begin_inset Formula $R^{2}$
\end_inset

 is that it would only go up as the number of predictors increases.
 
\begin_inset Formula $RSE=\sqrt{RSS/(n-p-1)}$
\end_inset

 slightly adjusted for this since if the reduction for 
\begin_inset Formula $RSS$
\end_inset

 is small compare to the addition of variables, 
\begin_inset Formula $RSE$
\end_inset

 might go up.
\end_layout

\begin_layout Subsubsection*
How well can we make future predictions?
\end_layout

\begin_layout Standard
Note, these criteria above 
\begin_inset Formula $R^{2}$
\end_inset

 and 
\begin_inset Formula $RSE$
\end_inset

 does not tell us how well we will do for future unseen observation.
 It simply tell us how well the predictors are fitting the current TRAINING
 data.
 The best approach is do training, and test it out a heldout set.
 The best approach now is cross validation (CV), which we will cover in
 Chapter 5.
\begin_inset Foot
status open

\begin_layout Plain Layout
So far in this chapter, we haven't ingrained the concept of measuring test
 error, or CV error into our practice.
 For example, Using 
\begin_inset Formula $R^{2}$
\end_inset

 to measure the quality of your model is not complete, doing model selection
 using ONLY the training set is also not correct.
 These are good questions to ask for providing interpretations for a problem
 at hand.
 However, when it comes to prediction, we really need the concept of test
 error and CV error.
\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Textbook pg.
 102 - 104 does a very nice summary of how to methodogically answer all
 these questions! Great case study!
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Other Considerations in the Regression Model
\end_layout

\begin_layout Standard
We can extend the regression model to be much richer than just using quantitativ
e variables, we could:
\end_layout

\begin_layout Itemize
Include qualitative (factor) variables: some details on encoding, two level,
 or multi-level qualitative variables (and the concept of a baseline)
\begin_inset Foot
status open

\begin_layout Plain Layout
Textbook pg.
 84-86
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Extension: Using interaction: (quantitative 
\begin_inset Formula $\times$
\end_inset

 quantitative) | (qualitiative 
\begin_inset Formula $\times$
\end_inset

 quantitative) 
\begin_inset Foot
status open

\begin_layout Plain Layout
Textbook pg.
 87-90
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Extension: Using higher order terms to create non-linear fits.
 
\begin_inset Foot
status open

\begin_layout Plain Layout
Textbook pg.
 91
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Potential problems with Regression on modeling assumption
\end_layout

\begin_layout Standard
Many problems may occur when fitting a linear regression model, most common
 among these are:
\end_layout

\begin_layout Itemize

\series bold
Non-linearity in the response-predictor
\end_layout

\begin_deeper
\begin_layout Itemize
If the true relationship is far from linear, then virtually all the conclusions
 we draw from the fit are suspect.
 In addition, the prediction accuracy of the model can be significantly
 reduced.
 
\series bold
Residual plots 
\series default
are useful graphical tool for identifying non-linearity.
 In the case of simple linear regression, we would plot the residuals 
\begin_inset Formula $y_{i}-\hat{y}_{i}$
\end_inset

, versus the predictor 
\begin_inset Formula $x_{i}$
\end_inset

.
 In the case of multiple regression, we plot residual 
\begin_inset Formula $e_{i}$
\end_inset

 against the fitted values 
\begin_inset Formula $\hat{y}_{i}$
\end_inset

.
 If the residual plots indicates that there are non-linearity, then a simple
 approach is to use non-linear transformation of the predictors, such as
 
\begin_inset Formula $log(X),\sqrt{X},X^{2}$
\end_inset

 in the regression model.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/non_linearity_in_fit.png
	scale 30
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Correlation of error terms
\end_layout

\begin_deeper
\begin_layout Itemize
An important assumption of linear regression is that the error ters, 
\begin_inset Formula $\epsilon_{1},\epsilon_{2}\dots\epsilon_{n}$
\end_inset

 are uncorrelated.
 This means the fact 
\begin_inset Formula $\epsilon_{i}$
\end_inset

 is positive, doesn't tell us the direction of 
\begin_inset Formula $\epsilon_{i+1}$
\end_inset

.
 The SE of the coefficients are computed assuming that errors are not correlated.
 If in fact there are correlations, then the estimated SE would tend to
 underestimate the true SE.
 As a result, confidence interval and prediction interval would necessarily
 be narrower than they truly are.
 In addition, p-values associated with the model will be lower than they
 should -- in short, we may have an unwarranted sense of confidence in our
 model.
\end_layout

\begin_layout Itemize

\color magenta
The best extreme example to illustrate this: suppose we accidentally doubled
 our data, leading to observations and errors terms identical in pairs.
 If we ignored this, we now have sample size 
\begin_inset Formula $2n$
\end_inset

, when in fact we only have 
\begin_inset Formula $n$
\end_inset

 samples.
 Our estimated parameters would be the same (think of each point overlap
 on itself, so the fit do not change) for samples 
\begin_inset Formula $2n$
\end_inset

 v.s.
 
\begin_inset Formula $n$
\end_inset

, but the confidence would be narrower by a factor of 
\begin_inset Formula $\sqrt{n}$
\end_inset

.
\end_layout

\begin_layout Itemize
This scenario is most common in time series data, when we can see 
\begin_inset Quotes eld
\end_inset

trackings
\begin_inset Quotes erd
\end_inset

 on the residual.
 That is, the residual tend to move in the same direction with each others.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Non-constant variance or error terms
\end_layout

\begin_deeper
\begin_layout Itemize
The same old homoscedasticity v.s.
 heteroscedasticity.
 
\color magenta
The solution is to transform the response 
\begin_inset Formula $Y$
\end_inset

 using a concave function such as 
\begin_inset Formula $log(Y)$
\end_inset

 or 
\begin_inset Formula $\sqrt{Y}$
\end_inset

.
 such a transformation results in a greater amount of shrinkage of the larger
 responses, leading to a reduction in heteroscedacitity
\color inherit
.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/non_constant_variance.png
	scale 30
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Outliers
\end_layout

\begin_deeper
\begin_layout Itemize
Its is typical that outliers with non-unusual predictor values have little
 or no impact on the fit.
 But it could cause other problems, such as inflated RSE or 
\begin_inset Formula $R^{2}$
\end_inset

.
\end_layout

\begin_layout Itemize
The typical way to spot outliers is to plot the residual plot, or the studentize
d residual plot, typical outliers have studentized value exceeding -2 or
 2.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
High-leverage points
\end_layout

\begin_deeper
\begin_layout Itemize
In contrast to outliers (where 
\begin_inset Formula $x_{i}$
\end_inset

 might be typical but 
\begin_inset Formula $y_{i}$
\end_inset

 might be unusually large), high-leverage points are points where 
\begin_inset Formula $x_{i}$
\end_inset

 are atypical.
 This could be particularly dangerous, because they tend ot affect the fit
 significantly.
 
\end_layout

\begin_layout Itemize
The way to identify high-leverage point is to calculate leverage statistics.
 The basic idea is captured by how far away the particular 
\begin_inset Formula $x_{i}$
\end_inset

 is away from all the other 
\begin_inset Formula $x$
\end_inset

's.
 
\end_layout

\begin_layout Itemize
A point that has high studentized residual and leverage means that its 
\begin_inset Formula $x_{i}$
\end_inset

 and 
\begin_inset Formula $y_{i}$
\end_inset

 are both very atypical, a particuarly dangerous combination!
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Colinearity
\end_layout

\begin_deeper
\begin_layout Itemize
colinearity can make the coefficients jumpy.
 This means the SE of the coefficients would be greater, which in terms
 means that the 
\begin_inset Formula $t$
\end_inset

-statistic would be smaller, and we have a much smaller chance of rejecting
 the null.
 The implication is a decrease of POWER (in the case that 
\begin_inset Formula $\beta$
\end_inset

 is truly non-zero, colinearity can inflate the SE, and cause the 
\begin_inset Formula $t$
\end_inset

-statistic to be small, and 
\begin_inset Formula $p$
\end_inset

 values to be large).
\end_layout

\begin_layout Itemize
A simple to look for colinearity is to look at the correlation matrix, and
 see which entry is large.
 However, there are mult-collinearity that involves more than 3 predictors
 that are not detectable by looking at the correlation matrix.
 In such cases, we want to use the VIF (Variance inflation factor)
\end_layout

\begin_layout Itemize

\color magenta
A VIF that exceeds 5 or 10 indicates a problematic amount of colinearity
\color inherit

\begin_inset Formula \[
VIF(\beta_{j})=\frac{1}{1-R^{2}X_{j}|X_{-j}}\]

\end_inset


\end_layout

\begin_layout Itemize
where 
\begin_inset Formula $R_{X_{j}|X_{-j}}^{2}$
\end_inset

 is the 
\begin_inset Formula $R^{2}$
\end_inset

 from a regression of 
\begin_inset Formula $X_{j}$
\end_inset

 onto all of the other predictors.
 A large 
\begin_inset Formula $R^{2}$
\end_inset

 means colinearity, and VIF would be large.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*

\color blue
Woes of (Interpreting) Linear/Logistic Regression
\end_layout

\begin_layout Itemize

\series bold
\color blue
Scenario
\series default
: Even though the coefficient for 
\begin_inset Formula $X_{1}$
\end_inset

 is positive (and statistically significant) when fitting 
\begin_inset Formula $Y$
\end_inset

 on 
\begin_inset Formula $X_{1}$
\end_inset

 alone, when 
\begin_inset Formula $Y$
\end_inset

 is fitted against 
\begin_inset Formula $X_{1},X_{2}$
\end_inset

, then 
\begin_inset Formula $X_{1}$
\end_inset

's coefficient became very close to 0, and the result is not statistically
 significant.
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
\color blue
Why?: 
\begin_inset Formula $X_{1}$
\end_inset

 
\series default
actually doesn't affect 
\begin_inset Formula $Y$
\end_inset

, but 
\begin_inset Formula $X_{2}$
\end_inset

 does.
 When 
\begin_inset Formula $X_{1}$
\end_inset

 is highly correlated with 
\begin_inset Formula $Y$
\end_inset

, when we increase 
\begin_inset Formula $X_{2}$
\end_inset

, 
\begin_inset Formula $Y$
\end_inset

 would increase, but 
\begin_inset Formula $X_{1}$
\end_inset

 would also increase.
 Hence, it would appear as if 
\begin_inset Formula $X_{1}$
\end_inset

 has an impact on 
\begin_inset Formula $Y$
\end_inset

 when in reality it is just masking the effect of 
\begin_inset Formula $X_{2}$
\end_inset

 variable that truly matters.
\begin_inset Foot
status open

\begin_layout Plain Layout

\color blue
See textbook 73 & 74.
 Ice cream (
\begin_inset Formula $X_{1}$
\end_inset

), Temperature (
\begin_inset Formula $X_{2}$
\end_inset

), and shark attacks 
\begin_inset Formula $(Y)$
\end_inset


\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
\color blue
Scenario
\series default
: When 
\begin_inset Formula $p$
\end_inset

-values for each of individual predictors (from 
\begin_inset Formula $t$
\end_inset

-test) is small, then it must be the case that at least one variable is
 related to the response.
 Why do we need to look at 
\begin_inset Formula $F$
\end_inset

-statistic at all?
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
\color blue
Why?
\begin_inset Foot
status open

\begin_layout Plain Layout

\color blue
see page 77 for the full explanation
\end_layout

\end_inset

: 
\series default
Suppose we have
\series bold
 
\begin_inset Formula $p=100$
\end_inset


\series default
 and 
\begin_inset Formula $\beta_{1}=\dots=\beta_{100}=0$
\end_inset

, then there still will be about 
\begin_inset Formula $5\%$
\end_inset

 of the predictors that will have small 
\begin_inset Formula $p$
\end_inset

-values (i.e.
 
\begin_inset Formula $p<0.05$
\end_inset

) just by chance, due to type I error.
 This means that when we look at however many replica of the training set,
 
\begin_inset Formula $5\%$
\end_inset

 of the predictors would appeared to be important when there is no relationship
 between 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

.
 In fact, the chance of having at least 
\begin_inset Formula $1$
\end_inset

 predictor to have small 
\begin_inset Formula $p$
\end_inset

-value is almost 1.
 So if we say, if at least 1, or a few predictors are significant, then
 we say the 
\begin_inset Formula $X$
\end_inset

's altogether are related to 
\begin_inset Formula $Y$
\end_inset

, that would have been wrong.
 The advantage of 
\begin_inset Formula $F$
\end_inset

-test is that it adjust for the number of predictors (i.e.
 regardless of the number of the predictors, the chance of making a mistake
 is bounded at 
\begin_inset Formula $5$
\end_inset

% when 
\begin_inset Formula $\alpha=5\%$
\end_inset

).
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
\color blue
Scenario
\series default
: [Credit example] How is it possible for student status to be associated
 with an increase in probability (i.e.
 
\begin_inset Formula $\beta$
\end_inset

 fitted from logistic regression 
\begin_inset Formula $>0$
\end_inset

) of credit default when student status is fitted alone, and a drecrease
 in probability of default (i.e.
 
\begin_inset Formula $\beta$
\end_inset

 fitted from logistic regression 
\begin_inset Formula $<0$
\end_inset

) when the predictor BALANCE is also included?
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
\color blue
Why? 
\series default
In the multiple logistic regression case, it is clear that increase in balance
 cause higher probability of default.
 More so, for a fixed balance, students are less likely to default (hence
 
\begin_inset Formula $\beta_{\text{balance}}>0$
\end_inset

 and 
\begin_inset Formula $\beta_{student}<0$
\end_inset

 when both variables are fitted).
 So why would 
\begin_inset Formula $\beta_{\text{student alone}}>0$
\end_inset

.
 The reason is correlation/con-founding.
 When we average over all incomes, we see that students, on average, have
 higher balance compared to non-student, and higher balance associated with
 high default rate.
 Therefore, when we are only looking at student status (and everything else
 is average out), it appears if student are more likely to default.
 In other words, student status is not really causing higher default rate,
 it was after all BALANCE, but the correlation with student status made
 it appear that students are more likely to default.
\begin_inset Foot
status open

\begin_layout Plain Layout

\color blue
This is very similar to the 
\begin_inset Formula $W$
\end_inset

= weight and 
\begin_inset Formula $H$
\end_inset

 = height example from lecture slides.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\color blue
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\color blue
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/regression_confounding.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Again, be very careful when interpreting the coefficients.
 It might very well be the case that when you fit the univariate logistic
 regression, the coefficient is positive.
 But when you fit a multivariate logistic regression, the coefficient becomes
 negative.
 Scenarios like these are often caused by:
\end_layout

\begin_deeper
\begin_layout Itemize
The variable DOES have a true association with 
\begin_inset Formula $Y$
\end_inset

, but when put together with other highly colinear predictors, it appeared
 to be redundant 
\begin_inset Foot
status open

\begin_layout Plain Layout
Football tackels = 
\begin_inset Formula $0.4\times$
\end_inset

H v.s.
 Football tackles = 
\begin_inset Formula $0.8W-0.1H$
\end_inset

 falls under this category.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
The variable DOES NOT have a true association with 
\begin_inset Formula $Y$
\end_inset

, and is significant only because it is correlated with another much relevant
 variable 
\begin_inset Foot
status open

\begin_layout Plain Layout
Stduent credit default example above
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
When to use what model? Linear Regression V.S.
 
\begin_inset Formula $K$
\end_inset

-nearest neighbors
\end_layout

\begin_layout Standard
Even though this section compares linear regression with 
\begin_inset Formula $K$
\end_inset

-nearest neighbors method, the comparison is really between 
\shape italic
\color blue
parametric 
\shape default
\color black
and
\shape italic
\color blue
 non-parametric 
\shape default
\color black
methods.
\end_layout

\begin_layout Itemize
Parametric methods assume a functional form of the underlying 
\begin_inset Formula $f$
\end_inset

, thus are more restrictive.
 But it often have less number of parameters to estimate (tend to have higher
 bias)
\end_layout

\begin_layout Itemize
Non-parametric methods often do not assume a form for 
\begin_inset Formula $f$
\end_inset

, and are more flexible.
 But there are more number of parameters to estimate (tend to have higher
 variance)
\end_layout

\begin_layout Standard
When does linear regression (a parametric method) outperforms 
\begin_inset Formula $k$
\end_inset

-nearest neighbors (a non-parametric method)? 
\shape italic
The parametric approach will outperform the non- parametric approach if
 the parametric form that has been selected is close to the true form of
 
\begin_inset Formula $f$
\end_inset

.
 
\shape default
More details below:
\end_layout

\begin_layout Itemize
When the underlying 
\begin_inset Formula $f(X)$
\end_inset

 is truly linear, then LR will do better than 
\begin_inset Formula $k$
\end_inset

-nearest neighbor, and 
\begin_inset Formula $k$
\end_inset

-NN method would do worse for smaller 
\begin_inset Formula $k$
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/LR_KNN_comparison_1.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
When the underlying 
\begin_inset Formula $f(X)$
\end_inset

 is somewhat nonlinear, then LR can started to lose for a slightly more
 complex (moderate 
\begin_inset Formula $k$
\end_inset

) 
\begin_inset Formula $k$
\end_inset

-NN method.
 But when the underlying 
\begin_inset Formula $f(X)$
\end_inset

 is highly non-linear, then 
\begin_inset Formula $k$
\end_inset

-NN can be significantly better
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/LR_KNN_comparison_2.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
When we add more irrelevant predictors (and 
\begin_inset Formula $p$
\end_inset

 increases), 
\begin_inset Formula $k$
\end_inset

-NN can really start to suffer from the 
\begin_inset Quotes eld
\end_inset

curse of dimensionality
\begin_inset Quotes erd
\end_inset

.
 LR also deteroriate, but at a much lower rate.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/LR_KNN_comparison_3.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Subsection*
When to use what model? A comparison of classification methods
\end_layout

\begin_layout Standard
These six examples
\begin_inset Foot
status open

\begin_layout Plain Layout
Textbook pg.153
\end_layout

\end_inset

 illustrate that no one method will dominate the oth- ers in every situation.
 When the true decision boundaries are linear, then the LDA and logistic
 regression approaches will tend to perform well.
 When the boundaries are moderately non-linear, QDA may give better results.
 Finally, for much more complicated decision boundaries, a non-parametric
 approach such as KNN can be superior.
 But the level of smoothness for a non-parametric approach must be chosen
 carefully.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/classification_comparison.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
When to use what model? Ridge Regression V.S.
 LASSO
\end_layout

\begin_layout Paragraph*
Bias-Variance-Trade-Off of Regularization Technique:
\end_layout

\begin_layout Standard
In general, in situations where the relationship between the response and
 the predictors is close to linear, the least squares estimates will have
 low bias but may have high variance.
 This means that a small change in the training data can cause a large change
 in the least squares coefficient estimates.
 In particular, when the number of variables 
\begin_inset Formula $p$
\end_inset

 is almost as large as the number of observations 
\begin_inset Formula $n$
\end_inset

, the least squares estimates will be extremely variable.
 And if 
\begin_inset Formula $p>n$
\end_inset

, then the least squares estimates do not even have a unique solution, whereas
 ridge regression or LASSO can still perform well by trading off a small
 increase in bias for a large decrease in variance.
 Hence, ridge regression and LASSO works best in situations where the least
 squares estimates have high variance.
\end_layout

\begin_layout Standard
\begin_inset Foot
status open

\begin_layout Plain Layout
Ridge regression also has substantial computational advantages over best
 subset selection, which requires searching through 
\begin_inset Formula $2^{p}$
\end_inset

 models.
 As we discussed previously, even for moderate values of 
\begin_inset Formula $p$
\end_inset

, such a search can be computationally infeasible.
 In contrast, for any fixed value of 
\begin_inset Formula $\lambda$
\end_inset

, ridge regression only fitrs a single model, and the model-fitting procedure
 can be performed quite quickly.
 It can be shown that the computations for ridge regression, simultaneously
 for all values of 
\begin_inset Formula $\lambda$
\end_inset

, are almost identical to those for fitting a model using least squares.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph*
Comparing the LASSO and Ridge Regression:
\end_layout

\begin_layout Standard
It is clear that the lasso has a major advantage over ridge regression,
 in that it produces simpler and more interpretable models that involve
 only a subset of the predictors.
 However, which method leads to better prediction accuracy? Here is the
 take-away:
\end_layout

\begin_layout Itemize
If the response is a function of many of the predictors, then ridge regression
 will generally perform better
\end_layout

\begin_layout Itemize
If the response is a function of only a handful of predictors, then LASSO
 will generally perform better
\end_layout

\begin_layout Standard
This very much has to do with the fact that these two methods shrink the
 coefficients different.
 Ridge regression will shrink all the coefficient estimates for ALL the
 predictors towards 0, but not necessarily exactly to 0.
 On the other hand, LASSO will drive certain coefficient estimates to exactly
 0! Here is a simulated example from the textbook:
\end_layout

\begin_layout Itemize
A comparison of Ridge & LASSO based on a simulated data set where all 45
 predictors were related to the response -- that is, none of the true coefficien
ts 
\begin_inset Formula $\beta_{1}\dots\beta_{45}$
\end_inset

 equaled zero.
 The LASSO implicitly assumes that a number of the coefficients truly equal
 0.
 Consequently, it is not surprising that ridge regression outperforms the
 LASSO.
\begin_inset Foot
status open

\begin_layout Plain Layout
Notice here we plot both model against their 
\begin_inset Formula $R^{2}$
\end_inset

 on the training data as a useful way to index models, and is a popular
 way to compare models with different types of regularization.
 It might not be fair to compare different regularization based on absolute
 magnitude of 
\begin_inset Formula $\lambda$
\end_inset

, because different 
\begin_inset Formula $\lambda$
\end_inset

 in different regularization might have different trade-off between RSS
 and model complexity.
 Using 
\begin_inset Formula $R^{2}$
\end_inset

 or RSS directly give us fair playing field (we know how hard both methods
 have worked to fit the model).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/ridge_lasso_comparison_1.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
A comparison of Ridge & LASSO based on a simulated data set where only 2
 of the 45 predictors were related to the response -- in this case, since
 the true model is sparse to start with, LASSO does a better job at capturing
 the right relationship
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/ridge_lasso_comparison_2.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
These two examples illustrate that neither ridge regression nor the LASSO
 will universally dominate the other.
 In general, one might expect the LASSO to perform better when the responde
 is a function of only a relatively small number of predictors.
 However, the number of predictors that is related to the response is never
 known a prior for real data sets.
 A technique such as cross validation can be used in order to determin which
 approach is better on a particular data set!
\end_layout

\begin_layout Paragraph*
Bayesian Interpretation for Ridge Regression and the LASSO:
\end_layout

\begin_layout Standard
One can view ridge regression and LASSO through a Bayesian lens.
 The goal here is to find the posterior distribution 
\begin_inset Formula $P(\beta|X,Y)$
\end_inset

, based on likelihood 
\begin_inset Formula $f(Y|X,\beta)$
\end_inset

 and prior distribution 
\begin_inset Formula $p(\beta)$
\end_inset

.
 If we assume the usual linear model,
\end_layout

\begin_layout Standard
\begin_inset Formula \[
Y=\beta_{0}+X_{1}\beta_{1}+\dots+X_{p}\beta_{p}+\epsilon\]

\end_inset


\end_layout

\begin_layout Standard
and suppose that the errors are independent and drawn from a normal distribution.
 Furthermore, assume that 
\begin_inset Formula $p(\beta)=\prod_{j=1}^{p}g(\beta_{j})$
\end_inset

, for some density function g.
 It turns out that:
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $g$
\end_inset

 is a gaussian distribution with mean zero and standard deviation a function
 of 
\begin_inset Formula $\lambda$
\end_inset

, then it follows that the posterior mode for 
\begin_inset Formula $\beta$
\end_inset

—that is, the most likely value for 
\begin_inset Formula $\beta$
\end_inset

, given the data—is given by the ridge regression solution.
 (In fact, the ridge regression solution is also the posterior mean.)
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $g$
\end_inset

 is a Laplace distribution with mean zero and scale parameter a function
 of 
\begin_inset Formula $\lambda$
\end_inset

, then it follows that the posterior mode for 
\begin_inset Formula $\beta$
\end_inset

 is the LASSO solution.
 However, the LASSO solution is not the posterior mean.
\end_layout

\begin_layout Subsection*
When to use what model? Linear Models V.S.
 Tree-based models
\end_layout

\begin_layout Standard
Regression and classification trees have a very different flavor from linear
 models.
 In particular, linear models assume
\end_layout

\begin_layout Standard
\begin_inset Formula \[
f(X)=\beta_{0}+\sum_{j=1}^{p}\beta_{j}X_{j}\]

\end_inset


\end_layout

\begin_layout Standard
whereas regression trees assume a model of the form 
\end_layout

\begin_layout Standard
\begin_inset Formula \[
f(X)=\sum_{m=1}^{M}c_{m}1(X\in R_{m})\]

\end_inset


\end_layout

\begin_layout Standard
When the relationship between response and predictors are well approximated
 linearly, then linear model is likely to do better.
 When there is a more complex relationship between response and predictors,
 then a tree-based better might give better test error.
 Of course, when selecting a model, test error is not the only consideration,
 implementation, interpretability and visualization are very well important
 factors to consider.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/linear_model_vs_tree_based_model.png
	scale 60

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Advantages of Tree-based methods:
\end_layout

\begin_layout Itemize
Trees are very easy to explain to people.
 In fact, they are even easier to explain than linear regression!
\end_layout

\begin_layout Itemize
Some people believe that decision tree more closely mirror human decision
 making.
\end_layout

\begin_layout Itemize
Trees can be displayed graphically, and are easily interpreted even by a
 non-expert.
\end_layout

\begin_layout Itemize
Trees can easily handle qualitative predictors without the need to create
 dummy variables.
\end_layout

\begin_layout Subsubsection*
Disadvantages of Tree-based methods:
\end_layout

\begin_layout Itemize
Trees generally do not have the same level of predictive accuracy as some
 of the other regression and classification approachesseen in thebook.
\end_layout

\begin_layout Standard
We can improve this by using ensemble methods, where we combine weak learners
 like trees into strong learners.
 
\end_layout

\begin_layout Subsection*
Comparing SVM, Logistic Regression, and LDA for Classification Problems
\end_layout

\begin_layout Standard
As we have seen, in the 
\begin_inset Quotes eld
\end_inset

relationship to logistic regression
\begin_inset Quotes erd
\end_inset

 section in Chapter 9 notes, SVM and logistic regression (with regularization)
 have very similar LOSS+Penalty formulation, and their loss functions always
 behave very similarly.
 So when do we use which?
\end_layout

\begin_layout Itemize
When classes are completely (linearly) separable, logistic regression breaks
 down.
 In such cases, SVM & LDA is preferred
\end_layout

\begin_layout Itemize
Similarly, when classes are (nearly) separable, SVM does better than logistic
 regression.
 So does LDA (does better than logistic regression)
\end_layout

\begin_layout Itemize
If you wish to estimate probabilities, logistic regression is the choice.
 Logistic regression with regularization not only has the benefits of returning
 probabilities, but it also does feature selection.
\end_layout

\begin_layout Itemize
SVM does not return probabilities, and does not perform feature selection
 (the sparsity is in the few number of data points that determines the decision
 boundaries, not in the number of predictors we used [i.e.
 in row, not column]).
 However, for nonlinear boundaries, kernel SVMs are popular.
 We can use kernel tricks on LR and LDA as well, but computations are more
 expensive.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Lingering Questions:
\end_layout

\begin_layout Standard
None so far
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Chapter 4: Classification
\end_layout

\begin_layout Standard
Given a feature vector 
\begin_inset Formula $X$
\end_inset

 and a qualitative response 
\begin_inset Formula $Y$
\end_inset

 taking values in the discrete set 
\begin_inset Formula $\mathcal{C}$
\end_inset

, the clasification task is to build a function 
\begin_inset Formula $C(X)$
\end_inset

 that takes as input the feature vector 
\begin_inset Formula $X$
\end_inset

 and predicts its value for 
\begin_inset Formula $Y$
\end_inset

; i.e.
 
\begin_inset Formula $C(X)\in\mathcal{C}$
\end_inset

.
 We will talk about methods such as SVM (in chapter 9) that directly construct
 a classifer that produce classes 
\begin_inset Formula $C(X)\in\mathcal{C}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Often, we are more interested in estimating the probabilities that 
\begin_inset Formula $X$
\end_inset

 belongs to each category in 
\begin_inset Formula $\mathcal{C}$
\end_inset

.
 For example, it is more valuable to have an estimate of the probability
 that an insurance claim is fraudulent, than a classification fraudulent
 or not.
 Logistic regression, discrimnant analysis fall under this category of methods.
\end_layout

\begin_layout Subsection*
Logistic Regression
\end_layout

\begin_layout Standard
When the outcome is binary, using linear regression to perform classification
 can be suitable
\begin_inset Foot
status open

\begin_layout Plain Layout
Suitable because in the population 
\begin_inset Formula $E(Y|X=x)=P(Y=1|X=x)$
\end_inset

, and that's what we want to estimate.
 In the case of binary outcome, linear regression is equivalent to linear
 discrimnant analysis.
\end_layout

\end_inset

, but more often than not we ended up with probability estimates are outside
 of the 
\begin_inset Formula $[0,1]$
\end_inset

 range.
 When the outcome has more than two classes, there is no clear ordering
 of the classes in 
\begin_inset Formula $\mathcal{C}$
\end_inset

, but changing the encoding means we have some implicit ordering of the
 outcomes
\begin_inset Foot
status open

\begin_layout Plain Layout
Even if there is a natural ordering, we are assuming the difference between
 each pair of outcome has the same distance, this is often false.
\end_layout

\end_inset

 -- In such cases, logistic regression and discrimnant analysis are better
 tools.
\end_layout

\begin_layout Itemize
The probability 
\begin_inset Formula $P(Y=k|X=x)$
\end_inset

 is modeled as 
\begin_inset Formula $p(X)=\frac{e^{\beta_{0}+\mathbf{X}\mathbf{\beta}}}{1+e^{\beta_{0}+\mathbf{X}\mathbf{\beta}}}$
\end_inset

 or 
\begin_inset Formula $log\left(\frac{p(X)}{1-P(X)}\right)=\mathbf{X\beta}$
\end_inset

 -- the log-odd is linear in the parameters.
 This functional form has implications in terms of interpretation:
\end_layout

\begin_deeper
\begin_layout Itemize
increase 
\begin_inset Formula $X_{1}$
\end_inset

 by one unit changes the log odds by 
\begin_inset Formula $\beta_{1}$
\end_inset

.
 Equivalently, increase 
\begin_inset Formula $X_{1}$
\end_inset

 by one unit changes the odds by 
\begin_inset Formula $e^{\beta_{1}}$
\end_inset

, this is because
\begin_inset Formula \[
\frac{\left(\frac{P(X_{new})}{1-P(X_{new})}\right)}{\left(\frac{P(X)}{1-P(X)}\right)}=\frac{\left(e^{\beta_{0}+(X+1)\beta_{1}+\dots}\right)}{\left(e^{\beta_{0}+(X)\beta_{1}+\dots}\right)}=e^{\beta_{1}}\]

\end_inset


\end_layout

\begin_layout Itemize
The amount of change in 
\begin_inset Formula $p(X)$
\end_inset

 due to one unit change in 
\begin_inset Formula $X$
\end_inset

 will depend on the value of 
\begin_inset Formula $X$
\end_inset


\end_layout

\begin_layout Itemize

\color red
But regardless of the value of 
\begin_inset Formula $X_{1}$
\end_inset

, if 
\begin_inset Formula $\beta_{1}$
\end_inset

 is positive/negative, then increasing 
\begin_inset Formula $X_{1}$
\end_inset

 will associate with increasing/decreasing 
\begin_inset Formula $p(X)$
\end_inset

 [WHY?]
\end_layout

\end_deeper
\begin_layout Itemize
We can also use indicator variables as predictors in the logistic regression
 model.
\end_layout

\begin_layout Itemize
The fitting of the parameters is done via Maximum Likelihood Estimation.
 It has no closed form solution, so it is often solved using iterative least
 squares (in R).
\end_layout

\begin_layout Itemize

\color blue
The logistic regression coefficient might be small -- ALWAYS remember, the
 unit of the coefficient comes in the same units as the predictors! 
\end_layout

\begin_layout Itemize
When the number of classes in 
\begin_inset Formula $\mathcal{C}$
\end_inset

 is 
\begin_inset Formula $>2$
\end_inset

, we would model 
\begin_inset Formula $P(Y=k|X)$
\end_inset

 as 
\begin_inset Formula \begin{eqnarray*}
P(Y=k|X) & = & \frac{exp(\beta_{0k}+\beta_{1k}X_{1}+\dots+\beta_{pk}X_{p})}{\sum_{l=1}^{K}exp(\beta_{0l}+\beta_{1l}X_{1}+\dots+\beta_{pl}X_{p})}\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
This is often referred to as multinomial regression or (soft-max regression
\begin_inset Foot
status open

\begin_layout Plain Layout
See CS 229 Generative Learning Algorithm (PDF2) section 9.3 on Softmax regression
\end_layout

\end_inset

).
 However, in practice, this model is used much less often than discriminant
 analysis.
\end_layout

\end_deeper
\begin_layout Itemize
Logistic regression can also fit quadratic boundaries like QDA, by explictly
 including quadratic terms in the model.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Discrimnant Analysis
\end_layout

\begin_layout Standard
Generally, in classification, when we are interested in estimating the class
 probabilities, we have two modeling approach:
\end_layout

\begin_layout Itemize

\series bold
Discrimnant Learning Algorithm
\series default
: This approach models 
\begin_inset Formula $P(Y=k|X=x)$
\end_inset

 directly, without modeling 
\begin_inset Formula $P(X=x)=f_{k}(x)$
\end_inset

 explicitly (i.e.
 Logistic Regression).
\end_layout

\begin_layout Itemize

\series bold
Generative Learning Algorithm
\series default

\begin_inset Foot
status open

\begin_layout Plain Layout
See CS 229 Generative Learning Algorithm (PDF 2) to get a better understanding
\end_layout

\end_inset

: This approach models the distribution of 
\begin_inset Formula $X$
\end_inset

 in each of the classes separately, and then use Bayes theorem to flip things
 around and obtain 
\begin_inset Formula $P(Y|X)$
\end_inset

.
 LDA, QDA actually falls under this category, don't get confused by the
 'discrimnant' in the naming.
 
\end_layout

\begin_layout Standard
As a reminder for Bayes theorem:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
P(Y=k|X=x) & = & \frac{P(X=x|Y=k)\times P(Y=k)}{P(X=x)}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
One writes this slightly differently for discriminant analysis:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
P(Y=k|X=x) & = & \frac{\pi_{k}f_{k}(x)}{\sum_{l=1}^{K}\pi_{l}f_{l}(x)}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $f_{k}(x)=P(X=x|Y=k)$
\end_inset

 is the density for 
\begin_inset Formula $X$
\end_inset

 in class 
\begin_inset Formula $k$
\end_inset

.
 Here, we will use normal densities in LDA/QDA, but we can have other variants:
\end_layout

\begin_deeper
\begin_layout Itemize
Assume 
\begin_inset Formula $X$
\end_inset

 follow some other distribution other than normal
\end_layout

\begin_layout Itemize
Assume the components of 
\begin_inset Formula $\mathbf{X}=(X_{1}\dots X_{p})$
\end_inset

 are independent from each other, so 
\begin_inset Formula $f_{k}(\mathbf{X})=\prod f_{kj}(x_{j})$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $\pi_{k}=P(Y=k)$
\end_inset

 is the marginal prior probability for class 
\begin_inset Formula $k$
\end_inset


\end_layout

\begin_layout Standard
Why do we need another method, when we already have logistic regression?
\end_layout

\begin_layout Itemize
When the classes are well separated, the parameters of the logistic regression
 model is surprsingly unstable.
 LDA does not suffer from that issue.
\begin_inset Foot
status open

\begin_layout Plain Layout
Logistic regression was invented by Biostatistician, where the data are
 usually not clearly separable
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $n$
\end_inset

 is small and the 
\begin_inset Formula $x$
\end_inset

 are approximately normal, LDA does better.
\end_layout

\begin_layout Itemize
Finally, when 
\begin_inset Formula $K>2$
\end_inset

, then LDA is very popular.
\end_layout

\begin_layout Itemize
When the underlying assumption about 
\begin_inset Formula $X$
\end_inset

 is correct, this approach will give us the optimal classification rule
 (Bayes optimal classifer)
\begin_inset Foot
status open

\begin_layout Plain Layout
CS 229 Generative Learning Algorithm (PDF 2) also has a section that compare
 logistic regression with LDA.
 The basic trade-off is that GDA is better when the assumption about 
\begin_inset Formula $X$
\end_inset

 is correct, otherwise, logistic regression is more robust
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Linear Discrimnant Analysis when 
\begin_inset Formula $p=1$
\end_inset

 or 
\begin_inset Formula $p>1$
\end_inset

 (LDA)
\end_layout

\begin_layout Standard
Our goal is to classify the example with max 
\begin_inset Formula $p_{k}(x)=\frac{\pi_{k}f_{k}(x)}{\sum\pi_{k}f_{k}(x)}$
\end_inset

.
 In LDA, we assume that each of the 
\begin_inset Formula $f_{k}(x)$
\end_inset

 is univariate normal and all the classes have the same covariance 
\begin_inset Formula $\sum_{k}=\sum\:\forall k$
\end_inset

.
 Under this assumption, getting the predicted 
\begin_inset Formula $p_{k}(x)$
\end_inset

 amounts to estimating 
\begin_inset Formula $\pi_{k}$
\end_inset

 and 
\begin_inset Formula $f_{k}(x)$
\end_inset

.
 
\begin_inset Formula $\pi_{k}$
\end_inset

 would simply be the proportion of samples that falls in class 
\begin_inset Formula $k$
\end_inset

, and to get 
\begin_inset Formula $f_{k}(x)$
\end_inset

, we just need to estimate 
\begin_inset Formula $\mu_{k}$
\end_inset

 and 
\begin_inset Formula $\sigma_{k}^{2}$
\end_inset

.
 Once we have these, we can plug into 
\begin_inset Formula $p_{k}(x)$
\end_inset

, and classify 
\begin_inset Formula $x$
\end_inset

 with the class where 
\begin_inset Formula $p_{k}(x)$
\end_inset

 is maximized.
 It can be shown, with some algebraic simplification that 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
\delta_{k}(x) & = & x\times\frac{\mu_{k}}{\sigma^{2}}-\frac{\mu_{k}^{2}}{2\sigma^{2}}+log(\pi_{k})\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We do not know the parameters in the discriminant score, so they need to
 be estimated and plug-in:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\hat{\delta}_{k}(x)=x\times\frac{\hat{\mu}_{k}}{\hat{\sigma_{k}}}-\frac{\hat{\mu}_{k}^{2}}{2\hat{\sigma_{k}}}+log(\hat{\pi}_{k})\]

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\delta_{k}(x)$
\end_inset

 is a linear function of 
\begin_inset Formula $x$
\end_inset

.
 The case of 
\begin_inset Formula $p>1$
\end_inset

 is very similar, except that now 
\begin_inset Formula $f_{k}(\mathbf{X})$
\end_inset

 is multivariate normal, the resulting discriminant score 
\begin_inset Formula $\delta_{k}$
\end_inset

 is also a linear function (i.e.
 
\begin_inset Formula $\delta_{k}(x)=c_{k0}+c_{k1}x_{1}+c_{k2}x_{2}+\dots c_{kp}x_{p}$
\end_inset

).
\end_layout

\begin_layout Subsubsection*
Quadratic Discrimnant Analysis (QDA)
\end_layout

\begin_layout Standard
As we have discussed, LDA assumes that the observations within each class
 are drawn from a multivariate Gaussian distribution with a class- specific
 mean vector and a covariance matrix that is common to all K classes.
 Quadratic discriminant analysis (QDA) provides an alternative approach.
 Like LDA, the QDA classifier results from assuming that the observations
 from each class are drawn from a Gaussian distribution, and plugging estimates
 for the parameters into Bayes’ theorem in order to per- form prediction.
 However, unlike LDA, QDA assumes that each class has its own covariance
 matrix.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
That is, each x in each class 
\begin_inset Formula $k\sim N(\mathbf{\mu_{k}},\mathbf{\sum_{K}})$
\end_inset

, where 
\begin_inset Formula $\mathbf{\sum_{k}}$
\end_inset

 is covariance matrix of the 
\begin_inset Formula $k^{th}$
\end_inset

 class.
 In LDA, we assume 
\begin_inset Formula $\sum_{k}=\sum\:\forall\: k$
\end_inset

.
 And it turns out the the decision boundary for QDA is quadratic.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
In both LDA & QDA, once we have obtained the 
\begin_inset Formula $\hat{\delta}_{k}(x)$
\end_inset

, we can turn these into estimates for class probabilities:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
\hat{P}(Y=k|X=x) & = & \frac{exp(\hat{\delta}_{k}(x))}{\sum_{l=1}^{K}exp(\hat{\delta}_{l}(x))}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
so classifying to the largest 
\begin_inset Formula $\hat{\delta}_{k}(x)$
\end_inset

 amounts to classifying to the class for which 
\begin_inset Formula $\hat{P}(Y=k|X=x)$
\end_inset

 is largest.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard

\series bold
\color blue
When would we prefer LDA over QDA?
\end_layout

\begin_layout Standard
The answer lies in bias-variance-trade-off.
 When there are 
\begin_inset Formula $p$
\end_inset

 predictors, then estimating a covariance matrix requires estimating 
\begin_inset Formula $p(p+1)/2$
\end_inset

 parameters (the correlation entries in the correlation matrix).
 QDA estimates a separate covariance matrix for each class, for a total
 of 
\begin_inset Formula $Kp(p+1)/2$
\end_inset

 parameters.
 With 50 predictors this is some multiple of 1225, which is a lot of parameters.
 By instead assum- ing that the K classes share a common covariance matrix,
 the LDA model becomes linear in 
\begin_inset Formula $x$
\end_inset

, which means there are 
\begin_inset Formula $Kp$
\end_inset

 linear coefficients to estimate.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Consequently, LDA is a much less flexible classifier than QDA, and so has
 substantially lower variance.
 This can potentially lead to improved prediction performance.
 But there is a trade-off: if LDA’s assumption that the K classes share
 a common covariance matrix is badly off, then LDA can suffer from high
 bias.
 
\color blue
Roughly speaking, LDA tends to be a better bet than QDA if there are relatively
 few training observations and so reducing variance is crucial.
 In contrast, QDA is recommended if the training set is very large, so that
 the variance of the classifier is not a major concern
\color inherit
, or if the assumption of a common covariance matrix for the 
\begin_inset Formula $K$
\end_inset

 classes is clearly untenable.
\end_layout

\begin_layout Subsubsection*
Other forms of Discriminant Analysis
\end_layout

\begin_layout Standard
Naive Bayes assumes features are independent in each class.
 useful when 
\begin_inset Formula $p$
\end_inset

 is large, and so multivariate methods like QDA and even LDA break down,
 because there are simply too many parameters to estimate.
\end_layout

\begin_layout Itemize
Gaussian naive Bayes assumes each 
\begin_inset Formula $\sum_{k}$
\end_inset

 is diagonal: 
\begin_inset Formula $\delta_{k}(x)\:\propto log\left[\pi_{k}\prod_{j=1}^{p}f_{kj}(x_{j})\right]$
\end_inset


\end_layout

\begin_layout Itemize
can use for mixed feature vectors (qualitative and quantitative).
 If 
\begin_inset Formula $X_{j}$
\end_inset

 is qualitative, replace 
\begin_inset Formula $f_{kj}(x_{j})$
\end_inset

 with probability mass function over discrete categories.
\end_layout

\begin_layout Itemize
This modeling approach is almost always wrong in approximating the underlying
 true 
\begin_inset Formula $f$
\end_inset

, but in the context of classification, we are not aiming to get the probability
 exactly right.
 All we care about is which probability is the highest.
 So this technique allowed us to be more biased, but in returns we get a
 huge reduction in the parameters we need to estimate.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/different_form_discriminant_analysis.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Assessing model accuracy
\end_layout

\begin_layout Standard
Assessing model accuracy in the context of classification is more complicated
 than regression, where we just need to calculate 
\begin_inset Formula $MSE$
\end_inset

.
 In the context of classification, the underlying data set might be unbalanced,
 this means that the absolute number of mis-classification error is meaningless.
 If we have an extremely unbalanced data set, say 
\begin_inset Formula $3.3\%$
\end_inset

 default class.
 A naive classification that always predict non-default class would achieve
 this low mis-classification error, but it does not mean it is good.
 If we look beyond the misclassification rate, and look at specificity and
 sensitivity, it has a very low sensitity (The % of true defaulter capture
 is low, because we just blindly say everyone is non-defaulter).
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
As we have seen, LDA is trying to approximate the Bayes classifer, which
 have the lowest total error rate out of all classifers.
 That is, Bayes classifer will yield the smallest possible total number
 of misclassified observations, irrespective of which class the error comes
 from.
 This means we should not just look at misclassification rate to assess
 the quality of a classifer, we need to look at other metrics.
 This is where the typical concept of thresholding, false positive, false
 negative, and the ROC curve + AUC (Area under the curve) are useful.
\begin_inset Foot
status open

\begin_layout Plain Layout
I find the other book 
\begin_inset Quotes eld
\end_inset

Applied Predictive Modeling
\begin_inset Quotes erd
\end_inset

 does quite a nice job in discussing in more details how to deal with unbalanced
 data set.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
I also thought this chart is neat, because for the longest time I cannot
 remember the definitions of false +, false -, sensitivity, specificity:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/classification_metric_1.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/classification_metric_2.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
ROC Curve
\end_layout

\begin_layout Standard
Often, binary classification algorithm will not output the classification
 label directly.
 For example, in logistic regression, the output is a probability estimate
 
\begin_inset Formula $\hat{p}\left(Y=1\vert X\right)=f(X)$
\end_inset

, and it's our responsibility to choose the appropriate threshold 
\begin_inset Formula $t$
\end_inset

 such that for 
\begin_inset Formula $f(X)>t$
\end_inset

, assigned prediction to class 
\begin_inset Formula $1$
\end_inset

, and 
\begin_inset Formula $0$
\end_inset

 otherwise.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
The ROC curve is obtained by forming these predictions and computing the
 false positive and true positive rates for a range of values of 
\begin_inset Formula $t$
\end_inset

.
 This is a great visual tool that allows us to see the trade-off of true
 positive and false positive rates at a glance:
\end_layout

\begin_layout Itemize
As we increase 
\begin_inset Formula $t$
\end_inset

, we become more conservative, so false positive rate 
\begin_inset Formula $\downarrow$
\end_inset

, but at the same time, it also become harder for true positive to be classified
 as positive, so true positive rate 
\begin_inset Formula $\downarrow$
\end_inset

.
\end_layout

\begin_layout Itemize
As we decrease 
\begin_inset Formula $t$
\end_inset

, we become more careless, so false positive rate 
\begin_inset Formula $\uparrow$
\end_inset

, and at the same time, it also become much easier for true positive to
 be classified as positive, so true positive rate 
\begin_inset Formula $\uparrow$
\end_inset

.
\end_layout

\begin_layout Standard
The ROC curve not only allows us to see the trade-off of true positive rate
 and false positive for a SINGLE classification, it is also a great tool
 to compare the model performance of MULTIPLE binary classification algorithm.
 
\end_layout

\begin_layout Itemize
An optimal classifier will hug the top left corner of the ROC plot (i.e.
 high true positive rate, and very low false positive rate).
 
\end_layout

\begin_layout Itemize
A random classifier that does not take 
\begin_inset Formula $X$
\end_inset

 into account in making predictions for 
\begin_inset Formula $Y$
\end_inset

 will fall on the 45 degree line.
 Why? Here is the visual explanation (I drew myself):
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/rc8138/Desktop/ISL/graphs/random_classifier_45_degree_line_explanation.jpg
	scale 13
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
AUC (Area under the ROC curve)
\end_layout

\begin_layout Standard
Often, we might be interested in comparing the model performance of different
 candidates by comparing them on the basis of one single numerical metric,
 the AUC is one such metric.
 It is simply the area under the ROC curve.
 It alos has a particularly nice probablisitc and statistical interpretation.
\end_layout

\begin_layout Itemize

\series bold
Probablistic interpretation
\series default
: AUC is the probability that a classifier will rank a randomly chosen positive
 instance higher than a randomly chosen negative one (asssuming 'positive'
 rank higher than 'negative').
 i.e.
 
\begin_inset Formula $P(f(X_{\text{pos}})>f(X_{\text{neg}})\vert\left\{ X_{\text{pos}},Y=\text{pos}\right\} ,\left\{ X_{\text{neg}},Y=\text{neg}\right\} )$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Statistical Interpretation
\series default

\begin_inset Foot
status open

\begin_layout Plain Layout
Need to review what these tests are
\end_layout

\end_inset

: AUC is closely related to the 
\color red
Mann-Whitney U test
\color inherit
, which tests whether positives are ranked higher than negatives.
 It is also equivalent to the 
\color red
Wilcoxon test of ranks
\color inherit
.
\end_layout

\begin_layout Standard
Make sure the ROC & AUC comparison is done on the test set, not training
 set!
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Lingering Questions:
\end_layout

\begin_layout Standard
We talk about prospective & restrospective study (case-control campling)
 in this segment.
 
\end_layout

\begin_layout Itemize

\series bold
Prospective Study: 
\series default
In prospective study, we might follow 1000 individuals, for 20 years, and
 observe their risk factor, and eventually figure out if each one of them
 have heart diseases.
 This is a valid approach, but will take very long (and hence expensive).
\end_layout

\begin_layout Itemize

\series bold
Retrospective Study
\series default
: Also known as case-control sampling, is a good way to get around the above
 problem.
 We simply pick out all the patients who have heart diseases, and sample
 a control sample from people who don't have heart disease, and study them
 together.
 This is faster, and cheaper, and especially helpful when the positive class
 that you care about is very rare (CTR problems).
\end_layout

\begin_layout Standard

\color red
This approach will give us accurate estimate for the coefficient, except
 the intercept.
 We can always correct for the estimate for the intercept.
\color inherit

\begin_inset Foot
status open

\begin_layout Plain Layout
Slide 16/40 of class notes
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Finally, if you have unbalanced data set, in what ratio should one sample?
 Based on the diminishing return with respect to the reduction in the coefficien
t variance, the suggestion is 5:1
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/unbalanced_data_set_on_predictor_variance.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Chapter 5: Resampling Methods
\end_layout

\begin_layout Standard
In this section, we will discuss resampling methods.
 These methods refit a model of interest to samples formed from the training
 set itself, in order to obtain additional information about the fitted
 model.
 For example, they provide estimates of test-set prediction error, and the
 standard deviation and bias of our parameter estimates.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
We will introduce two widely used methods here:
\end_layout

\begin_layout Itemize

\series bold
Cross Validation
\series default
: This technique aims to get a more reliable and accurate estimate of the
 test error (
\shape italic
model assessment
\shape default
), or to select the appropriate level of flexibility (
\shape italic
model selection
\shape default
)
\end_layout

\begin_layout Itemize

\series bold
Boostrap Sampling
\series default
: This technique is used in several context, but most oftenly used to estimate
 the standard deviation and bias of the parameter estimate
\end_layout

\begin_layout Standard
In certain situations, we might use bootstrap to estimate CV error (e.g.
 OOB error in random forest), but it is often easier to use CV directly.
\end_layout

\begin_layout Subsection*
Cross Validation
\end_layout

\begin_layout Standard
Before introducing what cross validation is, it's important to draw distinction
 between the 
\shape italic
test error
\shape default
 and the 
\shape italic
training error.
 
\shape default
The test error is the average error that result from using a statistical
 learning method to predict the response of an unseen, new observation.
 In contrast, the training error can be easily calculated by applying the
 statistical learning method to the observations used in training.
 But the training error rate often is quite different from the test error,
 and can dramatically underestimate the test error.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/bias_variance_tradeoff_curve.png
	scale 30
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
As a result, we need to be very careful when estimating the test error to
 avoid false optimism.
 Some methods may include:
\end_layout

\begin_layout Itemize
Make a mathematical adjustment to the training error rate in order to better
 estimate test error rate.
 These include the Cp statistic, AIC, BIC, that take into account model
 complexity (i.e.
 bias-variance-trade-off).
\end_layout

\begin_layout Itemize
In this chapter, we will talk about estimating test error using heldout
 set.
\end_layout

\begin_layout Subsubsection*
Validation-Set Approach
\end_layout

\begin_layout Standard
The first approach is just validation - Take half of data for training,
 and take the other half for validation.
 We then train the model using the training set exclusively, and evaluate
 the test error using the validation test set only.
 The validation set approach is conceptually simple and easy to implement,
 but it has two potential drawbacks:
\end_layout

\begin_layout Itemize
As is shown in the right-hand panel below, the test error estimate is highly
 variable, since the calculation depends on precisely which observations
 are included in the training set and which observations are included in
 the validation set.
\begin_inset Foot
status open

\begin_layout Plain Layout
Textbook pg.
 177
\end_layout

\end_inset

 
\end_layout

\begin_layout Itemize
In the validation approach, only a subset of the observations—those that
 are included in the training set rather than in the validation set—are
 used to fit the model.
 Since statistical methods tend to per- form worse when trained on fewer
 observations, this suggests that the validation set error rate may tend
 to overestimate the test error rate for the model fit on the entire data
 set.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/validation_curve_high_variance.png
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\color red
I think these two particular drawbacks is CLOSELY related to what Caltech
 Learning from Data (Lecture 13 - validation, slide 16) is trying to convey,
 except in the Caltech course makes the trade-off a little bit more explicit.
 Basically, this approach sounds like the estimated test error using CV
 will be biased (overestimated) (point 2), and the variance of the estimate
 is going to be highly variable (point 1).
\end_layout

\begin_layout Subsubsection*
K-Fold Cross Validation
\end_layout

\begin_layout Standard
This is a widely used approach.
 The idea is to randomly divide the data into 
\begin_inset Formula $K$
\end_inset

 equal-sized parts.
 We leave out part 
\begin_inset Formula $k$
\end_inset

, fit the model to the other 
\begin_inset Formula $K-1$
\end_inset

 parts (combined), and then obtain predictions for the left-out 
\begin_inset Formula $k^{th}$
\end_inset

 part.
 This is done in turn for each part 
\begin_inset Formula $k=1,2,\dots K$
\end_inset

 and then the squared deviation of the prediction and the label are average
 to get the test error estimate.
 The details:
\end_layout

\begin_layout Itemize
Let the 
\begin_inset Formula $K$
\end_inset

 parts be 
\begin_inset Formula $C_{1},C_{2},\dots C_{K}$
\end_inset

 where 
\begin_inset Formula $C_{k}$
\end_inset

 denotes the indices of the observations in part 
\begin_inset Formula $k$
\end_inset

.
 There are 
\begin_inset Formula $n_{k}$
\end_inset

 observations in part 
\begin_inset Formula $k$
\end_inset


\end_layout

\begin_layout Itemize
Compute 
\begin_inset Formula \begin{eqnarray*}
CV_{(K)} & = & \sum_{k=1}^{K}\frac{n_{k}}{n}MSE_{k}\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
where 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none

\begin_inset Formula $MSE_{k}=\frac{1}{n_{k}}\sum_{i\in C_{k}}\left(y_{i}-\hat{y}_{i}\right)^{2}$
\end_inset

 , and 
\begin_inset Formula $\hat{y}_{i}$
\end_inset

 is the fit for observation 
\begin_inset Formula $i$
\end_inset

, obtained from the data with part 
\begin_inset Formula $k$
\end_inset

 removed.
 Setting 
\begin_inset Formula $K=n$
\end_inset

 yields 
\begin_inset Formula $n$
\end_inset

-fold or leave-one-out cross validation (LOOCV).
\end_layout

\end_deeper
\begin_layout Subsubsection*
A nice special case (LOOCV)
\begin_inset Foot
status open

\begin_layout Plain Layout
Ignoring the math details since it's just a special case of 
\begin_inset Formula $K$
\end_inset

-fold CV
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Now, the LOOCV approach could be computationally expensive, since we have
 to refit the model 
\begin_inset Formula $O(\text{number of samples})$
\end_inset

 times.
 Luckily, in the case of linear regression and polynomial regression, there
 is a convenient formula we can use to calculate LOOCV in one fit (using
 the original fit from the whole data set):
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
CV_{(n)} & = & \frac{1}{n}\sum_{i=1}^{n}\left(\frac{y_{i}-\hat{y}_{i}}{1-h_{i}}\right)^{2}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\hat{y}_{i}$
\end_inset

 is the 
\begin_inset Formula $i^{th}$
\end_inset

 fitted value from the original least squares fit, and 
\begin_inset Formula $h_{i}$
\end_inset

 is the leverage defined in earlier chapter.
 This makes sense, since points with high levarge, when removed, would have
 a huge impact on fit, and so the error on that point should be inflated.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Leave-one-out cross validation is closely related to the validation approach
 above, but it attempts to address that method's drawback:
\end_layout

\begin_layout Itemize
Like the validation set approach, LOOCV involves in splitting the data into
 2 parts.
 However, instead of creating two subsets of comparable size, the training
 sets are much larger (
\begin_inset Formula $n-1$
\end_inset

 training observations in each 
\begin_inset Formula $C_{k}$
\end_inset

).
 The bias is likely going to be smaller, because we used up 
\begin_inset Formula $\frac{n-1}{n}$
\end_inset

 fraction of the whole data set instead of only 
\begin_inset Formula $\frac{1}{2}$
\end_inset

.
 Consequently, the LOOCV approach tend not to overestimate the test error
 rate as much as validation approach.
\end_layout

\begin_layout Itemize
In contrast to the validation approach which will yield different results
 when applied repeatedly due to randomness in the training/validation split,
 performing LOOCV multiple times will always yield the same results: there
 is no randomness in the training/validation splits (for that particular
 data set).
\end_layout

\begin_layout Itemize
Typically, this approach doesn't shake up the data enough.
 The estimates from each fold are highly correlated and hence their average
 can have high variance!
\end_layout

\begin_layout Itemize

\color red
Here, although the bias is better, the variance is still high (that is,
 from training set to training set).
 Because 
\begin_inset Formula $Var(C_{(K)})=Var(\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2})$
\end_inset

, and since the particular training data for each 
\begin_inset Formula $k$
\end_inset

 overlap with each other substaintially, the covariance terms would inflate
 
\begin_inset Formula $Var(CV_{(K)})$
\end_inset

, which means that we will still have a wide CI, this implies that we will
 still be uncertain about the actual test error, since our overall goal
 is to pin that down.
\end_layout

\begin_layout Subsubsection*
Back to 
\begin_inset Formula $K$
\end_inset

-fold Cross Validation
\end_layout

\begin_layout Standard
\begin_inset Formula $K$
\end_inset

-fold Cross Validation is a common and attractive alternative to LOOCV,
 because of its computational efficiency.
 However, the advantage of 
\begin_inset Formula $K$
\end_inset

-fold CV is not only computational, there are other advantages involving
 bias-variance trade-off.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
When we divided up the data by 10-fold, we are still using 
\begin_inset Formula $\sim4/5$
\end_inset

 or 
\begin_inset Formula $9/10$
\end_inset

 of the data for training, so the bias is likely not to be overestimated.
 Also, when we compute the variance of 
\begin_inset Formula $CV$
\end_inset

, the data are shaken up enough that the covariance term wouldn't play a
 big role, so the variability of the estimate wouldn't as high compare to
 validation approach of LOOCV.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
When we perform cross-validation, our goal might be to determine how well
 a given statistical learning procedure can be expected to perform on independen
t data; in this case, the actual estimate of the test MSE is of interest.
 But at other times we are interested only in the location of the minimum
 point in the estimated test MSE curve.
 This is because we might be performing cross-validation on a number of
 statistical learning methods, or on a single method using different levels
 of flexibility, in order to identify the method that results in the lowest
 test error.
 In those cases, we might be off in terms of MSE, but 
\begin_inset Formula $K$
\end_inset

-fold CV can usually do a very good job in finding the minimum.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/LOOCV_kfold_cv_curve.png
	scale 35
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Two important points from this figure:
\end_layout

\begin_layout Itemize
LOOCV has no variability on that data set.
 All the data point is used for CV, so there is only 1 curve
\end_layout

\begin_layout Itemize
The variability for 10-fold CV test error is much smaller compared to the
 validation approach
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Bias-Variance-Trade-off in 
\begin_inset Formula $K$
\end_inset

-fold CV
\end_layout

\begin_layout Standard
To emphasize again, we mentioned that 
\begin_inset Formula $k$
\end_inset

-fold CV with 
\begin_inset Formula $k<n$
\end_inset

 has a computational advantage to LOOCV.
 But putting computational issues aside, a less obvious but potentially
 more important advantage of 
\begin_inset Formula $k$
\end_inset

-fold CV is that it often gives more accurate estimates of the test error
 rate than does LOOCV.
 This has to do with a bias-variance trade-off.
\end_layout

\begin_layout Subsubsection*
Bias
\end_layout

\begin_layout Itemize
The validation approach can lead to overestimates of the test error rate,
 since it only used half of the observations for training, compared to using
 the whole data set.
\end_layout

\begin_layout Itemize
In the case of LOOCV, we used 
\begin_inset Formula $n-1$
\end_inset

 for training, which is almost as many as the number of observations in
 the full data set.
\end_layout

\begin_layout Itemize
With 
\begin_inset Formula $k$
\end_inset

-fold CV where 
\begin_inset Formula $k=5,10$
\end_inset

 will, this leads to intermediate level of bias, since each training set
 contain 
\begin_inset Formula $\frac{K-1}{K}\times n$
\end_inset

 observations - fewer than LOOCV, but substantially more than in the validation
 set.
\end_layout

\begin_layout Standard
Therefore, from the perspective of bias reduction, it is clera that LOOCV
 is to be preferred to 
\begin_inset Formula $k$
\end_inset

-fold CV.
\end_layout

\begin_layout Subsubsection*
Variance
\end_layout

\begin_layout Standard
However, we know that bias is not hte only source for concern in an estimating
 procedure; we must also consider the procedure's variance.
\end_layout

\begin_layout Itemize
The validation approach has high variance in its procedure, because the
 test error estimate depends on the random split of which observations is
 in training, and which are in test set.
\end_layout

\begin_layout Itemize
It turns out that LOOCv has higher variance than does 
\begin_inset Formula $k$
\end_inset

-fold CV with 
\begin_inset Formula $k<n$
\end_inset

.
 Why? Although we are effectively averaging the prediction of 
\begin_inset Formula $n$
\end_inset

 fitted models (averaging reduce variance), each of the model trained are
 using almost an identical set of observations.
 Therefore, the ouput are highly (postively) correlated with each other.
 This can lead to higher variance to the procedure due to covariance terms.
\end_layout

\begin_layout Itemize
In the case of 
\begin_inset Formula $k$
\end_inset

-fold CV, we are averaging the outputs of 
\begin_inset Formula $k$
\end_inset

 fitted models that are somewhat less correlated with each other, since
 the overlap between training sets in each model is smaller.
\end_layout

\begin_layout Standard
To summarize, there is a bias-variance trade-off associated with the choice
 of 
\begin_inset Formula $k$
\end_inset

 in 
\begin_inset Formula $k$
\end_inset

-fold cross-validation.
 Choosing 
\begin_inset Formula $k=5$
\end_inset

 or 
\begin_inset Formula $10$
\end_inset

 empirically yield testg error rate estimates that suffer neither from excessive
ly high bias nor from very high variance.
\end_layout

\begin_layout Subsection*
\begin_inset Formula $K$
\end_inset

-fold Cross Validation for Classification
\end_layout

\begin_layout Standard
We divide the data into 
\begin_inset Formula $K$
\end_inset

 rouhgly equal-sized parts 
\begin_inset Formula $C_{1},C_{2},\dots C_{K}$
\end_inset

.
 
\begin_inset Formula $C_{k}$
\end_inset

 denotes the indices of the observations in part 
\begin_inset Formula $k$
\end_inset

.
 There are 
\begin_inset Formula $n_{k}$
\end_inset

 observations in part 
\begin_inset Formula $k$
\end_inset

: If 
\begin_inset Formula $n$
\end_inset

 is a multiple of 
\begin_inset Formula $K$
\end_inset

, then 
\begin_inset Formula $n_{k}=n/K$
\end_inset

.
 Compute
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
CV_{(K)} & = & \sum_{k=1}^{K}\frac{n_{k}}{n}Err_{k}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $Err_{k}=\frac{1}{n_{k}}\sum_{i\in C_{k}}I\left(y_{i}\neq\hat{y}_{i}\right)$
\end_inset

.
 The estimated standard deviation of 
\begin_inset Formula $CV_{K}$
\end_inset

 is
\end_layout

\begin_layout Standard
\begin_inset Formula $\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad$
\end_inset


\begin_inset Formula $\widehat{SE}\left(CV_{(K)}\right)=\sqrt{\frac{1}{K-1}\sum_{k=1}^{K}\left(Err_{k}-\overline{ERR_{k}}\right)^{2}}$
\end_inset


\end_layout

\begin_layout Subsection*
Right or Wrong Way of Doing Cross Validation
\begin_inset Foot
status open

\begin_layout Plain Layout

\color blue
The general lesson here -- We can use the labels to do feature selection,
 or use it to construct a separate classfier to form a new predictor.
 There is nothing wrong with these approaches, as long as the feature selection
 that use labels & construction of the classifier are only done using the
 labels from the training set AND the labels from the test set are not being
 used in this procedure before prediction!
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\series bold
(Feature selection)
\series default
 How to assess whether pre-filtering out predictors (this is in some sense
 feature selection) is an effective procedure for prediction -- See slide
 17 - 21 of class notes.
\end_layout

\begin_layout Itemize

\series bold
(Pre-validation)
\series default
 How to construct a new predictor based on training labels, and use that
 predictor for prediction, compared with some other benchmark predictors
 -- See slide 38 - 43 of class notes.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
The Bootstrap
\end_layout

\begin_layout Standard
The bootstrap is a flexible and powerful statistical tool that can be used
 to quantify the uncertainty associated with a given estimator or statistical
 learning method.
 For example, it can provide an estimate of the standard error of a coefficient,
 or a confidence interval for that coefficient.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
In an idealized world, if we would like to quantify the uncertainty of an
 estimator or statistical learning method, we would just get a huge number
 of data replica from the mother population, calculate the estimate, and
 see the variance/spread in order to understand the uncertainty.
 However, in real world, we often do not have access to the population,
 the boostrap approach allows us to use a computer to mimic the process
 of obtaining new data sets, so that we can estimate the variability of
 our estimate without generating additional samples.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Rather than repeatedly obtaining independent data sets from the population,
 we instead obtain dstinct data sets by repeatedly sampling observations
 from the original data set 
\series bold
with replacement
\series default
.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/bootstrap.png
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Sampling bootstrap samples can be tricky
\end_layout

\begin_layout Standard
When we just have samples that are 
\begin_inset Formula $i.i.d$
\end_inset

, sampling bootstrap samples are simple -- all we need to do is to sample
 from this data set with replacement.
 However, in cases where the data points are correlated with each other
 (say a time series), one needs to be more careful in sampling:
\end_layout

\begin_layout Itemize
One technique is called block sampling, where we create blocks of consecutive
 observations, and sample those with replacements.
 Then we paste together sampled blocks to obtain a bootstrap dataset.
 Each block are assume to be uncorrelated with each other.
\end_layout

\begin_layout Itemize

\color red
I am not entirely sure why we need to have 
\begin_inset Formula $i.i.d$
\end_inset

 samples for bootstraps -- Maybe it has to do with convergence (proof from
 STAT 200B, Berkeley?).
 I think another issue with non 
\begin_inset Formula $i.i.d$
\end_inset

 samples is that we would ended up with a sampling distribution that does
 not represent the true underlying population (e.g.
 only a small snapshot of the whole data sets), and the boostrap procedure,
 in that case, would be operating on samples that are all biased.
\end_layout

\begin_layout Subsubsection*
Obtaining Confidence Interval Using Bootstrap
\end_layout

\begin_layout Standard
The main utility of bootstrapping is to obtain standard errors of an estimate.
 It also give us a convenient way to get the approximate confidence interval
 (based on the sampling distribution).
 This method is called "Bootstrap percentile confidence interval", and is
 one of the simplest method to extract CI for the estimate using bootstrap
 samples.
 Remember, the interpretation for the Confidence Interval is still the same
 (frequentist):
\end_layout

\begin_layout Itemize
If we repeat the procedure many times, then about 90% of the time the random
 interval would contain the true parameter.
 However, for this particular interval, whether it contains the true parameter
 or not, we do not know.
\end_layout

\begin_layout Subsubsection*
Using Bootstrap to estimate prediction error?
\end_layout

\begin_layout Standard
In cross-validation, each of the 
\begin_inset Formula $K$
\end_inset

 validation folds is distinct from the other 
\begin_inset Formula $K-1$
\end_inset

 folds used for training: there is no overlap.
 This is crucial for its success.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Itemize
To estimate prediction error using the bootstrap, we could think about using
 each bootstrap dataset as our training sample, and the original sample
 as our validation smaple.
 But each bootstrap sample has significant overlap with the original data.
 About 
\begin_inset Formula $2/3$
\end_inset

 of the original data points appear in each bootstrap sample.
 This will cause the bootstrap to seriously underestimate the true prediction
 error.
\end_layout

\begin_layout Standard
There are fixes -- by only using predictions for those observations that
 did not (by chance) occur in the current bootstrap sample (this is what
 random forest and out-of-bag estimate is), but the method gets complicated,
 and in the end, CV provides a simpler, more attractive approach for estimating
 prediction error.
\end_layout

\begin_layout Itemize
The other way around -- with original sample = training sample, bootstrap
 dataset = validation is even worse! This is because all the test data are
 in the training set, and the learning procedure have already seen every
 observations.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Lingering Questions
\end_layout

\begin_layout Standard
I think by now, I have a pretty good grasp of the bias-variance trade-off
 for cross validation (i.e.
 whether to use validation approach, LOOCV, or 
\begin_inset Formula $k$
\end_inset

-fold CV).
 I will introduce some of the explanation and trend of thoughts from 
\begin_inset Quotes eld
\end_inset

Learning from Data
\begin_inset Quotes erd
\end_inset

 course (Caltech), specifically from lecture 13 (Validation) to reinforce
 the insight we learned here.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
The overall goal of validation is the following formula:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
\underbrace{E_{\text{out}}\left(h\right)}_{\text{validation estimate this quantity}} & = & E_{\text{in}}\left(h\right)+\text{overfit penalty}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
What we want, is to not only have a low 
\begin_inset Formula $E_{\text{out}}$
\end_inset

, but also have a very reliable estimate of 
\begin_inset Formula $E_{\text{out}}$
\end_inset

 so we know how well exactly we are doing!
\end_layout

\begin_layout Subsubsection*
Analyzing the estimate
\end_layout

\begin_layout Standard
One a single out-of-sample 
\begin_inset Formula $\left(\mathbf{x},y\right)$
\end_inset

, the error is 
\begin_inset Formula $e\left(h\left(\mathbf{x}\right),y\right)$
\end_inset

.
 When we have regression, we typically use the squared error 
\begin_inset Formula $\left(h\left(\mathbf{x}\right)-y\right)^{2}$
\end_inset

, and in the context of classification, we use the binary error 
\begin_inset Formula $\left[h\left(\mathbf{x}\right)\neq y\right]$
\end_inset

.
 More importantly, when we take the expected value (over 
\begin_inset Formula $X,Y$
\end_inset

?), we have:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
E\left[e\left(h\left(\mathbf{x}\right),y\right)\right] & = & E_{out}\left(h\right)\\
Var\left[e\left(h\left(\mathbf{x}\right),y\right)\right] & = & \sigma^{2}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Therefore, it's good news that the expected value of this error is an unbiased
 estimate of 
\begin_inset Formula $E_{out}$
\end_inset

.
 However, this is not enough, we need to make sure that the variance of
 this estimate is not highly variable.
 This is where we move from a single point to a set.
 On a validation set 
\begin_inset Formula $(x_{1},y_{1}),\cdots(x_{K},y_{K})$
\end_inset

, the error is 
\begin_inset Formula $E_{val}(h)=\frac{1}{K}\sum_{k=1}^{K}e(h(x_{k}),y_{,k})$
\end_inset

, which gives us 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
\mathbb{E}\left[E_{val}(h)\right] & = & \frac{1}{K}\sum_{k=1}^{K}\mathbb{E}\left[e(h(x_{k}),y_{k})\right]=E_{out}(h)\\
Var\left[E_{val}(h)\right] & = & \frac{1}{K^{2}}\sum_{k=1}^{K}Var\left[e(h(x_{k}),y_{k})\right]=\frac{\sigma^{2}}{K}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This means, under a validation set, we use 
\begin_inset Formula $E_{val}(h)$
\end_inset

 as an estimate for 
\begin_inset Formula $E_{out}(h)$
\end_inset

, and it is an unbiased estimator, where the variability of the estimate
 is determined by the size of the validation set 
\begin_inset Formula $K$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
E_{val}(h) & = & E_{out}(h)+O\left(\frac{1}{\sqrt{K}}\right)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
To summarize, the more samples we have in the validation set, the more accurate
 our estimate 
\begin_inset Formula $E_{val}$
\end_inset

 of 
\begin_inset Formula $E_{out}$
\end_inset

 would be!
\end_layout

\begin_layout Subsubsection*
But there is a cost
\end_layout

\begin_layout Standard
It's important to remember that we have limited data.
 The full data set has 
\begin_inset Formula $N$
\end_inset

 data points, and taking more points for validation means that we have less
 points for training 
\begin_inset Formula $(N-K)$
\end_inset

.
 
\begin_inset Formula $K$
\end_inset

 is taken out of 
\begin_inset Formula $N$
\end_inset

! This means that there is a cost associated with having a large validation
 set -- Remember from the learning curve, when the number of example is
 small, then the inherent 
\begin_inset Formula $E_{out}$
\end_inset

 is going to be high.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/training_validation_tradeoff_caltech.png
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We do not want to be an extreme situation where we have a large validation
 set, and only have 1 left for traning.
 In such case, we will be able to deliver a very good estimate of 
\begin_inset Formula $E_{out}$
\end_inset

, but the 
\begin_inset Formula $E_{out}$
\end_inset

 itself is going to be a terrible one.
 That is not what we want! We want both low 
\begin_inset Formula $E_{out}$
\end_inset

 and have its estimate to be reliable AT THE SAME TIME.
\end_layout

\begin_layout Subsubsection*
The dilemma about 
\begin_inset Formula $K$
\end_inset


\end_layout

\begin_layout Standard
The following chain of reasoning really pin down the bias-variance trade-off
 for Cross Validation:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/dilemma_of_k_caltech.png
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
We want 
\begin_inset Formula $K$
\end_inset

 to be small, because we want to make sure 
\begin_inset Formula $g^{-}$
\end_inset

 is not too far away from 
\begin_inset Formula $g$
\end_inset

, so that 
\begin_inset Formula $E_{out}(g^{-})\approx E_{out}(g)$
\end_inset


\end_layout

\begin_layout Itemize
We want 
\begin_inset Formula $K$
\end_inset

 to be large, because we want the estimate of 
\begin_inset Formula $E_{out}(g^{-})$
\end_inset

, which is 
\begin_inset Formula $E_{val}(g^{-})$
\end_inset

 to be a reliable estimate, so 
\begin_inset Formula $E_{out}(g^{-})\approx E_{val}(g)$
\end_inset


\end_layout

\begin_layout Standard
The overall goal, is to make 
\begin_inset Formula $E_{out}(g)$
\end_inset

 as small as possible, but using 
\begin_inset Formula $E_{val}(g^{-})$
\end_inset

 as our guiding principles to make the various learning choices.
 The trade-off is then described quite nicely under the 
\begin_inset Quotes eld
\end_inset

Bias-Variance Trade-off of 
\begin_inset Formula $K$
\end_inset

-fold CV
\begin_inset Quotes erd
\end_inset

 section.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Chapter 6: Linear Model Selection and Regularization
\end_layout

\begin_layout Standard
Recall the linear model
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
Y & = & \beta_{0}+\beta_{1}X_{1}+\dots+\beta_{p}X_{p}+\epsilon\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In the lectures that follow, we consider some approaches for extedning the
 linear model framework.
 In the lectures covering Chapter 7, we generalize the linear model in order
 to accommodate non-linear, but still additive, relationships.
 In Chapter 8, we consider even more general non-linear models.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Why might we want to use another fitting procedure instead of least squares?
 As we will see, alternative fitting procedures can yield better 
\shape italic
prediction accuracy
\shape default
 and 
\shape italic
model interpretability
\series bold
\shape default
.
\end_layout

\begin_layout Itemize

\series bold
\shape italic
\color blue
Prediction Accuracy
\series default
\shape default
\color inherit
: Provided that the true relationship is linear, then the least square procedure
 will have low bias.
 
\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $n\gg p$
\end_inset

, that is, if there are way more observations than the number of predictors,
 we would also have low variance
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $p$
\end_inset

 is comparable to 
\begin_inset Formula $n$
\end_inset

, then there can be a lot of variability in the least square fit, resulting
 in overfitting 
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $p>n$
\end_inset

, then there is no longer a unique least squares coefficient estimate: the
 variance is infinite
\end_layout

\begin_layout Standard
By constraining the coefficient, we will be able to control the variance
 part of the fitting procedure.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
\shape italic
\color blue
Model Interpretability
\series default
\shape default
\color inherit
: It is often the case that some or many of the variables used in a multiple
 regression are in fact not associated with the response.
 Including irrelevant variables leads to unnecessary complexity.
 By removing these variables, that is, by setting some of the coefficients
 to 
\begin_inset Formula $0$
\end_inset

, we can obtain a model that is more easily interpreted.
 We will consider feature selection or variable selection AND shrinkage
 methods.
\begin_inset Foot
status open

\begin_layout Plain Layout
Remember 
\begin_inset Formula $L_{1}$
\end_inset

 method shrink coefficients towards 0, which effectively is doing feature
 selection.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
There are many alternatives, in this chapter, we will discuss three importnat
 classes of methods:
\end_layout

\begin_layout Itemize

\series bold
\shape italic
\color magenta
Subset selection
\series default
\shape default
\color inherit
: This approach involves identifying a subset of the 
\begin_inset Formula $p$
\end_inset

 predictors that we believe to be related to the response.
 We then fit a model using least squares on the reduced set of variables.
\end_layout

\begin_layout Itemize

\series bold
\shape italic
\color magenta
Shrinkage
\series default
\shape default
\color inherit
: This approach involves fitting a model involving all 
\begin_inset Formula $p$
\end_inset

 predictors.
 However, the estimated coefficients are shrunken towards zero relative
 to the least squares estimates.
 This shrinkage (also known as regularization) has the effect of reducing
 variance.
 Depending on what type of shrinkage is performed, some of the coefficients
 may be estimated to be exactly zero.
 Hence, shrinkage methods can also perform variable selection.
\end_layout

\begin_layout Itemize

\series bold
\shape italic
\color magenta
Dimension reduction
\series default
\shape default
\color inherit
: This approach involves projecting the 
\begin_inset Formula $p$
\end_inset

 predictors into a 
\begin_inset Formula $M$
\end_inset

-dimensional subspace, where 
\begin_inset Formula $M<p$
\end_inset

.
 This is acheived by computing 
\begin_inset Formula $M$
\end_inset

 different linear combinations, or projections, of the original predictors.
 Then these 
\begin_inset Formula $M$
\end_inset

 projections are used as predictors to fit the linear regression model by
 least squares.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Subset Selection
\end_layout

\begin_layout Subsubsection*
Best subset selection
\end_layout

\begin_layout Standard
This can be considered as a brute force approach, where we consider all
 
\begin_inset Formula $2^{p}$
\end_inset

 possible models 
\begin_inset Formula $\left(\binom{p}{1}+\binom{n}{2}+\dots+\binom{p}{p}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/best_subset_selection.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The important thing to notice here is that the process is broken down to
 two parts (and the same pattern is true for stepwise regression as well):
\end_layout

\begin_layout Itemize
Part 1: For each choice of 
\begin_inset Formula $k$
\end_inset

, we apply the same procedure, we pick the BEST model in terms of smallest
 RSS / largest 
\begin_inset Formula $R^{2}$
\end_inset

.
 
\color red
My understanding here is that we are going after how many predictors we
 should use, not what is the exact set of predictors, given a fixed size
 
\begin_inset Formula $k$
\end_inset

.
 Therefore, as long as we standardize the fitting procedure (in this case,
 we choose the best model, based on RSS/
\begin_inset Formula $R^{2}$
\end_inset

 for each size), then it's a fair comparison.
\end_layout

\begin_layout Itemize
Part 2: Once we have pin down the representative for each size 
\begin_inset Formula $k$
\end_inset

, we select a single best model from among 
\begin_inset Formula $\mathcal{M}_{0},\dots\mathcal{M}_{p}$
\end_inset

 not based on RSS, but based on some estimate of the test error.
 After all, we are going after the test error, not training error.
 using RSS & 
\begin_inset Formula $R^{2}$
\end_inset

 would have been inappropriate because they monotonically decrease as we
 increase the number of predictors.
 And since training error almost always underestimate test error when we
 have a large number of predictors, it's important to use metrics such as
 
\begin_inset Formula $C_{p}$
\end_inset

, AIC, BIC, or adjusted 
\begin_inset Formula $R^{2}$
\end_inset

, those that do a much better job at estimating test error, to be the judge.
\end_layout

\begin_layout Standard
Although we have presetned best subset selection here for least squares
 regression, the same ideas apply to other type of models, such as logistic
 regression.
 However, instead of using RSS, we use deviance -- negative two times the
 maximized log-likelihood for a broader class of models.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
For computational reasons, best subset selection cannot be applied with
 very large 
\begin_inset Formula $p$
\end_inset

.
 
\color red
Best subset selection may also suffer from statistical problems when 
\begin_inset Formula $p$
\end_inset

 is large.
 The larger the search space, the higher the chance of finding models that
 look good on the training data, even though they might not have any predictive
 power on future data.
\begin_inset Foot
status open

\begin_layout Plain Layout

\color red
I don't understand, wouldn't we solve this problem if we always faithfully
 use test error instead?
\end_layout

\end_inset


\color inherit
 Thus an enormous search space can lead to overfitting and high variance
 of the coefficient estimates.
 For both of these reasons, stepwise methods, which explore a far more restricte
d set of models, are attractive alternatives to best subset selection.
\end_layout

\begin_layout Subsubsection*
Forward Stepwise Regression
\end_layout

\begin_layout Standard
Instead of searching through 
\begin_inset Formula $2^{p}$
\end_inset

 models, forward selection only search through 
\begin_inset Formula $\sum_{k=0}^{p-1}(p-k)=1+p(p+1)/2\sim O\left(\left(p^{2}\right)\right)$
\end_inset

 instead of 
\begin_inset Formula $O\left(2^{p}\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/forward_stepwise_regression.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Again, if we decompose the algorithm into two parts, the idea becomes apparent:
\end_layout

\begin_layout Itemize
Part I: Again, here we are trying to construct, for each size 
\begin_inset Formula $k$
\end_inset

, the best model.
 However, instead of what subset selection did, we choose the models (or
 best models for each size 
\begin_inset Formula $k$
\end_inset

) in a nested fashion.
 The searching is different, but the end result is the same, we pick the
 best model for each of the size 
\begin_inset Formula $k$
\end_inset

! That's what's important
\end_layout

\begin_layout Itemize
Part II: Again, once we have our 
\begin_inset Formula $\mathcal{M}_{0}\dots\mathcal{M}_{p}$
\end_inset

, we use estimates of test error to compare them among each others.
\end_layout

\begin_layout Standard
There is no guarantee that stepwise regression's selection of 
\begin_inset Formula $\mathcal{M}_{k}$
\end_inset

 would be the same as best subset's selection of 
\begin_inset Formula $\mathcal{M}_{k}$
\end_inset

.
 This could happen if 
\begin_inset Formula $\mathcal{M}_{k}$
\end_inset

 from best subset selection contains variable other than those in 
\begin_inset Formula $\mathcal{M}_{k-1}$
\end_inset

 from stepwise regression + the new variable introduced in step 
\begin_inset Formula $k$
\end_inset

.
 (See slide 12 as an example; Also see the plot that Rob plot).
\begin_inset Foot
status open

\begin_layout Plain Layout
Rob's comment: f there is no correlation between the predictors, it can
 be shown that best subset selection and stepwise selection would yield
 the same sequence of models.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Backward Stepwise Regression
\end_layout

\begin_layout Standard
The backward stepwise regression is very similar to forward stepwise procedure,
 but it still with a model that uses all the variables.
 It has the same computational advantage over best subset selection, but
 it also has no guarantee that it will be able to find the same 
\begin_inset Formula $\mathcal{M}_{k}$
\end_inset

 as best subset regression.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/backward_stepwise_regression.png
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Like forward stepwise selection, the backward selection approach searches
 through only 
\begin_inset Formula $1+p(p+1)/2$
\end_inset

 models, and so can be applied in settings where 
\begin_inset Formula $p$
\end_inset

 is too large.
 Also like forward stepwise selection, it performs a guided search over
 model space, and so effectively considers substaintially more than 
\begin_inset Formula $1+p(p+1)/2$
\end_inset

 models.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
There is one major drawback of backward selection that is not present at
 forward stepwise method.
 Since we need to start from the full model in backward stepwise method,
 we must need 
\begin_inset Formula $n>p$
\end_inset

, we cannot fit the full model otherwise.
 On the other hand, we can use the forward stepwise procedure even when
 
\begin_inset Formula $p>n$
\end_inset

, since we didn't use the full model to start with.
 Therefore, the only subset selection method that is feasible when 
\begin_inset Formula $p$
\end_inset

 large is forward stepwise selection.
\end_layout

\begin_layout Subsection*
Choosing the Optimal Model
\end_layout

\begin_layout Standard
As we discussed in the earlier section, the part II of the subset selection
 algorithm is important -- by picking the right estimate of test error,
 we will be able to determine which size 
\begin_inset Formula $k$
\end_inset

 is the most competitive in terms of test error.
 As we have alluded earlier, the training error can be a poor estimate of
 the test error.
 Therefore, RSS and 
\begin_inset Formula $R^{2}$
\end_inset

 are not suitable for selecting the best model among a collection of models
 with different numbers of predictors.
 There are two common approach to find good estimates of test error:
\end_layout

\begin_layout Itemize
We can indirectly estimate test error by making a mathematical adjustment
 to the training error to account for the bias-variance trade-off.
\end_layout

\begin_layout Itemize
We can directly estimate the test error, using either validation approach
 of cross validation approach, as discussed in Chapter 5.
\end_layout

\begin_layout Subsubsection*
Make mathematical adjustment to training errors
\end_layout

\begin_layout Paragraph*

\bar under
\begin_inset Formula $C_{p}$
\end_inset

- mallow's statistic:
\end_layout

\begin_layout Standard
For a fitted least squares modle containing 
\begin_inset Formula $d$
\end_inset

 predictors, the 
\begin_inset Formula $C_{p}$
\end_inset

 estimate of test MSE is computed using the equation
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
C_{p} & = & \frac{1}{n}\left(RSS+2d\hat{\sigma^{2}}\right)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\hat{\sigma^{2}}$
\end_inset

 is an estimate of the variance of the error 
\begin_inset Formula $\epsilon$
\end_inset

 associated with each response measurement.
 Essentially, the 
\begin_inset Formula $C_{p}$
\end_inset

 statistic takes into account model complexity and the variance introduced
 as we increase the number of variables introduced in the model.
 Here, we choose the model that gives us the lowest 
\begin_inset Formula $C_{p}$
\end_inset

 -- low 
\begin_inset Formula $RSS$
\end_inset

 and low penalty on complexity.
\end_layout

\begin_layout Paragraph*

\bar under
AIC:
\end_layout

\begin_layout Standard
The AIC criterion is defined for a large class of models fit by maximum
 likelihood.
 In the case of the model (6.1) with Gaussian errors, maximum likelihood
 and least squares are the same thing.
 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
AIC & = & \frac{1}{n\hat{\sigma^{2}}}\left(RSS+2d\hat{\sigma^{2}}\right)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Again, the lower the AIC, the better the model test error performance.
\end_layout

\begin_layout Paragraph*

\bar under
BIC:
\end_layout

\begin_layout Standard
Also very similar to the above two statistics, but it is more aggresive
 in penalizing models with bigger complexity.
 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
BIC & = & \frac{1}{n}\left(RSS+log(n)d\hat{\sigma^{2}}\right)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is because 
\begin_inset Formula $log(n)>2$
\end_inset

 for 
\begin_inset Formula $n>7$
\end_inset

.
 The basic idea is the same though, it's trying to balance fitting with
 complexity.
\end_layout

\begin_layout Paragraph*

\bar under
Adjusted 
\begin_inset Formula $R^{2}$
\end_inset

:
\end_layout

\begin_layout Standard
This one is slightly different from the above three, but it's very popular
 among non-statisticians who know 
\begin_inset Formula $R^{2}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
\text{Adjusted \ensuremath{R^{2}}} & = & 1-\frac{RSS/\left(n-d-1\right)}{TSS/\left(n-1\right)}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The intuition behind the adjusted 
\begin_inset Formula $R^{2}$
\end_inset

 is that once all of the correct variables have been included in the model,
 adding additional noise variables will lead to only a very small decrease
 in RSS.
 Since adding noise variables leads to an increase in d, such variables
 will lead to an increase in 
\begin_inset Formula $RSS/\left(n-d-1\right)$
\end_inset

.
 and consequently a decrease in the adjusted 
\begin_inset Formula $R^{2}$
\end_inset

.
 Therefore, in theory, the model with the largest adjusted 
\begin_inset Formula $R^{2}$
\end_inset

 will have only correct variables and no noise variables.
 Unlike the 
\begin_inset Formula $R^{2}$
\end_inset

 statistic, the adjusted 
\begin_inset Formula $R^{2}$
\end_inset

 statistic pays a price for the inclusion of unnecessary variables in the
 model.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
One important thing is that 
\begin_inset Formula $C_{p}$
\end_inset

, BIC, and Adjusted 
\begin_inset Formula $R^{2}$
\end_inset

 cannot be generalized to a broader class of models, since the concept of
 RSS do not exist.
 
\begin_inset Foot
status open

\begin_layout Plain Layout
Here we have presented the formulas for AIC, BIC, and 
\begin_inset Formula $C_{p}$
\end_inset

 in the case of a linear model fit using least squares; however, these quantitie
s can also be defined for more general types of models, and they have good
 theoretical justification based on study of asymtoptics.
 Adjusted 
\begin_inset Formula $R^{2}$
\end_inset

, on the other hand, has less theoretical support.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsubsection*
Validation Approach of Cross Validation
\end_layout

\begin_layout Standard
Each of the PART I, procedures in subset selections methods returns a sequence
 of model 
\begin_inset Formula $\mathcal{M}_{k}$
\end_inset

 indexed by model size 
\begin_inset Formula $k=0,1,2,\dots$
\end_inset

 Our job here is to select 
\begin_inset Formula $\hat{k}$
\end_inset

.
 Once selected, we will return model 
\begin_inset Formula $\mathcal{M}_{\hat{k}}$
\end_inset

 that gives us the best performance on test error.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
We compute the validation set error or the cross validation error for each
 model 
\begin_inset Formula $\mathcal{M}_{k}$
\end_inset

 under consideration, and then select 
\begin_inset Formula $k$
\end_inset

 for which the resulting estimated test error is smallest.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
This procedure has an advantage relative to AIC, BIC, 
\begin_inset Formula $C_{p}$
\end_inset

, and adjusted 
\begin_inset Formula $R^{2}$
\end_inset

, in that it provides a direct estimate of the test error, and doesn't require
 an estimate of the error variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

, which is not estimable when 
\begin_inset Formula $p>n$
\end_inset

 (when a saturated model is fitted because 
\begin_inset Formula $\hat{\sigma^{2}}=0)$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
It can also be used in a wider range of model selection tasks, even in case
 when it is hard to pinpoint the model degrees of freedom (e.g.
 the number of predictors in the model in the case of shrinkage) or hard
 to estimate 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Shrinkage / Regularization
\end_layout

\begin_layout Standard
The subset selection methods use least squares to fit a linear model that
 contains a subset of the predictors.
 As an alternative, we can fit a model containing 
\series bold
all
\series default
 
\begin_inset Formula $p$
\end_inset

 predictors using a technique that 
\shape italic
constrains
\shape default
 or 
\shape italic
regularizes
\shape default
 the coefficient estimates, or equivalently, that shrinks the coefficient
 estimates towards zero.
 Such technique would generally increase the bias component moderately,
 in exchange for a large(r) reduction in variance in the bias-variance-trade-off.
 The two best-known techniques for shrinking the regression coefficients
 towards zero are ridge regression and the lasso.
\end_layout

\begin_layout Subsubsection*
Ridge Regression
\end_layout

\begin_layout Standard
Recall that the least squares fitting procedure estimates 
\begin_inset Formula $\beta_{0},\beta_{1}\dots\beta_{p}$
\end_inset

 using the values that minimize:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
RSS & = & \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij}\right)^{2}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The aim here is to solely minimize the RSS.
 In contrast, the rige regression coefficient estimates 
\begin_inset Formula $\hat{\beta}^{R}$
\end_inset

 are the values that minimize
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula \[
{\color{blue}\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij}\right)^{2}}+{\color{magenta}\lambda\sum_{j=1}^{p}\beta_{j}^{2}=RSS+\lambda\sum_{j=1}^{p}\beta_{j}^{2}}\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\lambda\ge0$
\end_inset

 is a tuning parameter, to be determined separately.
 The optimization problem now juggles between two terms in its objective
 function: 1).
 RSS - or how good the model fit is, 2).
 the model complexity penalty, or whether the model is overly complex.
 
\begin_inset Formula $\lambda$
\end_inset

 here plays an important role in adjusting the bias-variance tradeoff:
\end_layout

\begin_layout Itemize
As 
\begin_inset Formula $\lambda\downarrow0$
\end_inset

, there are essentially no penalty in having large 
\begin_inset Formula $\beta$
\end_inset

 coefficients (or small for that matter).
 Therefore, the 
\begin_inset Formula $\beta$
\end_inset

 coefficients are not really being constrained, and it will work very hard
 to minimize the RSS to produce a good model fit.
 When 
\begin_inset Formula $\lambda=0$
\end_inset

, the fit is essentially the same as the least squares fit.
\end_layout

\begin_layout Itemize
As 
\begin_inset Formula $\lambda\uparrow\infty$
\end_inset

, the cost of choosing large 
\begin_inset Formula $\beta$
\end_inset

 coefficients becomes much more costly, and the coefficients are much more
 constrained.
 In this case, the model fit is likely to be poorer, but the model complexity
 is likely to be lower.
 At 
\begin_inset Formula $\lambda=\infty$
\end_inset

, the resulting model will be a null model with all coefficients equal 0.
\end_layout

\begin_layout Standard
The key concept here is that we do not just focus on the fit (having low
 bias), we also incorporate the penalty for having an overly complex model
 into the optimization problem! 
\begin_inset Formula $\lambda$
\end_inset

 does the job of balancing these trade-offs, and the best practice is to
 use Cross validation to select the right level of balance.
\begin_inset Foot
status open

\begin_layout Plain Layout
We want to shrink the estimated association of each variable with the response;
 however, we do not want to shrink the intercept coefficient 
\begin_inset Formula $\beta_{0}$
\end_inset

, which is simply a measure of the mean value of the response when all 
\begin_inset Formula $x$
\end_inset

's are 0.
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph*
Scaling the predictors:
\end_layout

\begin_layout Standard
The standard least squares coefficient estimates are 
\shape italic
\color black
scale invariant: 
\shape default
multiplying 
\begin_inset Formula $X_{j}$
\end_inset

 by a constant 
\begin_inset Formula $c$
\end_inset

 simply leads to a scaling of the least squares coefficient estimates by
 a factor of 
\begin_inset Formula $1/c$
\end_inset

.
 In other words, regardless of how the 
\begin_inset Formula $j^{th}$
\end_inset

 predictor is scaled, 
\begin_inset Formula $X_{j}\hat{\beta}_{j}$
\end_inset

 will remain the same.
 In contrast, the ridge regression coefficient estimates can change substantiall
y when multiplying a given predictor by a constant.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard

\color black
For instance, consider the income variable, which is measured in dollars.
 One could reasonably have measured income in thousands of dollars, which
 would result in a reduction in the observed values of income by a factor
 of 1,000.
 Now due to the sum of squared coefficients term in the ridge regression
 formulation, such a change in scale will not simply cause the ridge regression
 coefficient estimate for income to change by a factor of 1,000.
 In other words, 
\begin_inset Formula $X_{j}\hat{\beta}_{j,\lambda}^{R}$
\end_inset

 will depend not only on 
\begin_inset Formula $\lambda$
\end_inset

, but also on the scaling of the 
\begin_inset Formula $j^{th}$
\end_inset

 predictor.
 In fact, the value of 
\begin_inset Formula $X_{j}\hat{\beta}_{j,\lambda}^{R}$
\end_inset

 may even depend on the scaling of the other predictors! Therefore, it is
 best to apply ridge regression after standardizing the predictors, using
 the formula:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
\tilde{x}_{ij} & = & \frac{\tilde{x}_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left(x_{ij}-\bar{x}_{j}\right)^{2}}}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
so that they are all on the same scale.
\end_layout

\begin_layout Subsubsection*
LASSO
\end_layout

\begin_layout Standard
Ridge regression does have one obvious disadvantage: unlike subset selection,
 which will generally select models that involve just a subset of the variables
 (parsimony), ridge regression will include all 
\begin_inset Formula $p$
\end_inset

 predictors in the final model.
 This may not be a problem for prediction accuracy, but it can create a
 challenge in model interpretation in settings in which the number of variables
 
\begin_inset Formula $p$
\end_inset

 is quite large.
 The LASSO is a great alternative to ridge regression.
 The LASSO coefficients, 
\begin_inset Formula $\hat{\beta}^{L}$
\end_inset

, minimize the quantity:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula \[
{\color{blue}\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij}\right)^{2}}+{\color{magenta}\lambda\sum_{j=1}^{p}|\beta_{j}|}=RSS+{\color{magenta}\lambda\sum_{j=1}^{p}|\beta_{j}|}\]

\end_inset


\end_layout

\begin_layout Standard
As with ridge regression, the lasso shrinks the coefficient estimates towards
 zero.
 However, in the case of the lasso, the 
\begin_inset Formula $L_{1}$
\end_inset

 penalty has the effect of forcing some of the coefficient estimates to
 be exactly equal to 0 when the tuning parameter 
\begin_inset Formula $\lambda$
\end_inset

 is sufficiently large.
 Hence, much like best subset selection, the LASSO performs variable selection
 and encourages parsimony.
 We say that the LASSO yields spare, or parsimonious model.
 
\end_layout

\begin_layout Subsubsection*
Another Formulation for Best Subset Regression, Ridge Regression, and the
 LASSO
\end_layout

\begin_layout Standard
We can see the close connections of the three approaches mentioned above
 if we consider the problem
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
\min_{\beta}\left\{ \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij}\right)^{2}\right\}  & \text{subject to} & \sum_{j=1}^{p}|\beta_{j}|\leq s\quad OR\quad\sum_{j=1}^{p}\beta_{j}^{2}\leq s\quad OR\quad\sum_{j=1}^{p}I\left(\beta_{j}\neq0\right)\leq s\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
While all three problems aim to minimize RSS, the three optimization problem
 have different constraints.
 In the LASSO and ridge regression case, we are constraining the coefficient
 estimates using 
\begin_inset Formula $L_{1}$
\end_inset

 norm and 
\begin_inset Formula $L_{2}$
\end_inset

 norm respectively.
 In the case of best subset selection, we are subject to the constraint
 that no more than 
\begin_inset Formula $s$
\end_inset

 coefficients can be nonzero.
 Unfortuanately, solving the last optimization is computationally infeasible,
 since it requires considering all 
\begin_inset Formula $\binom{p}{s}$
\end_inset

 models containing 
\begin_inset Formula $s$
\end_inset

 predictors.
 Therefore, we can interpret ridge and LASSO as computationally feasible
 alternatives to best subset selectio nthat replace the intractable form
 of the budget with forms that are easier to solve! Of course, LASSO is
 more closely related to best subset selection since it actually perform
 feature selection.
\end_layout

\begin_layout Paragraph*
The variable selection property of LASSO:
\end_layout

\begin_layout Standard
Why is it that the LASSO, unlike ridge regression, results in coefficient
 estimates that are exactly equal to 0? The constrained optimization above
 can be used to shed light on the issue -- The key is that the feasible
 region of 
\begin_inset Formula $L_{1}$
\end_inset

 penalty has corners, but not for 
\begin_inset Formula $L_{2}$
\end_inset

.
 This means that the optimal solution 
\begin_inset Formula $\hat{\beta}^{L}$
\end_inset

 are likely to take place at corners, which corresponding to some coefficient
 estimates equal to 0! A picture in 
\begin_inset Formula $p=2$
\end_inset

 best illustrates this:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/variable_selection_property_of_LASSO.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Paragraph*
Selecting the tuning parameter 
\begin_inset Formula $\lambda$
\end_inset

:
\end_layout

\begin_layout Standard
As for subset selection, for ridge regression and LASSO we require a method
 to determine which of the models under consideration is best.
 That is, we require a method selecting a value for the tuning parameter
 
\begin_inset Formula $\lambda$
\end_inset

 or equivalently, the value of the constraint 
\begin_inset Formula $s$
\end_inset

.
 Cross Validation provides a simple way to tackle this problem.
 We choose a grid of 
\begin_inset Formula $\lambda$
\end_inset

 values, and compute the CV error for each value of 
\begin_inset Formula $\lambda$
\end_inset

.
 We then select the tuning parameter value for which the CV error is smallest.
 Finally, the model is re-fit using all of the available observation and
 the selected value of the tuning parameter.
\begin_inset Foot
status open

\begin_layout Plain Layout
This very last step/point is explained much more clearly in 
\begin_inset Quotes eld
\end_inset

Learning from Data
\begin_inset Quotes erd
\end_inset

 Caltech course, Lecture 13 
\begin_inset Quotes eld
\end_inset

Validation
\begin_inset Quotes erd
\end_inset

, notes slide 7 (
\begin_inset Formula $K$
\end_inset

 is put back to 
\begin_inset Formula $N$
\end_inset

).
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Dimension Reduction Methods
\end_layout

\begin_layout Standard
The methods we have discussed so far in this chapter have involved fitting
 linear regression models, via least squares or shruken approach, using
 
\series bold
ALL 
\series default
the original predictors, 
\begin_inset Formula $X_{1}\dots X_{p}$
\end_inset

.
 We now explore a class of approaches that 
\shape italic
transform 
\shape default
the predictors and then fit a least squares modle using the transformed
 variables.
 We will refer to these techniques as dimension reduction methods.
\end_layout

\begin_layout Subsubsection*
Dimension Reduction Methods: Details
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $Z_{1}\dots Z_{M}$
\end_inset

 represent 
\begin_inset Formula $M<p$
\end_inset

 linear combinations of our original 
\begin_inset Formula $p$
\end_inset

 predictors, That is
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
Z_{m} & = & \sum_{j=1}^{p}\phi_{mj}X_{j}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
for some constant 
\begin_inset Formula $\phi_{m1},\dots\phi_{mp}$
\end_inset

.
 We can then fit the linear regression model,
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
y_{i} & = & \theta_{0}+\sum_{m=1}^{M}\theta_{m}z_{im}+\epsilon_{i},\: i=1\dots n\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
using ordinary least squares.
 Note that the regression coefficients are given by 
\begin_inset Formula $\theta$
\end_inset

's.
 Also note that,
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
\sum_{m=1}^{M}\theta_{m}z_{im} & = & \sum_{m=1}^{M}\theta_{m}\sum_{j=1}^{p}\phi_{mj}x_{ij}=\sum_{j=1}^{p}\sum_{m=1}^{M}\theta_{m}\phi_{mj}x_{ij}=\sum_{j=1}^{p}\beta_{j}x_{ij}\\
\text{where}\quad\beta_{j} & = & \sum_{m=1}^{M}\theta_{m}\phi_{mj}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Hence, a dimension reduction least squares model can be thought of as a
 special case of the original linear regression model.
 Dimension reduction serves to constrain the estimated 
\begin_inset Formula $\beta_{j}$
\end_inset

 coefficients, since now they must satisfied the constraints above.
 This can help us to win in terms of bias-variance-trade-offs if 
\begin_inset Formula $\phi$
\end_inset

's are chosen appropriately.
 In this section, we talk about two ways of selecting 
\begin_inset Formula $Z_{m}$
\end_inset

's (or equivalently choosing 
\begin_inset Formula $\theta$
\end_inset

s).
\end_layout

\begin_layout Subsubsection*
Principal Component Regression (PCR, not PCA)
\end_layout

\begin_layout Standard
The PCR approach involves constructing the first 
\begin_inset Formula $M$
\end_inset

 principal components, 
\begin_inset Formula $Z_{1}\dots Z_{m}$
\end_inset

 and then using these components as the predictors in a linear regression
 model that is fit using least squares.
 The key idea is that often a small number of principal components suffice
 to explain most of the variability in the data, as well as the relationship
 with the response.
 In other words, 
\shape italic
we assume that the directions in which 
\begin_inset Formula $X_{1}\dots X_{p}$
\end_inset

 show the most variation are the directions that are associated with 
\begin_inset Formula $Y$
\end_inset

!
\shape default
 While this assumption is not guarantee to be true, it often turns out to
 be a reasonable enough approximation.
 To re-iterate, the steps for PCR are:
\end_layout

\begin_layout Itemize
Standardize 
\begin_inset Formula $X_{1}\dots X_{p}$
\end_inset

.
 Run PCA on the data matrix 
\begin_inset Formula $X$
\end_inset

 to get principal components 
\begin_inset Formula $Z_{m}$
\end_inset

's.
\end_layout

\begin_layout Itemize
Fit an least squares model using the 
\begin_inset Formula $M$
\end_inset

 principal components where 
\begin_inset Formula $M<p$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Notice that we are still able to recover the original coefficients 
\begin_inset Formula $\beta_{j}$
\end_inset

 because 
\begin_inset Formula $\beta_{j}=\sum_{m=1}^{M}\theta_{m}\phi_{mj}$
\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
It is generally recommended to standardize each original predictor before
 running PCA.
 In the absence of standardization, the high variance variables will tend
 to play a larger role in the principal components obtained, and the scale
 on which the variables are measured will ultimately have an effect on the
 final PCR model.
 The choice of 
\begin_inset Formula $M$
\end_inset

, the number of principal components, is determined via Cross Validation.
 When 
\begin_inset Formula $M=p$
\end_inset

, then we are back to the typical OLS model.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
We note that even though PCR provides a simple way to perform regression
 using 
\begin_inset Formula $M<p$
\end_inset

 predictors, it is NOT a feature selection method! This is because each
 of the 
\begin_inset Formula $M$
\end_inset

 principal components used in the regression is a linear combination of
 all 
\begin_inset Formula $p$
\end_inset

 of the original features.
 Therefore, while PCR often performs quite well in many practical settings,
 it does not result in a development of a model that relies upon a small
 set of the original features.
 In this sense, PCR is more closely related to ridge regression than to
 the LASSO.
 I nfact, one can show that PCR and ridge regression are very closely related.
 One can even think of ridge regression as a continuous version of PCR.
\end_layout

\begin_layout Subsubsection*
Partial Least Squares (PLS)
\end_layout

\begin_layout Standard
The PCR approach that we just described involves identifying linear combinations
, or directions, that best represent the predictors 
\begin_inset Formula $X_{1}\dots X_{p}$
\end_inset

.
 These directions are identified in an 
\series bold
unsupervised
\series default
 way, since the response 
\begin_inset Formula $Y$
\end_inset

 is not used to help determine the principal component directions.
 That is, the response does not supervise the identification of the principal
 components.
 Consequently, PCR suffers from a drawback: there is no guarantee that the
 directions that best explain the predictors will also be the best directions
 to use for predicting the response.
 PLS aims to address this issue.
 The steps for PLS are:
\end_layout

\begin_layout Itemize
Standardize 
\begin_inset Formula $X_{1}\dots X_{p}$
\end_inset

.
 Run PLS on the data matrix 
\begin_inset Formula $X$
\end_inset

 to get principal components 
\begin_inset Formula $Z_{m}$
\end_inset

s
\end_layout

\begin_layout Itemize
Fit a least squares model using the 
\begin_inset Formula $M$
\end_inset

 PLS directions
\end_layout

\begin_deeper
\begin_layout Standard
PLS procedure:
\end_layout

\begin_layout Itemize
Compute the first direction 
\begin_inset Formula $Z_{1}$
\end_inset

 by setting each 
\begin_inset Formula $\phi_{j1}$
\end_inset

 equal to the coefficient from the simple linear regression of 
\begin_inset Formula $Y$
\end_inset

 onto 
\begin_inset Formula $X_{j}$
\end_inset

.
 This allows us to give more weights to predictors that are more correlated
 with the response
\end_layout

\begin_layout Itemize
Once we obtained 
\begin_inset Formula $Z_{1}=\sum_{j=1}^{p}\phi_{j1}X_{j}$
\end_inset

, we then adjust each of the variables for 
\begin_inset Formula $Z_{1}$
\end_inset

.
 Specifically, we regress each variable on 
\begin_inset Formula $Z_{1}$
\end_inset

 and take the residuals.
 These residuals can be interpret as the remaining information that has
 not been explained by the first PLS direction.
 We then compute 
\begin_inset Formula $Z_{2}$
\end_inset

 using this orthogonalized data in exactly the same fashion as 
\begin_inset Formula $Z_{1}$
\end_inset

 was computed based on the original data.
 This means we will compute 
\begin_inset Formula $\phi_{j2}$
\end_inset

 equal to the coefficient from the simple linear regression of 
\begin_inset Formula $Y$
\end_inset

 onto 
\begin_inset Formula $Res(X_{j}\sim Z_{1})$
\end_inset

.
\end_layout

\begin_layout Itemize

\color red
For 
\begin_inset Formula $Z_{k}$
\end_inset

, we compute 
\begin_inset Formula $\phi_{jk}$
\end_inset

 equal to the coefficient from the simple linear regression of 
\begin_inset Formula $Y$
\end_inset

 onto 
\begin_inset Formula $Res(X_{j}\sim(Z_{1}+\dots Z_{k-1}))$
\end_inset

?
\end_layout

\begin_layout Itemize
This iterative approach can be repeated 
\begin_inset Formula $M$
\end_inset

 times to identify multiple PLS directions for a least squares fit.
\end_layout

\end_deeper
\begin_layout Standard
Rob said that generally he found Ridge regression and PCR to be comparable
 or better than PLS, there is no clear evidence that PLS is more superior.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Lingering Questions
\end_layout

\begin_layout Subsubsection*
What is the right mental model for thinking about subset selection (Use
 RSS/
\begin_inset Formula $R^{2}$
\end_inset

 in part I, and test error for part II)?
\end_layout

\begin_layout Standard
The way I think about subset selection is a two part process:
\end_layout

\begin_layout Itemize
Part 1: For each choice of 
\begin_inset Formula $k$
\end_inset

, we apply the same procedure to arrive at that model for that size.
 The relationship across models might be {unrelated, nested...etc}, but the
 important thing is that they are selected the same way (i.e.
 for each size, pick the model with the smallest RSS).
 
\color black
My understanding here is that we are going after how many predictors we
 should use, not what is the exact set of predictors, given a fixed size
 
\begin_inset Formula $k$
\end_inset

.
 Therefore, as long as we standardize the fitting procedure (in this case,
 we choose the best model, based on RSS/
\begin_inset Formula $R^{2}$
\end_inset

 for each size), then it's a fair comparison.
\end_layout

\begin_layout Itemize
Part 2: Once we have pin down the representative for each size 
\begin_inset Formula $k$
\end_inset

, we select a single best model from among 
\begin_inset Formula $\mathcal{M}_{0},\dots\mathcal{M}_{p}$
\end_inset

 not based on RSS, but based on some estimate of the test error.
 After all, we are going after the test error, not training error.
 using RSS & 
\begin_inset Formula $R^{2}$
\end_inset

 would have been inappropriate because they monotonically decrease as we
 increase the number of predictors.
 And since training error almost always underestimate test error when we
 have a large number of predictors, it's important to use metrics such as
 
\begin_inset Formula $C_{p}$
\end_inset

, AIC, BIC, or adjusted 
\begin_inset Formula $R^{2}$
\end_inset

, those that do a much better job at estimating test error, to be the judge.
\end_layout

\begin_layout Standard
Is this way of thinking correct?
\end_layout

\begin_layout Subsubsection*
How would you use validation and/or cross validation for subset selection
 exactly?
\end_layout

\begin_layout Paragraph*
Validation:
\end_layout

\begin_layout Standard
In the case of validation approach, we will break up data into a single
 training set and a single validation set.
 We would use the training set to build 
\begin_inset Formula $\mathcal{M}_{1}\dots\mathcal{M}_{p}$
\end_inset

, without referencing the validation at all.
 Once we have our 
\begin_inset Formula $p$
\end_inset

 best models 
\begin_inset Formula $\mathcal{M}$
\end_inset

, we would then apply them to the same validation set, and compute the estimated
 test error, and pick 
\begin_inset Formula $k$
\end_inset

 such that 
\begin_inset Formula $\mathcal{M}_{k}$
\end_inset

 has the lowest test error, say denoted 
\begin_inset Formula $err_{test}(\mathcal{M}_{k})\:\forall k$
\end_inset

.
\end_layout

\begin_layout Paragraph*
Cross-Validation:
\end_layout

\begin_layout Standard
It's very similar to the validation case, but we repeat the process for
 different pairs of (training, validation) sets, for 
\begin_inset Formula $K=5,10$
\end_inset

.
 In the end of each pair of (training, validation) set, we would arrive
 at 
\begin_inset Formula $err_{test}(\mathcal{M}_{j})^{(l)}$
\end_inset

, to find the best model, we would simply compare
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\frac{1}{K}\sum_{l=1}^{K}err_{test}(\mathcal{M}_{1})^{(l)},\frac{1}{K}\sum_{l=1}^{K}err_{test}(\mathcal{M}_{2})^{(l)},\dots\frac{1}{K}\sum_{l=1}^{K}err_{test}(\mathcal{M}_{p})^{(l)}\]

\end_inset


\end_layout

\begin_layout Standard
and pick the model with lowest cross validation (average) error.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
The important thing here is that for a fixed size 
\begin_inset Formula $\bar{k}$
\end_inset

, the 
\begin_inset Formula $\mathcal{M}_{\bar{k}}^{(l)}$
\end_inset

 for different 
\begin_inset Formula $l$
\end_inset

 might yield a different size 
\begin_inset Formula $\bar{k}$
\end_inset

.
 In other words, even when 
\begin_inset Formula $\bar{k}$
\end_inset

 is fixed, because we are using different replica of full data for training,
 we will yield differet model model (i.e.
 different set of 
\begin_inset Formula $\bar{k}$
\end_inset

 predictors are being chosen), despite they all have 
\begin_inset Formula $\bar{k}$
\end_inset

 predictors in the model.
 I think this is O.K, and it again emphasize the importance that we are model
 selecting for the hyper-parameter 
\begin_inset Formula $k$
\end_inset

 (what is the right 
\begin_inset Formula $k$
\end_inset

), not what exact 
\series bold
set of predictors we should choose!
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Chapter 7: Moving Beyond Linearity
\end_layout

\begin_layout Standard
Up until Chapter 6, we have discussed linear models and variants of them.
 From the basic simple linear model, multiple regression, logistic regression,
 to shrunkage methods such as Ridge regression, LASSO, PCR & PLS, we have
 learned a rich set of modeling techniques.
 However, we are still fitting linear models, which can only be improved
 so far! In this chapter, we relax the linearity assumption while still
 attempting to maintain as much as interpretability as possible.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
We can view this chapter as a transitional chapter from linear models to
 non-linear models (like trees in Chapter 8).
 Here are the list of techniques we will study:
\end_layout

\begin_layout Itemize

\shape italic
Polynomial Regression
\shape default
: It extends the linear model by adding extra predictors, obtained by raising
 each of the original predictors to a power.
 For example, a cubic regression uses three variables, 
\begin_inset Formula $X,\: X^{2},\: X^{3}$
\end_inset

, as predictors.
 This approach provides a simple way to provide a nonlinear fit to data.
\end_layout

\begin_layout Itemize

\shape italic
Step Functions
\shape default
: It cuts the range of the variable into 
\begin_inset Formula $K$
\end_inset

 distinct regions in order to produce a qualitative (factor) variable.
 This has the effect of fitting a piecewise constant function.
\end_layout

\begin_layout Itemize

\shape italic
Regression Splines
\shape default
: These are more flexible than polynomials and step functions, and in fact
 are an extension of the two.
 They involve dividing the range of 
\begin_inset Formula $X$
\end_inset

 into 
\begin_inset Formula $K$
\end_inset

 regions.
 Within each region, a polynomial function is fit to the data.
 However, these piecewise polynomials are constrained so that they join
 smoothly at the region boundaries, or Knots.
 Provided that the interval is divided into enough regions, this can produce
 an extremely flexible fit.
\end_layout

\begin_layout Itemize

\shape italic
Smoothing Splines
\shape default
: These are similar to regression splines, but arise in a slightly different
 formulation.
 Smoothing splines result from minimizing a residual sum of squares of the
 fit subject to a smoothness penalty.
 
\end_layout

\begin_layout Itemize

\shape italic
Local Regression
\shape default
: similar to splines, but differs in an important way that the regions are
 allowed to overlap, and indeed they do so in a very smooth way.
\end_layout

\begin_layout Itemize

\shape italic
Generalized Additive Model (GAM)
\shape default
: It allows us to extend the methods above methods (mostly used for single
 predictor) to deal with multiple predictors!
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Polynomial Regression
\end_layout

\begin_layout Standard
One way to extend linear regression to settings in which the relationship
 between the predictors and the response is nonlinear has been to replace
 the standard linear model with a polynomial function
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\left(y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\right)\Longrightarrow\left(y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}\dots+\beta_{d}x_{i}^{d}+\epsilon_{i}\right)\]

\end_inset


\end_layout

\begin_layout Standard
Essentially, we create new predictor by taking the powers of the original
 single predictors 
\begin_inset Formula $X_{1}=X,\: X_{2}=X^{2}$
\end_inset

, etc.
 In this scenario, 
\color blue
we are often not interested in the coefficients.
 We are more interested in the fitted values 
\begin_inset Formula ${\color{black}\hat{f}(x_{0})=\hat{\beta_{0}}+\hat{\beta_{1}}x_{0}+\hat{\beta_{2}}x_{0}^{2}\dots+\hat{\beta_{d}}x_{0}^{d}+\epsilon_{i}}$
\end_inset

 
\end_layout

\begin_layout Itemize
We either fix the degree 
\begin_inset Formula $d$
\end_inset

 at some reasonably low value, else use corss-validation to choose 
\begin_inset Formula $d$
\end_inset

.
\end_layout

\begin_layout Itemize
Since 
\begin_inset Formula $\hat{f}(x_{0})$
\end_inset

 is a linear function of the 
\begin_inset Formula $\hat{\beta}_{l}$
\end_inset

, we can get a simple expression for pointwise-variances 
\begin_inset Formula $Var\left[\hat{f}\left(x_{0}\right)\right]$
\end_inset

 at any value 
\begin_inset Formula $x_{0}$
\end_inset

.
 If 
\begin_inset Formula $\hat{\mathbf{C}}$
\end_inset

 is the 
\begin_inset Formula $(d+1)\times(d+1)$
\end_inset

 covariance matrix of 
\begin_inset Formula $\hat{\beta}_{j}$
\end_inset

, and if 
\begin_inset Formula $l_{0}^{T}=\left(1,x_{0},x_{0}^{2},\dots x_{0}^{d}\right)$
\end_inset

, then 
\begin_inset Formula $Var\left[\hat{f}\left(x_{0}\right)\right]=l_{0}^{T}\hat{\mathbf{C}}l_{0}$
\end_inset

.
 This allows us to estimate the variability of the estimate using 
\begin_inset Formula $\hat{f}(x_{0})\pm2\times se\left[\hat{f}(x_{0})\right]$
\end_inset

 as confidence band.
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
This extension also applies to logistic regression.
 For example,
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
P\left(y_{i}>250|x_{i}\right) & = & \frac{exp(\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+\dots+\beta_{d}x_{i}^{d})}{1+exp(\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+\dots+\beta_{d}x_{i}^{d})}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize
To get the confidence interval, compute upper and lower bounds on on the
 logit scale, and then invert to get on probability scale.
\end_layout

\begin_layout Standard
More generally
\begin_inset Foot
status open

\begin_layout Plain Layout
Can fit using 
\begin_inset Formula $y\sim poly(x,degree=3)$
\end_inset

 in R
\end_layout

\end_inset

,
\end_layout

\begin_layout Itemize
We can do this separately on several variables - just stack the variables
 into one matrix, and separate out the pieces afterwards.
\end_layout

\begin_layout Itemize
Caveat: Polynomial have notorious tail behavior -- meaning the confidence
 band is very wiggling at the tails.
 It doesn't a bad job at extrapolation.
\end_layout

\begin_layout Subsection*
Step Functions
\end_layout

\begin_layout Standard
Using polynomial functions of the features as predictors in a linear model
 imposes a global structure on the non-linear function of 
\begin_inset Formula $X$
\end_inset

.
 We can instead use step functions in order to avoid imposing such a global
 structure.
 Here we break the range of 
\begin_inset Formula $X$
\end_inset

 into bins, and 
\color blue
fit a different constant in each bin
\color inherit
.
 This amounts to converting a continuous variable into an ordered categorical
 variable.
 In greater details, we create cutpoints 
\begin_inset Formula $c_{1},c_{2}\dots c_{K}$
\end_inset

 in the range of 
\begin_inset Formula $X$
\end_inset

, and then construct 
\begin_inset Formula $K+1$
\end_inset

 new variables
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
C_{0}(X) & = & I\left(X<c_{1}\right)\\
C_{1}\left(X\right) & = & I\left(c_{1}\leq X<c_{2}\right)\\
 & \vdots\\
C_{K-1}(X) & = & I\left(c_{K-1}\leq X<c_{K}\right)\\
C_{K}(X) & = & I\left(c_{K}\leq X\right)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We then use least squares to fit a linear model using 
\begin_inset Formula $C_{1}(X),C_{2}(X)\dots C_{K}(X)$
\end_inset

 as predictors:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
y_{i} & = & \beta_{0}+\beta_{1}C_{1}(x_{i})+\beta_{2}C_{2}(x_{i})+\dots+\beta_{K}C_{K}(x_{i})+\epsilon_{i}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We exclude 
\begin_inset Formula $C_{0}(X)$
\end_inset

 as a predictor because it is redundant with the intercept.
 This is similar to the fact that we need only two dummy variables to code
 a qualitative variable with three levels.
 To explain, when 
\begin_inset Formula $X<c_{1}$
\end_inset

, all of the predictors are zero, so 
\begin_inset Formula $\beta_{0}$
\end_inset

 can be interpreted as the mean value of 
\begin_inset Formula $Y$
\end_inset

 for 
\begin_inset Formula $X<c_{1}$
\end_inset

.
 By comparison, the function above predict a response of 
\begin_inset Formula $\beta_{0}+\beta_{j}$
\end_inset

 for 
\begin_inset Formula $c_{j}\leq X<c_{j+1}$
\end_inset

, so 
\begin_inset Formula $\beta_{j}$
\end_inset

 represents the average increase in the response for 
\begin_inset Formula $X$
\end_inset

 in 
\begin_inset Formula $c_{j}\leq X<c_{j+1}$
\end_inset

 relative to 
\begin_inset Formula $X<c_{1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Again, the same technique can be apply to logistic regression
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
P\left(y_{i}>250|x_{i}\right) & = & \frac{exp\left(\beta_{0}+\beta_{1}C_{1}(x_{i})+\beta_{2}C_{2}(x_{i})+\dots+\beta_{K}C_{K}(x_{i})\right)}{1+exp\left(\beta_{0}+\beta_{1}C_{1}(x_{i})+\beta_{2}C_{2}(x_{i})+\dots+\beta_{K}C_{K}(x_{i})\right)}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Step functions are useful for creating interactions that are easy to interpret.
 For example, interaction effect of Year and Age 
\end_layout

\begin_layout Standard
\begin_inset Formula \[
I\left(\text{Year}\right)<2005)\times\text{Age},\: I(\text{Year}\geq2005)\times\text{Age}\]

\end_inset


\end_layout

\begin_layout Standard
would allow for different linear function in each Year category.
 Choice of cutpoints or knots can be problematic, because there is no principle
 ways of choosing them.
 Nevertheless, this approach remain popular in biostatistics and epidemiology,
 among other disciplines.
 For example, 5-year age groups are often used to define the bins.
\end_layout

\begin_layout Subsection*

\color blue
Basis Function
\end_layout

\begin_layout Standard
Polynomial and piecewise-constant regression models are in fact two special
 cases of a 
\color blue
basis function
\color inherit
 approach.
 The idea is to have at hand a family of functions or transformation that
 can be applied to a variable 
\begin_inset Formula $X$
\end_inset

: 
\begin_inset Formula $b_{1}(X),\: b_{2}(X),\:\dots,\: b_{K}(X)$
\end_inset

.
 Instead of fitting a linear model of 
\begin_inset Formula $X$
\end_inset

, we fit the model
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
y_{i} & = & \beta_{0}+\beta_{1}b_{1}(x_{i})+\beta_{2}b_{2}(X)+\dots+\beta_{K}b_{K}(X)+\epsilon_{i}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note that the basis function 
\begin_inset Formula $b_{1}\left(\cdot\right),\: b_{2}\left(\cdot\right),\dots,\: b_{K}\left(\cdot\right)$
\end_inset

 are fixed and known.
 For polynomial regression, the basis functions are 
\begin_inset Formula $b_{j}(x_{i})=x_{i}^{j}$
\end_inset

, and for piecewise constant functions they are 
\begin_inset Formula $b_{j}(x_{i})=I\left(c_{j}\leq x_{i}<c_{j+1}\right)$
\end_inset

.
 Ordinary regression simply have 
\begin_inset Formula $b_{j}(x_{i})=x_{i}$
\end_inset

.
 Hence, we can use least squares to estimate the unknown regression coefficients
 above.
 Importantly, this means that all of the inference tools for linear models,
 such as standard errors for the coefficients and 
\begin_inset Formula $F$
\end_inset

-statistics for the model's overall significance, are available in this
 setting.
 There are many other alternatives for the basis functions, such as wavelets
 or Fourier series.
 In the next section, we investigate a very common choice for a basis function:
 regression splines.
\end_layout

\begin_layout Subsection*
Regression Splines
\end_layout

\begin_layout Standard
The criticism of fitting a global polynomial is that data point in one end
 might affect the fit on the other end -- The fit is SMOOTH but is not LOCAL.
 The criticism of step function is that it is LOCAL, but not SMOOTH (step
 functions might not take the same values).
 Regression splines is an extension of the above two methods in which we
 fit local splines in each of the local region, and at the same time, enforce
 constraints on the boundaries/knots so that the fit is smooth.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
We can first start by fitting piecewise polynomial (LOCAL, but not necessarily
 smooth) in the form:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
y_{i} & = & \begin{cases}
\beta_{01}+\beta_{11}x_{i}+\beta_{21}x_{i}^{2}+\beta_{31}x_{i}^{3} & \text{if}\: x_{i}<c;\\
\beta_{02}+\beta_{12}x_{i}+\beta_{22}x_{i}^{2}+\beta_{32}x_{i}^{3} & \text{if}\: x_{i}\geq c.\end{cases}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In each region, we fit a polynomial function (as we describe earlier).
 In the case of step function, we are simply fitting a piece-wise polynomial
 of degree 
\begin_inset Formula $d=0$
\end_inset

! The issue with this technique is that there is no gaurantee that, at the
 knots, the fit will be smooth.
 Therefore, we often want to impose additional constraints on the fit at
 the knots.
 This is where we introduce splines.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
It turns out that we can use the basis modle to represent a regression spline.
 The typical choices are linear and cubic splines.
 A cubic spline with 
\begin_inset Formula $K$
\end_inset

 knotes can be modeled as 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
y_{i} & = & \beta_{0}+\beta_{1}b_{1}(x_{i})+\beta_{2}b_{2}(X)+\dots+\beta_{K+3}b_{K+3}(X)+\epsilon_{i}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
for an appropriate choice of basis functions 
\begin_inset Formula $b_{1},b_{2}\dots b_{K+3}$
\end_inset

.
 Just as there are several ways to represent polynomials, there are many
 equivalent ways to represent cubic splines using different choices of basis
 function.
 The most direct way to represent a cubic spline is to start off with a
 basis for a cubic polynomial - namely, 
\begin_inset Formula $x,\: x^{2},\: x^{3}$
\end_inset

, and then add one truncated power basis function per knot:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
h\left(x,\zeta\right) & = & \left(x-\zeta\right)_{+}^{3}=\begin{cases}
\left(x-\zeta\right)^{3} & \text{if}\: x>\zeta\\
0 & \text{otherwise}\end{cases}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\zeta$
\end_inset

is the knot.
\begin_inset Foot
status open

\begin_layout Plain Layout
The definition for linear spline can be defined similarly.
 See lecture slide 9/23 - 12/23.
\end_layout

\end_inset

 In other words, in order to fit a cubic spline to a data set with 
\begin_inset Formula $K$
\end_inset

 knots, we perform least squares regression with an intercept and 
\begin_inset Formula $3+K$
\end_inset

 predictors, of the form 
\begin_inset Formula $X,\: X^{2},\: X^{3},\: h(X,\zeta_{1}),\: h(X,\zeta_{2})\dots h(X,\zeta_{K})$
\end_inset

 where 
\begin_inset Formula $\zeta_{1},\zeta_{2},\dots\zeta_{K}$
\end_inset

 are the knots.
 This amounts to estimating a total of 
\begin_inset Formula $K+4$
\end_inset

 regression coefficients.
 Again, why are we doing this? This is because we want SMOOTHNESS.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
The general definition of a degree-
\begin_inset Formula $d$
\end_inset

 spline is that it is a piecewise degree-
\begin_inset Formula $d$
\end_inset

 polynomial, with continuity in derivatives up to degree 
\begin_inset Formula $d-1$
\end_inset

 at each knot.
 Therefore:
\end_layout

\begin_layout Itemize
A cubic spline is a piecewise degree
\begin_inset Formula $-3$
\end_inset

 polynomial, with the first and second derivatives to be continuous at each
 knot (This means that we will not see kinks, because even the derivatives
 are smooth).
\end_layout

\begin_layout Itemize
A linear spline is a piecewise degree
\begin_inset Formula $-1$
\end_inset

 (linear) polynomial, with the 
\begin_inset Formula $0^{th}$
\end_inset

 derivative (meaning itself) being continuous.
\end_layout

\begin_layout Standard
Generally, regression splines also have bad behaviors at the tails, and
 Natural Cubic Splines improves this by extrapolates linearly beyond the
 boundary knots.
\begin_inset Foot
status open

\begin_layout Plain Layout
Use bs(x, ...) and ns(x, ...) for splines and natural cubic splines in R.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/spline_comparison.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Itemize
In the top left, we see that we are simply fitting a piecewise cubic polynomial
 function.
 It is adaptive in each local region, but there is no smoothness at the
 knots.
\end_layout

\begin_layout Itemize
In the top right, we have a slight improvement, where we restrict the piecewise
 cubic polynomial to be continuous.
 But this is still not good enough, as we see at the knot there is a kink
 (still not smooth).
\end_layout

\begin_layout Itemize
In the bottom two graphs, we fit a cubic and linear splines respectively
 (using the definitions above).
 In this case, we finally obtained both LOCALNESS & SMOOTHNESS because splines
 enforce additional constraints at the knot (fit must be continuous at the
 derivatives up to 
\begin_inset Formula $d-1$
\end_inset

 degree).
\end_layout

\begin_layout Subsubsection*
Knot Placements
\end_layout

\begin_layout Standard
When we fit a spline, where should we place the knots? The regression spline
 is most flexible in regions that contain a lot of knots, because in those
 regions the polynomial coefficients can change rapidly.
 Hence, one option is to place more knots in places where we feel the function
 might vary most rapidly, and to place fewer knots where it seems more stable.
 While this option can work well, in practice it is common to place knots
 in a uniform fashion.
 One way to do this is to specify the desired degrees of freedom, and then
 have the software automatically place the corresponding number of knots
 at uniform quantiles of the data.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
How many knots hsould we use, or equivalently how many degrees of freedom
 should our splien contain? One option is to try out different numbers of
 knots and see which produces the best looking curve.
 An more objective approach is to use cross-validation -- With this method,
 we remove a portion of the data (say 10%), fit a spline with a certain
 number of knots to the remaining data, and then use spline to make predictions
 for the held-out portion.
 We repeat this process multiple times until each observation has been elft
 out once, and then compute the overall cross-validated RSS.
 This procedure can be repated for different numbers of knots 
\begin_inset Formula $K$
\end_inset

.
 Then the value of 
\begin_inset Formula $K$
\end_inset

 giving the smallest cross-validated RSS is chosen.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
When we fit a Generalized Additive Modle (GAM), we need to fit multiple
 splines simultaneously on several variables at a time.
 This could potentially require the selection of degrees of freedom for
 each variable.
 In cases like this we typically adopt a more pragmatic approach and set
 the degrees of freedom to a fixed number, say four, for all terms.
\end_layout

\begin_layout Subsection*
Smoothing Splines
\end_layout

\begin_layout Standard
Consider the criterion for fitting a smooth function 
\begin_inset Formula $g(x)$
\end_inset

 to some data:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\min_{g\in\mathcal{S}}\sum_{i=1}^{n}\left(y_{i}-g\left(x_{i}\right)\right)^{2}+\lambda\int g^{''}(t)^{2}dt\]

\end_inset


\end_layout

\begin_layout Itemize
The first term is RSS, and tries to make 
\begin_inset Formula $g(x)$
\end_inset

 match the data at each 
\begin_inset Formula $x_{i}$
\end_inset


\end_layout

\begin_layout Itemize
The second term is a roughness penalty and controls how wiggly 
\begin_inset Formula $g(x)$
\end_inset

 is.
 It is modulated by the tuning parameter 
\begin_inset Formula $\lambda$
\end_inset

 (The wiggliness of the function 
\begin_inset Formula $g$
\end_inset

 can be approximate by the rate of change of 
\begin_inset Formula $g'$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Itemize
The smaller 
\begin_inset Formula $\lambda$
\end_inset

, the more wiggly the function, eventually interpolating 
\begin_inset Formula $y_{i}$
\end_inset

 when 
\begin_inset Formula $\lambda=0$
\end_inset


\end_layout

\begin_layout Itemize
As 
\begin_inset Formula $\lambda\uparrow\infty$
\end_inset

, the function 
\begin_inset Formula $g(x)$
\end_inset

 becomes linear.
\end_layout

\end_deeper
\begin_layout Standard
We can see from above formulation, this is an objective function in the
 form LOSS 
\begin_inset Formula $+$
\end_inset

 PENALTY on model complexity that tries to balance bias-variance-trade-offs.
 It turns out that the solution for this problem is a natural cubic spline,
 with a knot at every unique value of 
\begin_inset Formula $x_{i}$
\end_inset

.
\begin_inset Foot
status open

\begin_layout Plain Layout
it is not the same natural cubic spline that one would get if one applied
 the basis function approach in knots at 
\begin_inset Formula $x_{1}\dots x_{n}$
\end_inset

 -- rather, it is a 
\shape italic
\color black
shrunken version
\shape default
\color inherit
 of such cubic spline.
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
The obvious advantage of this approach is that we avoid the knot-selection-place
ment issue, levaing a single 
\begin_inset Formula $\lambda$
\end_inset

 to be choosen
\end_layout

\begin_layout Itemize
The appropriate level of 
\begin_inset Formula $\lambda$
\end_inset

 can be chosen again by cross-validation.
 In particular, the LOOCV error is a popular choice
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula \[
RSS_{CV}(\lambda)=\sum_{i=1}^{n}\left(y_{i}-g_{\lambda}^{(-i)}(x_{i})\right)^{2}=\sum_{i=1}^{n}\left[\frac{y_{i}-\hat{g}_{\lambda}(x_{i})}{1-\left\{ \mathbf{S}_{\lambda}\right\} _{ii}}\right]^{2}\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
Finally, it is worth noting that Smoothing splines is an example of a more
 general modeling class which we can write 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
\hat{\mathbf{g}}_{\lambda} & = & \mathbf{S}_{\lambda}\mathbf{y}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathbf{y}$
\end_inset

 is the labels, and 
\begin_inset Formula $\hat{\mathbf{g}}_{\lambda}$
\end_inset

 is the fitted values (Linear operator on 
\begin_inset Formula $y$
\end_inset

 to get 
\begin_inset Formula $\hat{y}$
\end_inset

).
 For this class of models, we can define the 
\begin_inset Quotes eld
\end_inset

effective degrees of freedom
\begin_inset Quotes erd
\end_inset

 (that better captures the model complexity) as 
\begin_inset Formula $df_{\lambda}=\sum_{i=1}^{n}\left\{ \mathbf{S}_{\lambda}\right\} _{ii}$
\end_inset

, which is the trace of the 
\begin_inset Formula $\mathbf{S}_{\lambda}$
\end_inset

 operator.
\begin_inset Foot
status open

\begin_layout Plain Layout
Use smooth.spline(..., df) in R
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Local Regression
\end_layout

\begin_layout Standard
Local regression fits flexible non-linear functions in each local regions,
 which involves computing the fit at a target point 
\begin_inset Formula $x_{0}$
\end_inset

 using only the nearby training observations.
 It is somtimes referred to as a memory-based procedure, because like nearest-ne
ighbors, we need all the training data each time we wish to compute a prediction.
 Here is the algorithm -- Local Regression at 
\begin_inset Formula $X=x_{0}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/local_regression_algorithm.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
There are several tuning parameters for local regression:
\end_layout

\begin_layout Itemize
What kind of 
\begin_inset Formula $f$
\end_inset

 to use? The typical choice are linear 
\begin_inset Formula $f$
\end_inset

, but we can use constant, linear, quadratic functions.
\end_layout

\begin_layout Itemize
What Kernel function 
\begin_inset Formula $K$
\end_inset

 to use?
\end_layout

\begin_layout Itemize
Most importantly, what should be the span 
\begin_inset Formula $s$
\end_inset

.
 i.e.
 how large should the neighborhood of points should be used in the fitting
 procedure? This is probably the most important parameter in determining
 the wiggliness of the fit.
 Another way to think about this is 
\begin_inset Formula $s=k/n$
\end_inset

, where 
\begin_inset Formula $k$
\end_inset

 is the number of points we want to considered in the neighborhood, and
 is directly related to 
\begin_inset Formula $s$
\end_inset

.
\end_layout

\begin_layout Standard
Local regression can be generalized in many different ways.
 One such way is varying coefficient model, where we fit a mutliple regression
 model that is global in some variables, but local in another (often a time
 variable).
 We can also extend the model to 2 variables, in such cases the neighorhood
 would be a two dimensional neighborhood.
 Similar to KNN, it suffers from curse of dimensionality when the dimension
 is high.
\end_layout

\begin_layout Subsection*
Generalized Additive Model (GAM)
\end_layout

\begin_layout Standard
A natural way to extend the multiple linear regression model 
\begin_inset Formula $y_{i}=\beta_{0}+\sum_{j=1}^{p}\beta_{j}x_{ij}+\epsilon_{i}$
\end_inset

 in order to allow for non-linear relationships between each feature and
 the response is to replace each linear component 
\begin_inset Formula $\beta_{j}x_{ij}$
\end_inset

 with a (smooth) non-linear function 
\begin_inset Formula $f_{j}(x_{ij})$
\end_inset

.
 We would then write the model as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
y_{i} & = & \beta_{0}+\sum_{j=1}^{p}f_{j}\left(x_{ij}\right)+\epsilon\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This is an example of a GAM.
 It is called an additive model because we calculate a separate 
\begin_inset Formula $f_{j}$
\end_inset

 for each 
\begin_inset Formula $X_{j}$
\end_inset

, and then add togethre all of their contribution as the final fit.
 Many of the methods we disccused above can be used to fit 
\begin_inset Formula $f_{j}$
\end_inset

, and can be used as the building blocks for GAM.
\begin_inset Foot
status open

\begin_layout Plain Layout
For more explanations on how a GAM is fitted, take a look at textbook pg.
 284
\end_layout

\end_inset

.
 The same technique can be used for classification, where
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
log\left(\frac{p\left(X\right)}{1-p\left(X\right)}\right) & = & \beta_{0}+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\dots+f_{p}\left(X_{p}\right)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection*
pros and cons of GAM
\end_layout

\begin_layout Itemize
GAMs allow us to fit a non-linear 
\begin_inset Formula $f_{j}$
\end_inset

 to each 
\begin_inset Formula $X_{j}$
\end_inset

, so that we can automatically model non-linear relationships that standard
 linear regression will miss.
 
\color blue
This means that we do not need to manually try out many different transformation
s on each variable individually.
\end_layout

\begin_layout Itemize
The non-linear fit can potentially make more accurate predictions for the
 response 
\begin_inset Formula $Y$
\end_inset


\end_layout

\begin_layout Itemize
Because the model is additive, we can still examine the effect of each 
\begin_inset Formula $X_{j}$
\end_inset

 on 
\begin_inset Formula $Y$
\end_inset

 individually while holding all of the other variables fixed.
 Hence if we are interested in inference, GAMs provide a useful representation
\end_layout

\begin_layout Itemize
The smoothness of the function 
\begin_inset Formula $f_{j}$
\end_inset

 for the variable 
\begin_inset Formula $X_{j}$
\end_inset

 can be summarized via degrees of freedom
\end_layout

\begin_layout Itemize
The main limitation of GAMs is that the model is restricted to be additive.
 With many variables, important interactions can be missed.
 However, as with linear regression, we can manually add interaction terms
 to the GAM model by including additional predictors of the form 
\begin_inset Formula $X_{j}\times X_{k}$
\end_inset

.
 In addition we can add low-dimensional interaction functions of the form
 
\begin_inset Formula $f_{jk}\left(X_{j},X_{k}\right)$
\end_inset

 into the model; such terms can be fit using two-dimensional smoothers such
 as local regression, or two-dimensional splines (not covered here).
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Lingering Questions:
\end_layout

\begin_layout Standard
None so far.
 But my notes on R labs for Chapter 7 is potentially very helpful! It talks
 about using ANOVA to compare models, I don't think I fully understand that
 yet.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

R sessions
\end_layout

\begin_layout Plain Layout

==================================================================
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

polynomial regression
\end_layout

\begin_layout Plain Layout

===================
\end_layout

\begin_layout Plain Layout

- if we use the poly(variable, d = degree), it will automatically create
 an orthogonal basis functions for us.
 The advantage of this is that we can directly compared the coefficients
 for different degrees of that variable, and see to what degree the polynomial
 is appropriate.
 This is not going to work when we use variable + I(variable^2) + ...
 since they are not orthogonal, because we cannot easily interpret the coefficie
nts, one by one, because they are correlated and tend to move together.
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

- The good thing is that if all we care is the fitted values, the choice
 of basis would be affected the fitted values.
 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

- CAUTION: This interpretation of coefficient of orthogonal basis, no longer
 works when:
\end_layout

\begin_layout Plain Layout

    - we have a GLM instead of a linear model (because things become non-linear,
 and 
\end_layout

\begin_layout Plain Layout

      orthogonality is lost)
\end_layout

\begin_layout Plain Layout

    - when we have multiple distinct variables in the model
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

So the interpretation only works if you have one variable in a linear model.
 The solution here is to use the ANOVA function, and it's particularly helpful
 for comparing a nested sequence of models.
 The ANOVA output will tell us whether the additional variable helped in
 reducing the RSS.
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

polynomial, this time classification (using glm)
\end_layout

\begin_layout Plain Layout

====================
\end_layout

\begin_layout Plain Layout

- We can have polynomial features for a glm, and the fitting is similar.
 Remember that even if we use poly, the orthogonal basis will no longer
 be orthogonal because we give weights to fit logistic regression(?).
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

- To fit the confidence band, we need to get it at the logit scale, and
 then transform them into probability scale!
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

spline
\end_layout

\begin_layout Plain Layout

====================
\end_layout

\begin_layout Plain Layout

- teach how to use cubic spline using bs(variable, knots = ...) [again, more
 local than polynomial, have continuous 1st and 2nd derivative]
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

- teach how to use natural cubic spline using ns().
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

- teach how to use smoothing spline.
 smooth.spline(variable, response, df = ...) where df is the effective degrees
 of freedom.
 However, we can use smooth.spline(varaible, response, cv = true) to have
 the right df chosen for us by cross validation.
 When we call the fit, we can see the effective degrees of freedom of the
 smooth.spline(...cv=true) model.
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

- Generalized Additive Model (GAM).
 So far we have talked about fitting on a single nonlinear terms.
 GAM allowed us to fit on multiple nonlinear terms.
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

- Use gam(response ~ variable ..., data = ...) to fit a gam.
\end_layout

\begin_layout Plain Layout

- The s(varaible, df = ...) is the GAM syntax for smoothing spline.
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

- Can use anova(fit1, fit2, test = "Chisq") to compare just two models(?).
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

- Even for models fitted in lm or glm, we can use plot.glm to plot how each
 nonlinear term vary or contribute to the response variable.
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Chapter 8: Tree-Based Method
\end_layout

\begin_layout Standard
In this chapter, we describe tree-based methods for regression and classificatio
n.
 These involve 
\shape italic
stratifying
\shape default
 or 
\shape italic
segmenting
\shape default
 the predictor space into a number of simple, distinct regions.
 In order to make a prediction, we typically use the mean or mode of the
 training observations in the region to which it belongs.
 Since the set of splitting rules used to segment the predictor space can
 be summarized in a tree, these types of approaches are known as 
\series bold
decision tree
\series default
 methods.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Tree-based methods are simple and useful for interpretation.
 However, they typically are not competitive with the best supervised learning
 approaches, such as those seen in earlier chapters, in terms of prediction
 accuracy.
 Hence in this chapter we also introduce bagging, random forests, and boosting.
 Each of these approaches involves producing multiple trees which are the
 n combined to yield a single consensus prediction.
 We will see that combining a large number of trees can often result in
 dramatic improvements in prediction accuracy, at the expense of some loss
 in interpretation.
\end_layout

\begin_layout Subsection*
Decision Trees
\end_layout

\begin_layout Standard
Let's first discuss the process of building a regression tree.
 Roughly speaking, there are two steps
\end_layout

\begin_layout Itemize
We divide the predictor space, that is, the set of possible values for 
\begin_inset Formula $X_{1},X_{2}\dots X_{p}$
\end_inset

 into 
\begin_inset Formula $J$
\end_inset

 distinct and non-overlapping regions 
\begin_inset Formula $R_{1},R_{2}\dots R_{J}$
\end_inset

.
\end_layout

\begin_layout Itemize
For every new observation that falls into the region 
\begin_inset Formula $R_{j}$
\end_inset

, we make the same prediction, which is simply the mean of the response
 values for the training observations in 
\begin_inset Formula $R_{j}$
\end_inset

.
\end_layout

\begin_layout Standard
We now elaborate on the construction of 
\begin_inset Formula $R_{j}$
\end_inset

's above.
 In theory, the 
\begin_inset Formula $R_{j}$
\end_inset

's can have any shape, but we choose to divide the predictors space into
 high dimensional rectangles, or boxes, for simplicity and the ease of interpret
ation.
 The goal is to find boxes 
\begin_inset Formula $R_{1},R_{2}\dots R_{J}$
\end_inset

 that minimize the 
\begin_inset Formula $RSS$
\end_inset

, given by
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\sum_{j=1}^{J}\sum_{i\in R_{j}}\left(y_{i}-\hat{y}_{R_{j}}\right)^{2}\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\hat{y}_{R_{j}}$
\end_inset

 is the mean of the training observations in 
\begin_inset Formula $R_{j}$
\end_inset

.
 Unfortunately, this optimization problem is computationally infeasible.
 As an alternative, we take a 
\shape italic
top-down
\shape default
, 
\shape italic
greedy
\shape default
 approach that is known as 
\shape italic
recursive binary splitting
\shape default
.
 The approach is 
\shape italic
top down
\shape default
 because it starts from the top where all the observations belong to the
 same region and then successively splits the predictor space.
 Each split is indicated via two new branches further down on the tree (hence
 
\shape italic
binary
\shape default
).
 It is 
\shape italic
greedy
\shape default
 because at each step of the tree building process, the best split is made
 at the particular step, rather than looking ahead and picking a split that
 will lead to a better tree in some future step.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
In order to perform recursive binary splitting, in each of the step/node,
 we consider all predictors 
\begin_inset Formula $X_{1},X_{2}\dots X_{p}$
\end_inset

 and all possible values of the cutoff points 
\begin_inset Formula $s$
\end_inset

 such that the resulting tree has the lowest 
\begin_inset Formula $RSS$
\end_inset

.
 In greater details, we seek the value of 
\begin_inset Formula $j$
\end_inset

 and 
\begin_inset Formula $s$
\end_inset

 that minimize
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\sum_{i:x_{i}\in R_{L}(j,s)}(y_{i}-\hat{y}_{R_{L}})^{2}+\sum_{i:x_{i}\in R_{R}(j,s)}(y_{i}-\hat{y}_{R_{R}})^{2}\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\hat{y}_{R_{L}(j,s)}$
\end_inset

 and 
\begin_inset Formula $\hat{y}_{R_{R}(j,s)}$
\end_inset

 are the mean value of the training observations in the left branch and
 right branch respectively.
\begin_inset Foot
status open

\begin_layout Plain Layout

\color red
How exactly is this optimization being done?
\end_layout

\end_inset

.
 We then the repeat the same process, recursively, for each of the splitted
 region, so to minimize RSS.
 The process continues until a stopping criterion is reached: For example,
 that there are no more than 5 training observations.
 Once the regions 
\begin_inset Formula $R_{j}$
\end_inset

's have been determined, we predict the response for a given test observation
 using the mean response value in which the test observation belongs.
\end_layout

\begin_layout Subsubsection*
Tree Pruning
\end_layout

\begin_layout Standard
The process described above may produce good predictions on the training
 set, but not necessarily on the test set.
 This is because the tree might be too complex, as a result of fitting the
 training set too hard.
 Therefore, it is wise to construct a simpler tree (that is, fewer 
\begin_inset Formula $R_{j}$
\end_inset

's), which might lead to smaller variance in the test error at the cost
 of of some increase in bias.
 One possible alternative is to only split the node when the split gives
 us 
\begin_inset Formula $RSS$
\end_inset

 reduction that surpass certain threshold, but this approach is too short-sighte
d, in the sense that a harmless split (with no major reduction in 
\begin_inset Formula $RSS$
\end_inset

) might lead to a very good next split (that reduce 
\begin_inset Formula $RSS$
\end_inset

 significantly) in the next step.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
A better strategy is to grow a very large tree 
\begin_inset Formula $T_{0}$
\end_inset

, and prune it back to obtain its subtrees, calculate the cross-validated
 or validation error on each and everyone of them, and select the best tree.
 However, running CV on all these subtrees could be a huge search problem,
 and can be computationally infeasible.
 As a result, we simply the problem to considered a sequence of nested candidate
 trees, using the technique of 
\shape italic
\color black
cost complexity pruning
\shape default
\color inherit
.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Rather than considering every subtree, we consider a sequence of subtrees
 indexed by a nonnegative tuning parameter 
\begin_inset Formula $\alpha$
\end_inset

.
 For each value of 
\begin_inset Formula $\alpha$
\end_inset

 there corespond a subtree 
\begin_inset Formula $T\subset T_{0}$
\end_inset

 such that 
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\sum_{m=1}^{|T|}\sum_{x_{i}\in R_{m}}(y_{i}-\hat{y}_{R_{m}})^{2}+\alpha\vert T\vert\]

\end_inset


\end_layout

\begin_layout Standard
is as small as possible.
 Here 
\begin_inset Formula $\vert T\vert$
\end_inset

 represent the number of terminal nodes in the tree, 
\begin_inset Formula $R_{m}$
\end_inset

 is the rectangular predictors region corresponding to the 
\begin_inset Formula $m^{th}$
\end_inset

 terminal node, and 
\begin_inset Formula $\hat{y}_{R_{m}}$
\end_inset

 is the predicted response associated with 
\begin_inset Formula $R_{m}$
\end_inset

.
 The tuning parameter 
\begin_inset Formula $\alpha$
\end_inset

 control the trade-off between RSS and the model complexity, very much similar
 to the spirit of regularization.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
It turns out that as we increase 
\begin_inset Formula $\alpha$
\end_inset

 from 0, branches get pruned from the original tree 
\begin_inset Formula $T_{0}$
\end_inset

 in a predictable and nested fashion (This is the key that allows us to
 reduce the search space).
 We can then use cross-validation to select the optimal 
\begin_inset Formula $\alpha$
\end_inset

 in terms of test error.
 Finally, we return to the full data set and obtain the subtree correspond
 to 
\begin_inset Formula $\alpha^{*}$
\end_inset

 (we can do this because we build a sequence of trees using the full training
 set).
 The algorithm is described below:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/cost_complexity_pruning.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Classification Tree
\end_layout

\begin_layout Standard
The process of building a classification tree is very similar to regression
 tree:
\end_layout

\begin_layout Itemize
We divide the predictor space, that is, the set of possible values for 
\begin_inset Formula $X_{1},X_{2}\dots X_{p}$
\end_inset

 into 
\begin_inset Formula $J$
\end_inset

 distinct and non-overlapping regions 
\begin_inset Formula $R_{1},R_{2}\dots R_{J}$
\end_inset

.
\end_layout

\begin_layout Itemize
For every new observation that falls into the region 
\begin_inset Formula $R_{j}$
\end_inset

, we make the same prediction, which is simply the 
\series bold
mode (not the mean)
\series default
 of the response values for the training observations in 
\begin_inset Formula $R_{j}$
\end_inset

.
\end_layout

\begin_layout Standard
In the step where we construct distinct, non-overlapping 
\begin_inset Formula $R_{j}$
\end_inset

's, we also use recursive binary splitting to grow a classification tree.
 In the classification setting, RSS cannot be used as a criterion for splitting.
 A natural alternative is to use the classification error rate.
 This is simply the fraction of training examples that do not belong to
 the most common class 
\begin_inset Formula $E=1-\max_{k}(\hat{p}_{mk})$
\end_inset

.
 Therefore, for continuous predictor, we find the 
\begin_inset Formula $(j,s)$
\end_inset

 pair such that we minimize 
\begin_inset Formula $E_{L}+E_{R}=1-\max_{k}(\hat{p}_{Lk})+\sum_{x_{i}\in R_{R}}1-\max_{k}(\hat{p}_{Rk})$
\end_inset

 where 
\begin_inset Formula $\hat{p}_{mk}$
\end_inset

 is the proportion of training example that belongs to class 
\begin_inset Formula $k$
\end_inset

 in region 
\begin_inset Formula $R_{m}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard

\color red
However, classification error is often not sufficiently sensitive for tree
 growing
\color inherit
, and there are two alternative:
\end_layout

\begin_layout Itemize

\series bold
Gini Index
\series default
: It is defined to be 
\begin_inset Formula $G=\sum_{k=1}^{K}p_{k}(1-p_{k})$
\end_inset

, which is a measure of total variance of the 
\begin_inset Formula $K$
\end_inset

 classes.
 When 
\begin_inset Formula $p_{k}$
\end_inset

's are close to 
\begin_inset Formula $0$
\end_inset

 or 
\begin_inset Formula $1$
\end_inset

, the Gini index is close to 0.
 Therefore, it is often used as a measure of node purty.
 If the node is pure, then Gini's index would be small; When the node is
 uniform/not pure, then the index will be high.
 For continuous predictor, we again choose the 
\begin_inset Formula $(j,s)$
\end_inset

 pair such that it minimize
\begin_inset Formula \begin{eqnarray*}
G_{L}+G_{R} & = & \sum_{k=1}^{K}p_{Lk}(1-p_{Lk})+\sum_{k=1}^{K}p_{Rk}(1-p_{Rk})\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Cross Entropy
\series default
: An alternative to Gini's index is cross-entropy, given by 
\begin_inset Formula $D=-\sum_{k=1}^{K}\hat{p}_{mk}log(\hat{p}_{mk})$
\end_inset

.
 It turns out that Cross-entropy is numerically very similar to Gini's index.
 For continuous predictor, we again choose the 
\begin_inset Formula $(j,s)$
\end_inset

 pair such that it minimize
\begin_inset Formula \begin{eqnarray*}
D_{L}+D_{R} & = & -\sum_{k=1}^{K}p_{Lk}log(p_{Lk})+-\sum_{k=1}^{K}p_{Rk}log(p_{Rk})\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
When building a classification tree, either the Gini index or the cross-entropy
 are typically used to evaluate the quality of a particular split, since
 these two approaches are more sensitive to node purity than is the classificati
on error rate.
 Any of these three approaches might be used when pruning the tree, but
 the classification error rate is preferable if prediction accuracy of the
 final pruned tree is the goal.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Sometimes a split might yield two terminal nodes that gives the same prediction
 value (e.g.
 both prediction predicts class 
\begin_inset Formula $k$
\end_inset

).
 Why would the split still occurs if the prediction is the same for either
 node? This has to do with node purity.
 If in the right branch, all the training observations belong to class 
\begin_inset Formula $k$
\end_inset

, while the other branch only have 
\begin_inset Formula $7/11$
\end_inset

 of them to be in class 
\begin_inset Formula $k$
\end_inset

, then if a new observation goes to right branch, we are much more confident
 about the class prediction of the new observation.
 
\series bold
A lot of time, we are not only interested in the class prediction, but we
 are also interested in the node purity because it tells us how confident
 we are about the prediction!
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Before moving on to ensemble methods, I should point out that we have only
 discussed node splitting in the case of continuous variables, but splitting
 based on qualitiative variables is also very common.
 Often, we choose to put a subset of the factors into one branch, and the
 others into another branch.
 The math was not discussed in this book, but I remember CS 246 talk in
 length about this, at least in the homeworks.
 
\color red
I don't really know the details here though.
 
\end_layout

\begin_layout Subsection*
Bagging (Bootstrap Aggregating)
\end_layout

\begin_layout Standard
The bootstrap, introduced in chapter 5, is an extremely powerful idea.
 It is used in many situations in which it is hard or even impossible to
 directly compute the standard deviation of a quantity of interest.
 We see that the bootstrap can be used in a completely different context,
 in order to improve statistical learning methods such as decision trees.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
The decision trees discussed in the earlier section suffers from high variance.
 This means that if we split the data set into two-halves and fit a model
 on each half, the results that we get could be quite different.
 In contrast, a procedure with low variance will yield similar results if
 applied repeatedly to distinct data set.
 
\shape italic
Bootstrap aggregation
\shape default
, or 
\shape italic
bagging
\shape default
, is a general purpose procedure for reducing the variance of a statistical
 learning method.
 We introduce it here because it is often used in the context of decision
 tree.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Recall that given a set of 
\begin_inset Formula $n$
\end_inset

 observations 
\begin_inset Formula $Z_{1}\dots Z_{n}$
\end_inset

, each with variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

, the variance of the mean 
\begin_inset Formula $\bar{Z}$
\end_inset

 will have variance 
\begin_inset Formula $\sigma^{2}/n$
\end_inset

.
 In other words, averaging independent samples reduces variance.
 Hence, a natural way to reduce the variance and hence increase the prediction
 accuracy of a statistical learning method is to take many training sets
 from the population, build a separate prediction model using each training
 set, and average the resulting predictions.
 In other word, we could calculate 
\begin_inset Formula $\hat{f}^{1}(x),\hat{f}^{2}(x),\dots,\hat{f}^{B}(x)$
\end_inset

 using 
\begin_inset Formula $B$
\end_inset

 separate training sets, and average them in order to obtain a single low-varian
ce statistical learning model, given by
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
\hat{f}_{\text{avg}}\left(x\right) & = & \frac{1}{B}\sum_{b=1}^{B}\hat{f}^{b}(x)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Of course, this is not practical because we often do not have the leisure
 to access to multiple training sets.
 Instead, we can bootstrap, by taking repeated samples of the data set at
 hand.
 In this approach, we generate 
\begin_inset Formula $B$
\end_inset

 bootstrapped training data sets.
 We then train our method on each of the training set in order to get 
\begin_inset Formula $\hat{f}^{b*}(x)$
\end_inset

, and finally average all the predictions, to obtain
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
\hat{f}_{\text{bag}}(x) & = & \frac{1}{B}\sum_{b=1}^{B}\hat{f}^{b*}(x)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
While bagging can improve predictions for many regression methods, it is
 particularly useful for decision trees.
 To apply bagging to regression trees, we simply construct 
\begin_inset Formula $B$
\end_inset

 regression trees using 
\begin_inset Formula $B$
\end_inset

 bootstrapped training sets, and average the resulting predictions.
 
\series bold
These trees are grown deep, and not pruned
\series default
.
 The reason is that while a single deep tree have high variance, it is has
 low bias.
 By bootstrap aggregating, we will be able to reduce the variance part significa
ntly.
\begin_inset Foot
status open

\begin_layout Plain Layout
In the context of classification, we follow the same exact procedure, except
 the prediction is made by taking the majority vote
\end_layout

\end_inset

 Note that, the number of trees 
\begin_inset Formula $B$
\end_inset

 is not a critical parameter with bagging; using a very large 
\begin_inset Formula $B$
\end_inset

 will not lead to overfitting.
 In practice, we use a value of 
\begin_inset Formula $B$
\end_inset

 sufficiently large that the error has settled down.
\end_layout

\begin_layout Subsubsection*
Out-of-Bag Error Estimation
\end_layout

\begin_layout Standard
It turns out that there is a very straightforward way to estimate the test
 error of a bagged model, without the need to perform cross-validation or
 the validation set approach.
 Recall that the key to bagging is that trees are repeatedly fit to bootstrapped
 subsets of the observations.
 One can show that on average, each bagged tree makes use of around 
\begin_inset Formula $2/3$
\end_inset

 of the observations.
 Proof:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
P\left(\text{a single observation not be drawn in n trials}\right) & = & \left(1-P\left(\text{be drawn, with equally likely probability}\right)\right)^{n}\\
 & = & (1-\frac{1}{n})^{n}{\color{red}\longrightarrow}\frac{1}{e}\approx0.36\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This means that if we fixed on the 
\begin_inset Formula $i^{th}$
\end_inset

 observation, and walk through each of 
\begin_inset Formula $B$
\end_inset

 bootstrapped training sets, on average, the observation would only be present
 in 
\begin_inset Formula $2/3$
\end_inset

 of the bootstrapped training sets.
 Therefore, we can predict the response for the 
\begin_inset Formula $i^{th}$
\end_inset

 observation using each of the trees in which that observation was OOB,
 meaning that it was not used for training in that training set.
 This will yield around 
\begin_inset Formula $B/3$
\end_inset

 predictions for the 
\begin_inset Formula $i^{th}$
\end_inset

 observation.
 In order to obtain a single prediction for the 
\begin_inset Formula $i^{th}$
\end_inset

 observation, we can average these predicted responses or can take a majority
 vote.
 This leads to a single OOB prediction for the 
\begin_inset Formula $i^{th}$
\end_inset

 observation.
 We repeat the same process for all 
\begin_inset Formula $n$
\end_inset

 observations, from which the overall OOB MSE or classification error can
 be computed.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
The resulting OOB error is a valid estimate of the test error, since the
 response for each observation is predcited using only the trees that were
 not fit using that observation.
 It can be shown that with 
\begin_inset Formula $B$
\end_inset

 sufficiently large, the OOB approach is virtually equivalent to LOOCV.
 This approach is convenient when performing bagging on large data sets
 for which cross-validation would be computationally onerous.
\end_layout

\begin_layout Subsubsection*
Variable Importance Measures
\end_layout

\begin_layout Standard
As we have discussed, bagging typically results in improved prediction accuracy.
 Unfortunately, however, it can be difficult to interpret the resulting
 model, wheras in a single decision the interpretation is clear.
 Thus, bagging improves prediction accuracy at the expense of interpretability.
 Although the collection of bagged trees is much more difficult to interpret
 than a single tree (each observation goes down a different tree path, and
 the results are combined), one can still obtain an overall summary of the
 importance of each predictor.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
The typical approach for comparing variable importance measure is constructed
 in the following way:
\end_layout

\begin_layout Itemize

\series bold
Regression
\series default
: For each predictors, record the total amount that RSS is decreased due
 to splits over that given predictor, averaged over all 
\begin_inset Formula $B$
\end_inset

 trees.
\begin_inset Foot
status open

\begin_layout Plain Layout

\color red
I think a predictor can be used multiple times even in a single tree, record
 ALL of those splits
\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Classification
\series default
: For each predictors, record the total amount that Gini index/cross-entropy
 is decreased due to splits over that given predictor, averaged over all
 
\begin_inset Formula $B$
\end_inset

 trees.
\end_layout

\begin_layout Standard
This way, we have an objective way of comparing which predictor tries the
 hardest, and is most effective in working toward prediction generalization.
 
\end_layout

\begin_layout Subsection*
Random Forest
\end_layout

\begin_layout Standard
Random forests provide an improvement over bagged trees by way of a small
 tweak that 
\shape italic
\color black
decorrelates
\shape default
\color inherit
 the trees.
 As in bagging, we build a number of decision trees on bootstrapped training
 samples.
 But when building these decision trees, each time a split in a tree is
 considered, a random sample of 
\begin_inset Formula $m$
\end_inset

 predictors is chosen as split candidates from the full set of 
\begin_inset Formula $p$
\end_inset

 predictors.
 The split is allowed to use only one of those 
\begin_inset Formula $m$
\end_inset

 randomly chosen predictors.
 A fresh sample of 
\begin_inset Formula $m$
\end_inset

 predictors is taken at each and every split in consideration, and typically
 we choose 
\begin_inset Formula $m\approx\sqrt{p}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
In other words, in building a random forest, at each split in the tree,
 the algorithm is 
\shape italic
\color black
not even allowed
\shape default
\color inherit
 to consider a majority of the available predictors.
 The reason of doing so is decorrelations.
 Suppose that there is one very strong predictor in the data set, along
 with a number of moderately strong predictors.
 Then in the collection of bagged trees, most of all of the trees will use
 this strong predictor in the top split.
 Consequently, all bagged trees would look very similar in structure.
 Hence, the predictions from the bagged trees will be highly correlated.
 Unfortunately, averaging many highly correlated quantites does not lead
 to a significant reduction in variance.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Random forest overcome this problem by forcing each split to consider only
 a subset of the predictors, therefore, providing more variation to the
 each tree's growing process.
 On average, only 
\begin_inset Formula $m/p$
\end_inset

 of the splits would consider the strong predictor, and about 
\begin_inset Formula $1-m/p=(p-m)/p$
\end_inset

 would not.
 For 
\begin_inset Formula $m$
\end_inset

 small, we will be able to give more chance to other predictors, hence,
 decorrelating the trees so to make the averaging more effective in reducing
 variance.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Notice in random forest, the tuning parameters here are:
\end_layout

\begin_layout Itemize
\begin_inset Formula $m$
\end_inset

, the number of predictors that are allowed to be considered in each split.
 When 
\begin_inset Formula $m=p$
\end_inset

, we are back to bagging (bootstrap aggregating).
\end_layout

\begin_layout Itemize
\begin_inset Formula $B$
\end_inset

, the number of trees we grow.
 Similarly to bagging, 
\begin_inset Formula $B$
\end_inset

 is not a crucial parameter, and large 
\begin_inset Formula $B$
\end_inset

 is unlikely to overfit the data.
 We typically choose 
\begin_inset Formula $B$
\end_inset

 large enough for the variance to settle down.
\end_layout

\begin_layout Standard
Here is an example where we compare the test error & OOB error of a single
 decision tree, bagging, and random forest with 
\begin_inset Formula $m=\sqrt{p}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/tree_methods_comparison.png
	scale 60
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
For the test error, we split the data set into training set and test set:
\end_layout

\begin_layout Itemize

\series bold
Single decision tree
\series default
: we built the model using the training set, and calculate the test error
 on test set.
 There is no 
\begin_inset Formula $B$
\end_inset

, so the test error is a flat line.
\end_layout

\begin_layout Itemize

\series bold
Bagged model
\series default
: as a function of 
\begin_inset Formula $B$
\end_inset

, we use the training set to create 
\begin_inset Formula $B$
\end_inset

 bootstrapped samples, fit 
\begin_inset Formula $B$
\end_inset

 bagged trees, the collection of 
\begin_inset Formula $B$
\end_inset

 bagged trees is our model.
 We the run through each observation in the test set -- get 
\begin_inset Formula $\sum_{b=1}^{B}\hat{f}^{b}(x)$
\end_inset

 or majority vote as our prediction, we then calculate the squared error
 or whether we misclassify for that observation.
 Once we run through all samples in the test set, we can then finally get
 the overall MSE or misclassification error, which what we see above.
\end_layout

\begin_layout Itemize

\series bold
Random forest model
\series default
: as a function of 
\begin_inset Formula $B$
\end_inset

, we use the training set to create 
\begin_inset Formula $B$
\end_inset

 bootstrapped samples, fit 
\begin_inset Formula $B$
\end_inset

 random forest trees, the collection of 
\begin_inset Formula $B$
\end_inset

 random forest trees is our model.
 The rest are exactly the same as the bagged model case.
\end_layout

\begin_layout Standard
For the OOB error, we simply use the whole data set (no need to split them
 into training or test set):
\end_layout

\begin_layout Itemize

\series bold
Single decision tree
\series default
: It doesn't exist, since we do not have 
\begin_inset Formula $B$
\end_inset

 bootstrapped samples.
\end_layout

\begin_layout Itemize

\series bold
Bagged model
\series default
: as a function of 
\begin_inset Formula $B$
\end_inset

, we use the whole data set to create 
\begin_inset Formula $B$
\end_inset

 bootstrapped samples, fit 
\begin_inset Formula $B$
\end_inset

 bagged trees, the collection of these 
\begin_inset Formula $B$
\end_inset

 bagged trees is our model.
 We then run through each and every observation in the data set (not test
 set) -- get 
\begin_inset Formula $\sum_{b=1}^{B^{-}}\hat{f}^{b}(x)$
\end_inset

 or majority vote 
\series bold
over only the trees that didn't use that observation for fitting
\series default
 (I used 
\begin_inset Formula $B^{-}$
\end_inset

 to indicate this).
 The average of these error gives us MSE or misclassification rate.
\end_layout

\begin_layout Itemize

\series bold
Random forest model
\series default
: as a function of 
\begin_inset Formula $B$
\end_inset

, we use the whole data set to create 
\begin_inset Formula $B$
\end_inset

 bootstrapped samples, fit 
\begin_inset Formula $B$
\end_inset

 random forest trees, the collection of these 
\begin_inset Formula $B$
\end_inset

 random forest trees is our model.
 The calculation for MSE and misclassification is exactly the same as 'bagged
 model' case.
\end_layout

\begin_layout Standard
Here is another example, but only focusing on random forest and only using
 test misclassification error:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/random_forest_parameter_tuning.png
	scale 60
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This is pretty much the same as the approach above, but now we vary the
 
\begin_inset Formula $m$
\end_inset

 parameter.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Boosting
\end_layout

\begin_layout Standard
Like bagging, boosting is a general approach that can be applied to many
 statistical learning methods for regression or classification.
 We only discuss boosting for decision trees.
 Recall that bagging involves creating multiple copies of the original training
 data set using the bootstrap, fitting a separate decision tree to each
 copy, and then combining all of the trees in order to create a single predictiv
e model.
 Notably, each tree is built on a bootstrap dat set, independent of the
 other trees.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Boosting works in a similar way, except that the trees are grown 
\series bold
sequentially
\series default
: each tree is grown using information from previously grown trees.
 Boosting algorithm for regression tree
\begin_inset Foot
status open

\begin_layout Plain Layout

\color blue
Boosting classification was not introduced here! Worth looking into more
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/boosting_algorithm.png
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize
Unlike fitting a single large decision tree to the data, which amounts to
 fitting the data hard and potentially overfitting, the boosting approach
 instead learns slowly.
\end_layout

\begin_layout Itemize
Given the current model, we fit a decision tree to the residuals from the
 model.
 We then add this new decision tree into the fittd function in order to
 update the residuals.
\end_layout

\begin_layout Itemize
Each of these trees can be rather small, with just a few terminal nodes,
 determined by the parameter 
\begin_inset Formula $d$
\end_inset

 in the algorithm.
\end_layout

\begin_layout Itemize
By fitting small trees to the residuals, we slowly improve 
\begin_inset Formula $\hat{f}$
\end_inset

 in areas where it does not perform well.
 The shrinkage parameters 
\begin_inset Formula $\lambda$
\end_inset

 slows the process down even futher, allowing more and different shaped
 trees to attack the residuals.
\end_layout

\begin_layout Subsubsection*
Tuning parameters for Boosting
\end_layout

\begin_layout Itemize
The number of trees 
\begin_inset Formula $B$
\end_inset

.
 Unlike bagging and random forests, boosting can overfit if 
\begin_inset Formula $B$
\end_inset

 is too large, although this overfitting tends to occur slowly if at all.
 We use cross-validation to choose 
\begin_inset Formula $B$
\end_inset

.
\end_layout

\begin_layout Itemize
The shrinkage parameter 
\begin_inset Formula $\lambda$
\end_inset

, a small positive number.
 This controls the rate at which boosting learns.
 Typical values are 
\begin_inset Formula $0.01-0.001$
\end_inset

, and the right choice can depend on the problem.
 Very small 
\begin_inset Formula $\lambda$
\end_inset

 can require using a very large value of 
\begin_inset Formula $B$
\end_inset

 in order to acheive good performance.
\end_layout

\begin_layout Itemize
The number of splits 
\begin_inset Formula $d$
\end_inset

 in each tree, which controls the complexity of the boosted ensemble.
 Often 
\begin_inset Formula $d=1$
\end_inset

 works well, in which case each tree is a stump, consisting of a single
 split and resulting in an additive model.
 More generally 
\begin_inset Formula $d$
\end_inset

 is the interaction depth, and controls the interaction order of the boosted
 model, since 
\begin_inset Formula $d$
\end_inset

 splits can involve at most 
\begin_inset Formula $d$
\end_inset

 variables.
\end_layout

\begin_layout Standard
Another illustration of the test error as a function of 
\begin_inset Formula $B$
\end_inset

, number of trees built
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/boosting_randomForest_comparison.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Random Forest Model
\series default
: same as above
\end_layout

\begin_layout Itemize

\series bold
Boosting Model
\series default
: We break the data set into training set and test set.
 As a function of 
\begin_inset Formula $B$
\end_inset

, we use the training set for the boosting model (sequentially).
 We start with 
\begin_inset Formula $\left(X,r=y\right)$
\end_inset

, we then sequentially builds smaller trees using the residuals from the
 previous model successively, until we reach 
\begin_inset Formula $B$
\end_inset

 trees.
 We then use 
\begin_inset Formula $\hat{f}_{boost}(x)=\sum_{b=1}^{B}\lambda\hat{f}^{b}(x)$
\end_inset

 as our model.
 Run through the test set, and calculate the MSE or misclassification error.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Lingering Questions:
\end_layout

\begin_layout Subsection*
Some questions around the math beyond making a split
\end_layout

\begin_layout Subsubsection*
Regression Tree
\end_layout

\begin_layout Standard
How exactly is the 
\begin_inset Formula $(j,s)$
\end_inset

 being chosen in the problem of
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\min_{(j,s)}\sum_{i:x_{i}\in R_{L}(j,s)}(y_{i}-\hat{y}_{R_{L}})^{2}+\sum_{i:x_{i}\in R_{R}(j,s)}(y_{i}-\hat{y}_{R_{R}})^{2}\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Classification Tree
\end_layout

\begin_layout Standard
Similarly, how exactly is the 
\begin_inset Formula $(j,s)$
\end_inset

 being chosen in the classification setting when using Gini index or cross-entro
py
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\min_{(j,s)}G_{L}+G_{R}\quad\text{or}\quad\min_{(j,s)}D_{L}+D_{R}\]

\end_inset


\end_layout

\begin_layout Subsubsection*
Split involving categorical predictors?
\end_layout

\begin_layout Standard
Finally, in both regression and classification trees, how do we deal with
 the split when the predictor is categorical?
\end_layout

\begin_layout Subsection*
Some questions around Cost Complexity Pruning
\end_layout

\begin_layout Standard
When performing cost complexity pruning, we do the following:
\end_layout

\begin_layout Itemize
We first use the whole training set to build a large tree 
\begin_inset Formula $T_{0}$
\end_inset

, and use cost complexity pruning to build a sequence of subtrees 
\begin_inset Formula $T_{\alpha}$
\end_inset


\end_layout

\begin_layout Itemize
We then use the 
\series bold
SAME
\series default
 training set again and perform CV to figure out the optimal 
\begin_inset Formula $\alpha^{*}$
\end_inset

.
\end_layout

\begin_layout Itemize
We then return the best tree corresponding to 
\begin_inset Formula $\alpha^{*}$
\end_inset

.
\end_layout

\begin_layout Standard
In this process, we never touch the test set.
 This is good, because we can truly evalulate the final model using an un-contam
inated data set.
 However, we use the training set for both training and parameter tuning.
 Is this a good practice? It seems like people do this all the time, but
 I thought breaking the data set to training, validation, and test set is
 a better practice (if we have eough data to do this split).
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
In other word, using the training/validation/test set split, we do the following
 instead:
\end_layout

\begin_layout Itemize
We first use 
\series bold
training
\series default
 set to build 
\begin_inset Formula $T_{0}$
\end_inset

 and the sequence of subtrees 
\begin_inset Formula $T_{\alpha}$
\end_inset

.
 Set it aside.
\end_layout

\begin_layout Itemize
We use 
\series bold
validation
\series default
 set to perform cross validation:
\begin_inset Formula \begin{eqnarray*}
\text{exclude }k & \alpha=0\dots & \alpha\\
-1 & T_{0_{err(1,0)}}\dots & T_{\alpha_{err(1,\alpha)}}\\
-2 & T_{0_{err(2,0)}}\dots & T_{\alpha_{err(2,\alpha)}}\\
\vdots & \vdots\dots & \vdots\\
-K & T_{0_{err(K,0)}}\dots & T_{\alpha_{err(K,\alpha)}}\\
\min_{\alpha} & err(0)=\sum_{k=1}^{K}err(k,0)\dots & err(\alpha)=\sum_{k=1}^{K}err(k,\alpha)\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
In CV, using the 
\begin_inset Formula $(K-1)$
\end_inset

 fold excluding 
\begin_inset Formula $k^{th}$
\end_inset

 fold to build a sequence of 
\begin_inset Formula $T_{\alpha}$
\end_inset

 using cost complexity pruning, and use the 
\begin_inset Formula $k^{th}$
\end_inset

 fold for calculating validation error for each tree 
\begin_inset Formula $T_{\alpha}$
\end_inset

, we then get 
\begin_inset Formula $err(k,\alpha)$
\end_inset

.
 Repeat the same process over 
\begin_inset Formula $k=1\dots K$
\end_inset

, we eventually will have validation error for each 
\begin_inset Formula $T_{\alpha}$
\end_inset

 in each 
\begin_inset Formula $(K-1)|K$
\end_inset

 split.
 We can then average over the 
\begin_inset Formula $(K-1)|K$
\end_inset

 combinations, for each 
\begin_inset Formula $\alpha$
\end_inset

 -> 
\begin_inset Formula $\sum_{k=1}^{K}err(k,\alpha)\:\forall\alpha$
\end_inset

.
 Finally, we will be able to compare which 
\begin_inset Formula $\alpha$
\end_inset

 gives the best model using the validation set.
\begin_inset Foot
status open

\begin_layout Plain Layout
Notice, when we visit each of the 
\begin_inset Formula $(K-1)|K$
\end_inset

 combination, the sequence of trees we build, and at each 
\begin_inset Formula $\alpha$
\end_inset

, might be different.
 But that's ok, because we are not testing whether a particular tree performs
 best, we are testing the premise of using a tree with a 
\series bold
particular size
\series default
.
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
We can then return to training set to pick out the 
\begin_inset Formula $T_{\alpha*}$
\end_inset

.
 The advantage here is that the parameter tuning process does not cross-contamin
ate with thet training error.
\end_layout

\begin_layout Itemize
Finally, once the final model is settle, we can then run through 
\series bold
test
\series default
 set to calculate the test error.
 Here, we did not use the test set for either training or model parameter
 tuning.
\end_layout

\begin_layout Standard

\color red
I think this approach is cleaner, but is it necessarily better?
\end_layout

\begin_layout Subsection*
Different ways of dividing up the whole data set for prediction problem
\end_layout

\begin_layout Standard
There seem to be many different ways of dividing up the whole data for a
 prediction problem:
\end_layout

\begin_layout Itemize

\series bold
Option 1
\series default
: Wrong approach! Use the whole data set for training, without reserving
 a test set.
 Report the 
\begin_inset Quotes eld
\end_inset

test error
\begin_inset Quotes erd
\end_inset

 using the training set.
 This is dead wrong, because the training error is too optimistic.
\end_layout

\begin_layout Itemize

\series bold
Option 2
\series default
: Validation Approach - Split it into training and test error.
 Train on the training set, do not use parameter tuning to balance bias-variance
-trade-off, calculate test error straight up from the model from training
 set.
 This approach is fine, but likely to not give us too good of a test error,
 because it doesn't adjust for bias-variance-trade-off.
 
\end_layout

\begin_layout Itemize

\series bold
Option 3
\series default
: Cross Validation Approach - Essentially the same as option 2, but we split
 the data set to 
\begin_inset Formula $K-$
\end_inset

fold instead of 
\begin_inset Formula $2-$
\end_inset

fold.
 And we also take the average of the test error.
\end_layout

\begin_layout Standard
To do any sort of bias-variance-trade-off, we usually need to optimize not
 just the RSS, but 
\begin_inset Formula $RSS+\text{\text{parameter}\times penalty}$
\end_inset

.
 In such cases, we need to not only train the model, but also use validation
 set to do parameter tuning (Regularization, Cost complexity pruning 
\begin_inset Formula $\dots$
\end_inset

 etc):
\end_layout

\begin_layout Itemize

\series bold
Option 4
\series default
: We need to train, parameter tune, and test the model, and we combine everythin
g into the CV procedure.
 We use all the data, break it into 
\begin_inset Formula $K$
\end_inset

 fold (without reserving a test set aside), and use the CV procedure to
 fit and pick optimal parameter.
 The final model performance is reported directly using CV error.
 This is better than option 1, in the sense that the reported test error
 is at least on unseen observations.
 However, reporting the test error with the best complexity parameter is
 still optimistic, because after all, it is the parameter that gives the
 best model performance (In itself is one kind form of learning).
 So it is still not ideal.
\end_layout

\begin_layout Itemize

\series bold
Option 5
\series default
: Split the data into training set and test set
\end_layout

\begin_deeper
\begin_layout Itemize

\bar under
Training set:
\bar default
 use training set to train the model as well as picking the optimal parameter.
 This is usually done via doing cross validation using only the training
 set.
 Since the model with the best parameter is usually only using 
\begin_inset Formula $(K-1)/K$
\end_inset

 of the data, and the actual models might be different across fold, the
 typical thing to do is to re-fit the model using the full 
\begin_inset Formula $K$
\end_inset

 fold training data.
\end_layout

\begin_layout Itemize

\bar under
Test set
\bar default
: We then report the test set error using the test set on that chosen model.
\end_layout

\begin_layout Standard
This is an improvement from 4, where we have valid test error (in the sense
 of calculating on unseen data), and that test error is not cherry pick
 with respect to the best parameter.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Option 6
\series default
: Split the data into training, validation, and test set
\end_layout

\begin_deeper
\begin_layout Itemize

\bar under
Training set
\bar default
: This is used solely for building a model, indexed by the complexity parameter,
 which we will later pass down to the test set
\end_layout

\begin_layout Itemize

\bar under
Validation set
\bar default
: This is solely used for picking the right model complexity (parameter
 tuning), using using CV
\end_layout

\begin_layout Itemize
Once we have the optimal parameter, we find the model built from training
 set correspond to that parameter
\end_layout

\begin_layout Itemize

\bar under
Test set: 
\bar default
Finally, we test the model from training set with optimal parameter to get
 the test error
\end_layout

\begin_layout Standard
The advantage here is that we have a valid test error (test on unseen data),
 we have separate out the training and validation process, which is cleaner.
 
\color red
I am not sure if this is a more objective approach compared to option 5
 though.
\end_layout

\end_deeper
\begin_layout Standard
It seems like option 4 is analogous to option 1, and option 5,6 are analogous
 to option 2, we never really use cross validation approach again, 
\color red
is there an analogy to option 3 when we need to tune parameters? Can we
 use nested cross validation approach here?
\color inherit
 I think so, the idea is instead of (training, validation), test set (fixed),
 we create different folds of (training, validation), test set, and average
 over the test set.
 So what we really have is
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\{(\text{\ensuremath{\{}training+validation set\ensuremath{\}_{j=1}^{L}}), test set}\}_{i=1}^{K}\]

\end_inset

or
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\{(\text{(training set, \ensuremath{\{}validation set\ensuremath{\}_{j=1}^{L}}), test set}\}_{i=1}^{K}\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard

\color red
(?) I think we can, and it gives us a more objective way to seeing the uncertain
ty of the test error! I think Trevor Hastie might have talk about this,
 but I never seen it covered in a textbook so far!
\end_layout

\begin_layout Subsection*
Questions arise from Variable Importance Measures
\end_layout

\begin_layout Standard
The prescription for calculating variable importance is to look at the average
 decrease in RSS/Gini index due to splits responsible from a particular
 variables.
 It seems like we are only looking at the training set error? Why are we
 not looking at the test error? I remember vaguely that sometimes in linear
 models, we can remove/add predictors, and use some ANOVA technique to see
 if the increase/decrease in RSS is worth adding the additional varaible?
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
But it seems like they are all looking at the change in training error,
 instead of test error (or validation error to be more correct).
 Why is that? I thought ultimately we are going after prediction accuracy.
\end_layout

\begin_layout Subsection*
Questions for Boosting Algorithm
\end_layout

\begin_layout Standard
How do we do boosting for classification problem? Perhaps ESL would have
 more answers there?
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Chapter 9: Support Vector Machine
\end_layout

\begin_layout Standard
Support Vector Machine is an approach for classification that was developed
 in the computer science community in the 1990s.
 The approach is not statistical, it simply tries to find a plane that separates
 the classes in feature space.
 Despite its lack of statistical rooting, it has grown in its popularity
 and is one of the most competitive methods nowadays.
 We will discuss SVM in three layers of progression:
\end_layout

\begin_layout Itemize

\shape italic
Maximal Margin Classifier
\shape default
: It simply tries to find a separating hyperplane that separate the data
 in the feature space when the data is linearly separable.
\end_layout

\begin_layout Itemize

\shape italic
Support Vector Classifier
\shape default
: an extension of the maximal margin classifier that can be applied in a
 borader range of cases, i.g.
 when the data is noisy or not linearly separable (Softening).
\end_layout

\begin_layout Itemize

\shape italic
Support Vector Machine
\shape default
: A further extension of the support vector classifier in order to accommodate
 non-linear class boundaries (Kernel methods).
\end_layout

\begin_layout Standard
SVM is typically used for binary classification problem, but it can be extended
 to multi-class settings as well.
 Finally, we will draw the connections of SVM with other statistical methods
 we have seen so far.
\end_layout

\begin_layout Subsection*
Maximal Margin Classifier
\end_layout

\begin_layout Standard
The intuition of constructing a maximal margin classifier is simple, we
 want to find a hyperplane that is able to separate the two classes of data,
 and make the separation as large as possible.
 Let's introduce some mathematical constructs to characterize the maximal
 margin classifier problem.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Mathematically, a 
\begin_inset Formula $p$
\end_inset

 dimensional hyperplane is defined as:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\dots+\beta_{p}X_{p}=0\]

\end_inset


\end_layout

\begin_layout Standard
A given 
\begin_inset Formula $\left(X_{1}\dots X_{p}\right)$
\end_inset

 that satisfied the condition above lies on the hyperplane.
 
\begin_inset Formula $\left(\beta_{1}\dots\beta_{p}\right)$
\end_inset

 is the normal vector that is perpendicular to the surface of the hyperplane.
 For an 
\begin_inset Formula $\mathbf{X}$
\end_inset

 that does not lie on the plane, then either 
\begin_inset Formula $\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\dots+\beta_{p}X_{p}>0$
\end_inset

 or 
\begin_inset Formula $\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\dots+\beta_{p}X_{p}<0$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Each of the observations fall into one of the two classes
\begin_inset Formula $\in\left\{ 1,-1\right\} $
\end_inset

, and we label their classes as 
\begin_inset Formula $y_{i}$
\end_inset

's.
 A separating hyperplane has the property that
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\dots+\beta_{p}X_{p} & >0 & \text{if \ensuremath{y_{i}=1}}\\
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\dots+\beta_{p}X_{p} & <0 & \text{if \ensuremath{y_{1}=-1}}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Equivalently, a separating hyperplane has the property that for 
\begin_inset Formula $\left\{ x_{i},y_{i}\right\} $
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \[
y_{i}\left(\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\dots+\beta_{p}X_{p}\right)>0\:\forall i=1\dots n\]

\end_inset


\end_layout

\begin_layout Standard
If a separating hyperplane exists, we can use it to construct a very natural
 classifier: a test observation is assigned a class depending on which side
 of the hyperplane it is located.
 That is, we classify the test observation 
\begin_inset Formula $x^{*}$
\end_inset

 based on the sign of 
\begin_inset Formula $f\left(x^{*}\right)=\beta_{0}+\beta_{1}x_{1}^{*}+\dots\beta_{p}x_{p}^{*}$
\end_inset

.
 If 
\begin_inset Formula $f\left(x^{*}\right)$
\end_inset

 is positive, then we assign the test observation to class 
\begin_inset Formula $1$
\end_inset

, and if 
\begin_inset Formula $f\left(x^{*}\right)$
\end_inset

 is negative, then we assign the test observation to class 
\begin_inset Formula $-1$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
In general, if our data can be perfectly separated using a hyperplane, then
 there will in fact exist an infinite number of such hyperplanes, so how
 do we pick one amongst these infinitely many choices? A natural choice
 is the maximal margin hyperplane, which is the separating hyperplane that
 is arthest from the training observations.
 More precisely, we can compute the perpendicular distance of eaching training
 point to the hyperplane, the maximal margin classifier aims to maximize
 the margin of the training point that is closest to the hyperplane (because
 then all the other points from the two classes would be far apart too).
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
The intuition here is simple, the farther a point is from the margin, the
 more 
\begin_inset Quotes eld
\end_inset

confident
\begin_inset Quotes erd
\end_inset

 we are that it is not at the borderline (less ambiguity in determining
 its class).
 if we can find a classifier where the two classes are separated well, then
 we are likely to do a good job for a new test observations.
 The mathematical definition is as follows:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/maximal_margin_classifier.png
	scale 60
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
This formulation formalizes our characterization of the maximal margin classifie
r.
 The constraints ensure that each observation is not only on the correct
 side of the hyperplane, but they are also at least a distance 
\begin_inset Formula $M$
\end_inset

 from the hyperplane.
 Our goal is to maximize 
\begin_inset Formula $M$
\end_inset

 to make the data sa linearly separable as possible.
\end_layout

\begin_layout Subsection*
Support Vector Classifier (AKA Soft Margin Classifier)
\end_layout

\begin_layout Standard
In reality, not all data sets are linearly separable, in such cases, we
 will not be able to find a maximal margin classifier, so what do we do
 then? This is where a support vector classifier (a relaxed/softened version
 of the maximal margin classifier) can be extremely valuable.
 In fact, there are scenarios where even if the data is linearly separable,
 we might choose to use SVC instead of MMC.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Before we dive into the mathematical details of Support Vector Classifier,
 it's important to note (again) the cases where this tool can be useful,
 and why it is appealing:
\end_layout

\begin_layout Subsubsection*
Use cases
\end_layout

\begin_layout Itemize
The data is linearly separable, but noisy -- This is the case where we have
 a data point that is near the decision boundary.
 The existence of such point might greatly change the orientation of the
 hyperplane, and might make the margin very small.
 This is generally a bad sign, because we are trying to fit the data too
 hard and are less confident to classify future observation.
\end_layout

\begin_layout Itemize
The data is just outright not linearly separable.
\end_layout

\begin_layout Subsubsection*
Why it is appealing
\end_layout

\begin_layout Standard
The maximal margin classifier necessarily perfectly classify all the data
 points.
 Support vector classifier is willing to consider hyperplane that does not
 do a perfect job, in exchange for
\end_layout

\begin_layout Itemize
Greater robustness for individual observations, and 
\end_layout

\begin_layout Itemize
Better classification of most of the training, but also test observations
\end_layout

\begin_layout Standard
Essentially, we are trading off between bias and variance.
 A maximal margin classifier is dogmatic in fitting the particular data
 set perfectly (low bias, but high variance).
 The support vector classifier is willing to sacrafice bias for lower variance.
\end_layout

\begin_layout Subsubsection*
Mathematical Details of Support Vector Classifier
\end_layout

\begin_layout Standard
The support vector classifier classifies a test observation depending on
 which side of a hyperplane it lies.
 The hyperplane is chosen to correctly separate most of the training observation
s into the two classes, but may misclassify a few observations.
 It is to the solution to the optimization problem:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/support_vector_classifier.png
	scale 60
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The setting is very similar as in the case of maximal margin classifier.
 The additional tweak is that we are introducing 
\begin_inset Formula $\epsilon_{1}\dots\epsilon_{n}$
\end_inset

, slack variables for each of the observations, which allow individual observati
on to be on the wrong side of the margin OR wrong side of the hyperplane.
 In particular, the slack variables 
\begin_inset Formula $\epsilon_{i}$
\end_inset

 tells us where the 
\begin_inset Formula $i^{th}$
\end_inset

 observation is located, relative to the hyperplane and margin
\begin_inset Foot
status open

\begin_layout Plain Layout

\color red
Is it using some kind of complementary slackness argument?
\end_layout

\end_inset

:
\end_layout

\begin_layout Itemize

\color red
If 
\begin_inset Formula $\epsilon_{i}=0$
\end_inset

, then the observation is on the correct side of the margin
\end_layout

\begin_layout Itemize

\color red
If 
\begin_inset Formula $0<\epsilon_{i}<1$
\end_inset

, then the observation is on the wrong side of the margin, but correct side
 of hyperplane
\end_layout

\begin_layout Itemize

\color red
If 
\begin_inset Formula $\epsilon_{i}>1$
\end_inset

, then the observation is on the wrong side of margin AND wrong side of
 the hyperplane
\end_layout

\begin_layout Standard
We now consider the role of 
\begin_inset Formula $C$
\end_inset

.
 
\begin_inset Formula $C$
\end_inset

 bounds the number of non-zero 
\begin_inset Formula $\epsilon_{i}$
\end_inset

's, and can be think of as a budget that represent how much we are willing
 tolerate violations to the margin (and to the hyperplane):
\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $C=0$
\end_inset

, there is no budget for violations, so 
\begin_inset Formula $\epsilon_{i}=0\:\forall\: i$
\end_inset

.
 Every observation needs to be in the right side of the margin (of course,
 this is only possible if the data is linearly separable)
\end_layout

\begin_layout Itemize
Now, for 
\begin_inset Formula $C>0$
\end_inset

, this means that we cannot have more than 
\begin_inset Formula $C$
\end_inset

 observations that fall in the wrong side of the hyperplane, this is because
 (from above) we have seen that observations on the wrong side of the hyperplane
 has 
\begin_inset Formula $\epsilon_{i}>1$
\end_inset

.
 
\end_layout

\begin_layout Standard
Onto bullet point 2, this means that when 
\begin_inset Formula $C$
\end_inset

 is large, we have more budget/luxury to engaged in violations.
 However, when 
\begin_inset Formula $C$
\end_inset

 is small, we need to be more frugal about our violations.
 As we increase 
\begin_inset Formula $C$
\end_inset

, we are more tolerant, more points would violate the constraints, and the
 margin will be widen (because we have the budget and can afford violations
 so to maximize 
\begin_inset Formula $M$
\end_inset

).
 Conversely, when 
\begin_inset Formula $C$
\end_inset

 is small, we cannot afford to violate constraints so we cannot expand 
\begin_inset Formula $M$
\end_inset

 as much! In practice, 
\begin_inset Formula $C$
\end_inset

 is a tuning parameter, and we typically use cross-validation to figure
 out the optimal 
\begin_inset Formula $C$
\end_inset

:
\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $C$
\end_inset

 is small, we cannot violate too much, so we have to stick to the actual
 orientation of the data, so we will be fitting the data hard.
 This means low bias, but potentially high variance.
\end_layout

\begin_layout Itemize
When 
\begin_inset Formula $C$
\end_inset

 is large, we are more care-free, because we have the budget to violate
 the constraints.
 In this case, we are not fitting the data as hard.
 This means higher bias, but in exchange, the procedure will have lower
 variance.
\end_layout

\begin_layout Standard
Finally, one interesting property of the soft margin classifier is that
 only a few of the 
\begin_inset Quotes eld
\end_inset

support vectors
\begin_inset Quotes erd
\end_inset

 training examples are responsible in determining the orientation of the
 hyperplane.
 This means that for those observations that are far away from the hyperplane,
 so long that they lie on the right side of the margin, their position will
 not affect the 
\begin_inset Quotes eld
\end_inset

fit
\begin_inset Quotes erd
\end_inset

! This characteristic is in contrast to methods like LDA, where all the
 observations are being used to determine the fit.
\end_layout

\begin_layout Subsection*
Support Vector Machine
\end_layout

\begin_layout Standard
Sometimes a linear boundary simply won't work, no matter how generous we
 give the budget to 
\begin_inset Formula $C$
\end_inset

.
 This is often the case when the data have non-linear structures.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/svm_non_linear_boundary.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In these scenarios, the support vector machine (an extension of the support
 vector classifier) is particularly useful, and address the non-linearity
 issue by enlarging the feature space (often using kernel tricks).
 The idea is simple:
\end_layout

\begin_layout Itemize
We realized the data cannot be separated in the original space with hyperplane
 easily (even with soft margin)
\end_layout

\begin_layout Itemize
Expand the feature space
\begin_inset Foot
status open

\begin_layout Plain Layout
Using polynomail is one way, this is very similar to the spirit of polynomial
 regression in Chapter 7.
 But as we will see, polynomial are generaly not stable and computational
 burdensome
\end_layout

\end_inset

 so that training examples can be re-aligned and become linearly separable
 (or nearly) in the new space
\end_layout

\begin_layout Itemize
Apply MMC or SVC in the new space
\end_layout

\begin_layout Itemize
Transform back to the original space to get non-linear decision boundaries
\end_layout

\begin_layout Subsubsection*
Why Kernel tricks?
\end_layout

\begin_layout Standard
The kernel approach that we describe here is simply an efficient computational
 approach for enacting the idea abvoe.

\color blue
 We have not discussed exactly how the support vector classifier is computed
 (but remember to check out Andrew Ng's CS 229 lecture notes on SVM, where
 he carefully go through the formulation), 
\series bold
\color inherit
but it turns out that the solution to this problem only involve the inner
 products of the observations, and not the observations themselves
\series default
.
 It can be shown that:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
f\left(x\right) & = & \beta_{0}+\sum_{i=1}^{n}\alpha_{i}\langle x,x_{i}\rangle=\beta_{0}+\sum_{i\in\mathcal{S}}\alpha_{i}\langle x,x_{i}\rangle\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where there are 
\begin_inset Formula $n$
\end_inset

 parameters 
\begin_inset Formula $\alpha_{i}$
\end_inset

, 
\begin_inset Formula $i=1\dots n$
\end_inset

, one per training observation.
 To estimate the parameters 
\begin_inset Formula $\alpha_{i}$
\end_inset

's and 
\begin_inset Formula $\beta_{0}$
\end_inset

, all we need are the 
\begin_inset Formula $\binom{n}{2}$
\end_inset

 inner products 
\begin_inset Formula $\langle x_{i},x_{i'}\rangle$
\end_inset

 between all pairs of training observations.
 To make a classification of a new point, we need to calculate 
\begin_inset Formula $\langle x_{new},x_{i}\rangle$
\end_inset

.
 However, it turns out that 
\begin_inset Formula $\alpha_{i}$
\end_inset

 is nonzero only for support vectors in the solution, so in making classificatio
n prediction, we only need the inner products from 
\begin_inset Formula $x$
\end_inset

 and observations in 
\begin_inset Formula $\mathcal{S}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
I cannot emphasize enough about this observation -- 
\series bold
the solution to this problem only involves the inner products of the observation
s, not the observations themselves
\series default
.
 In fitting a SVM, we only need the inner products of the observations.
 Imagine we are trying to expand the feature space to tackle the non-linearty
 issue
\begin_inset Foot
status open

\begin_layout Plain Layout
Like (9.16) in page 350
\end_layout

\end_inset

, as we are about to enlarge the feature space, it might be tempting to
 write down the transformation for each of the observations, but this is
 not necessary! Fitting only invovles the inner products of these enlarge
 features.
 If we know the inner products of the new features, we are set to fit!
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
This means the transformation itself can be quite abstract, so long the
 (new space) inner products are simple to express mathematically.
 I.e., it doesn't matter if we can explicitly write down the space you have
 transformed the original variables to, if we can calculate the inner product
 of pairs of observations in that new space, we can fit a SVM! This is why
 Kernel 
\begin_inset Formula $K\left(x_{i},x_{i'}\right)$
\end_inset

 fits so well in the context.
 Once we used the trick, we can always project the decision boundary in
 the enlarged space back to the original space.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
The typical popular choices for Kernels are:
\end_layout

\begin_layout Itemize

\series bold
Linear Kernel
\series default
: 
\begin_inset Formula $K\left(x_{i},x_{i'}\right)=\sum_{j=1}^{p}x_{ij}x_{i'j}$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Polynomial Kernel of degree 
\begin_inset Formula $d$
\end_inset


\series default
: 
\begin_inset Formula $K(x_{i},x_{i'})=\left(1+\sum_{j=1}^{p}x_{ij}x_{i'j}\right)^{d}$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Radial Basis Kernel
\series default
: 
\begin_inset Formula $K\left(x_{i},x_{i'}\right)=exp\left(-\gamma\sum_{j=1}^{p}\left(x_{ij}-x_{i'j}\right)^{2}\right)$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
When 
\begin_inset Formula $x_{i}$
\end_inset

 is far from 
\begin_inset Formula $x^{*}$
\end_inset

, then 
\begin_inset Formula $\sum_{j=1}^{p}\left(x_{ij}-x_{i'j}\right)^{2}$
\end_inset

 is large, and 
\begin_inset Formula $K(x,x_{i})$
\end_inset

 would be small.
 This means that 
\begin_inset Formula $x_{i}$
\end_inset

 will virtually play no role in determining the label of 
\begin_inset Formula $x^{*}$
\end_inset

.
 This means RBK has very local behaviors.
\end_layout

\begin_layout Itemize
The tuning parameter 
\begin_inset Formula $\gamma$
\end_inset

 also controls localness.
 For large/small 
\begin_inset Formula $\gamma$
\end_inset

, 
\begin_inset Formula $K\left(\cdot\right)$
\end_inset

 will be small/large, so the behavior would be more/less local.
 
\end_layout

\begin_layout Itemize
In running a SVM with radial basis kernel, we need to tune both 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $\gamma$
\end_inset

 to balance bias-variance-tradeoff.
\end_layout

\end_deeper
\begin_layout Standard
In particular, the space provided by RBK is infinitely dimensional, using
 this kernel allows us to expand the feature to a infinite dimensional space
 (to perform SVM) without worrying what each individual observations would
 look like in that space.
 With the kernel trick, the non-linear function form of SVM classifier becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
f\left(x\right) & = & \beta_{0}+\sum_{i\in\mathcal{S}}\alpha_{i}K\left(x,x_{i}\right)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $K\left(x,x_{i}\right)$
\end_inset

 is the kernel of choice.
 A very powerful tool indeed! Attached is the result of decision boundary
 (after projection back to the original space) using the kernel trick using
 polynomial of degree 
\begin_inset Formula $3$
\end_inset

 and radial basis kernel respectively.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/kernel_trick_decision_boundary.png
	scale 40
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Multi-Class SVM
\end_layout

\begin_layout Standard
So far, our discussion has been limited to the case of binary classification.
 But we can easily extend the problem to multi-class classification.
 The popular approaches are one-versus-one (OVO) and one-versus-all (OVA):
\end_layout

\begin_layout Itemize

\series bold
OVA
\series default
: One versus All.
 Fit 
\begin_inset Formula $K$
\end_inset

 different 
\begin_inset Formula $2$
\end_inset

-class SVM classifier 
\begin_inset Formula $\hat{f}_{k}\left(x\right)$
\end_inset

, 
\begin_inset Formula $k=1,\dots K$
\end_inset

; each class versus the rest.
 Classify 
\begin_inset Formula $x^{*}$
\end_inset

 to the class for which 
\begin_inset Formula $\hat{f}_{k}(x^{*})$
\end_inset

 is largest
\end_layout

\begin_layout Itemize

\series bold
OVO
\series default
: One versus One.
 Fit all 
\begin_inset Formula $\binom{K}{2}$
\end_inset

 pairwise classifiers 
\begin_inset Formula $\hat{f}_{kl}\left(x\right)$
\end_inset

.
 Classify 
\begin_inset Formula $x^{*}$
\end_inset

 to the class that wins the most pairwise competitions.
\end_layout

\begin_layout Standard
Which one to choose? If 
\begin_inset Formula $K$
\end_inset

 is not too large, use OVO.
\end_layout

\begin_layout Subsection*
Relationship to Logistic Regression
\end_layout

\begin_layout Standard
When SVMs were first introduced in the mid-1990s, they made quite a splash
 in the statistical and machine learning communitites.
 This was due in part to their good performance, good marketing, and also
 to the fact that the underlying approach seemed both novel and mysterious.
 The approach seemed so distinct from classical approaches such as logistic
 regression and linear discriminant analysis.
 Moreover, the idea of using kernel to expand the feature space in order
 to accommodate non-linear class boundaries appeared to be a unique and
 valuable characteristics.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
However, since that time, deep connections between SVMs and other more classical
 statistical methods have emerged.
 It turns out that one can rewrite the optimization for fitting the support
 vector classifier 
\begin_inset Formula $f(X)=\beta_{0}+\sum_{j=1}^{p}\beta_{j}X_{j}$
\end_inset

 as
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\min_{\beta_{0},\beta_{1}\dots\beta_{p}}\left\{ \sum_{i=1}^{n}\max\left[0,\:1-y_{i}f(x_{i})\right]+\lambda\sum_{j=1}^{p}\beta_{j}^{2}\right\} \]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\lambda$
\end_inset

 is the tuning parameter that controls the bias-variance-trade-off.
 It turns out that 
\begin_inset Formula $\lambda$
\end_inset

 small corresponds to 
\begin_inset Formula $C$
\end_inset

 small (high variance, low bias) and 
\begin_inset Formula $\lambda$
\end_inset

 big corresponds to 
\begin_inset Formula $C$
\end_inset

 big (low variance, high bias).
 When the support vector classifier and SVM were first introduced, it was
 thought that the tuning parameter 
\begin_inset Formula $C$
\end_inset

 was an unimportant 
\begin_inset Quotes eld
\end_inset

nuisance
\begin_inset Quotes erd
\end_inset

 parameter that could be set to some default value, like 1.
 However, the loss + penalty formulation for the support vector classifier
 indicates that this is not the case.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Finally, it is important to note that the particular loss formulation (hinge
 loss)
\end_layout

\begin_layout Standard
\begin_inset Formula \[
L\left(\mathbf{X},\mathbf{y},\beta\right)=\sum_{i=1}^{n}\max\left[0,\:1-y_{i}\left(\beta_{0}+\beta_{1}x_{i1}+\dots+\beta_{p}x_{ip}\right)\right]\]

\end_inset


\end_layout

\begin_layout Standard
has an interesting characteristic that only support vectors play a role
 in the classifier obtained; observations on the correct side of the margin
 do not affect it.
 This is due to the fact that the loss function is exactly zero for observations
 for which 
\begin_inset Formula $y_{i}\left(\beta_{0}+\beta_{1}x_{i1}+\dots+\beta_{p}x_{ip}\right)\geq1$
\end_inset

; these correspond to observations that are on the correct side of the margin.
 In contrast, the loss function for logistic regression is not exactly zero
 anywhere.
 But it is very small for observations that are far from the decision boundary.
 Due to the similarities between their loss functions, logistic regerssion
 (with regularization) and SVM often given very similar results.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Below is an illustration comparing the loss function of logistic regression
 and SVM:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename graphs/svm_lr_loss_function_comparison.png
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Subsection*
Lingering Questions:
\end_layout

\begin_layout Itemize
For Support Vector Classifier (not SVM formulation), the observations was
 made in the textbook.
 I am still not exaclty sure why:
\end_layout

\begin_deeper
\begin_layout Itemize

\color red
If 
\begin_inset Formula $\epsilon_{i}=0$
\end_inset

, then the observation is on the correct side of the margin
\end_layout

\begin_layout Itemize

\color red
If 
\begin_inset Formula $0<\epsilon_{i}<1$
\end_inset

, then the observation is on the wrong side of the margin, but correct side
 of hyperplane
\end_layout

\begin_layout Itemize

\color red
If 
\begin_inset Formula $\epsilon_{i}>1$
\end_inset

, then the observation is on the wrong side of margin AND wrong side of
 the hyperplane
\end_layout

\end_deeper
\begin_layout Itemize
What exactly is the detailed formulation of the SVM optimization, how is
 it solved? 
\end_layout

\begin_deeper
\begin_layout Itemize
CS 229 Lecture notes provide a much more detailed explanation
\end_layout

\end_deeper
\begin_layout Itemize
For AUC, what exactly are 
\color red
Mann-Whitney U test 
\color inherit
and 
\color red
Wilcoxon test of ranks? What's the connections?
\end_layout

\begin_layout Itemize
In the textbook, the authors mentioned Support Vector Regression.
 Is it commonly used, how popular is it, and what is the details behind
 the algorithm.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section*
Chapter 10: Unsupervised Learning
\end_layout

\begin_layout Standard
Most of this book concerns about 
\shape italic
supervised learning
\shape default
 methods such as regression and classification algorithm.
 In the supervised learning setting, we typically have access to a set of
 
\begin_inset Formula $p$
\end_inset

 features 
\begin_inset Formula $X_{1},X_{2},\dots X_{p}$
\end_inset

, measured on 
\begin_inset Formula $n$
\end_inset

 observations, and a response 
\begin_inset Formula $Y$
\end_inset

 also measured on the 
\begin_inset Formula $n$
\end_inset

 observations.
 The goal is usually to make prediction in 
\begin_inset Formula $Y$
\end_inset

.
 In the context of unsupervised learning, we also have access to a set of
 
\begin_inset Formula $p$
\end_inset

 features 
\begin_inset Formula $X$
\end_inset

's, but there is not a corresponding 
\begin_inset Formula $Y$
\end_inset

.
 In this context, the goal is to discover interesting things/summarization
 about the measurements on 
\begin_inset Formula $X$
\end_inset

's such as:
\end_layout

\begin_layout Itemize
Data Visualization & Exploratory Data Analysis
\end_layout

\begin_layout Itemize
Finding interesting grouping among subgroups
\end_layout

\begin_layout Standard
In particular, we will discuss two particular types of unsupervised learning
 methods:
\end_layout

\begin_layout Itemize

\series bold
Principal Component Analysis (PCA)
\series default
: a tool used for data visualization or data pre-processing before supervised
 techniques are applied (like PCR)
\end_layout

\begin_layout Itemize

\series bold
Clustering
\series default
: a broad class of methods for discovering unknown subgroups in data
\end_layout

\begin_layout Standard
Unsupervised learning are often more subjective, since there is no objective
 'labels' that we can used to supervise how well we are predicting the data.
 However, unsupervised learning has become increasingly important because
 it is much easier to collect unlabled datat than labeled data.
\end_layout

\begin_layout Subsection*
Principal Component Analysis (PCA)
\begin_inset Foot
status open

\begin_layout Plain Layout
PCA can often be used as a pre-processing step before a supervised learning
 method is applied as we have discussed in the context of PCR).
 We will focus on the unsupervised use cases of PCA, in keeping with the
 topic of the chapter.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Motivation
\series default
: Suppose we wish to visualize 
\begin_inset Formula $n$
\end_inset

 observations with measurements on a set of 
\begin_inset Formula $p$
\end_inset

 features, as part of an exploratory analysis.
 We could do this by examining two-dimensional scatterplots of the data,
 but there are 
\begin_inset Formula $\binom{p}{2}$
\end_inset

 such plot.
 Moreover, most likely none of them will be informative since they each
 contain just a small fraction of the total information present in the data.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
PCA is a technique that allows us to find a low-dimensional representation
 of a data set that contains as much as possible of the variation.
 The idea is that each of the 
\begin_inset Formula $n$
\end_inset

 observations live in 
\begin_inset Formula $p$
\end_inset

-dimensional space, but not all of these dimensions are equally interesting.
 PCA seeks a smaller number of dimensions that are as interesting as possible,
 where the concept of 
\shape italic
interesting
\shape default
 is measured by the amount that the observations vary along each dimension.
 
\end_layout

\begin_layout Subsubsection*
Principal Component Analysis: details
\end_layout

\begin_layout Standard

\color blue
PCA involves finding a sequence of linear combinations of the original variables
, called principal components, that have maximal variance, and are mutually
 uncorrelated.
 What does this mean?
\color inherit
 
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
To formalize the notation, define the 
\shape italic
first principal component
\begin_inset Foot
status open

\begin_layout Plain Layout
subsequent PCs can be defined similarly
\end_layout

\end_inset


\shape default
 of a set of features 
\begin_inset Formula $X_{1},X_{2}\dots X_{p}$
\end_inset

 as the normalized linear combination of the features
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
Z_{1} & = & \phi_{11}X_{1}+\phi_{21}X_{2}+\dots+\phi_{p1}X_{p}\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
that has the largest variance.
 By normalized, we mean that 
\begin_inset Formula $\Vert\phi_{1}\Vert=\sum_{j=1}^{p}\phi_{j1}^{2}=1$
\end_inset

.
 We refer to the elements 
\begin_inset Formula $\phi_{11},\dots,\phi_{p1}$
\end_inset

 as the loadings of the first principal component.
 Together, the loadings make up the principal component loading vector,
 
\begin_inset Formula $\phi_{1}=\left(\phi_{11},\phi_{21},\dots\phi_{p1}\right)^{T}$
\end_inset

.
 We constrain the loadings so that the norm is 1, since otherwise setting
 these elements to be arbitrarily large in absolute value could result in
 an arbitrarily large variance.
 But what does it mean that 
\begin_inset Formula $Z_{1}$
\end_inset

 has the largest variance? We need some geometric intuition.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
In Linear Algebra terms, 
\begin_inset Formula $\phi_{1}$
\end_inset

 is simply a direction which we can project each record of the data matrix
 
\begin_inset Formula $X$
\end_inset

 onto; 
\begin_inset Formula $Z_{1}$
\end_inset

 is the projection of 
\begin_inset Formula $X$
\end_inset

 onto 
\begin_inset Formula $\phi_{1}$
\end_inset

, i.e.
 Proj
\begin_inset Formula $_{\phi_{1}}\left(X\right)=Z_{1}$
\end_inset

, or
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
Z_{1}=\left[\begin{array}{c}
z_{11}\\
z_{21}\\
\vdots\\
z_{n1}\end{array}\right] & = & \left[\begin{array}{c}
Proj_{\phi_{1}}\left(x^{(1)}\right)\\
Proj_{\phi_{1}}\left(x^{(2)}\right)\\
\vdots\\
Proj_{\phi_{1}}\left(x^{(n)}\right)\end{array}\right]=\left[\begin{array}{c}
\frac{\phi_{1}^{T}x^{(1)}}{\phi_{1}^{T}\phi_{1}}\\
\frac{\phi_{1}^{T}x^{(2)}}{\phi_{1}^{T}\phi_{1}}\\
\vdots\\
\frac{\phi_{1}^{T}x^{(n)}}{\phi_{1}^{T}\phi_{1}}\end{array}\right]=\left[\begin{array}{c}
\phi_{1}^{T}x^{(1)}\\
\phi_{1}^{T}x^{(2)}\\
\vdots\\
\phi_{1}^{T}x^{(n)}\end{array}\right]=Proj_{\phi_{1}}\left(X\right)\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/rc8138/Desktop/pca_projection.png
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
We refer to 
\begin_inset Formula $z_{i1}$
\end_inset

's as the 
\shape italic
scores
\shape default
 of the first principal component.
 It happens so that each element in 
\begin_inset Formula $Z_{1}$
\end_inset

 is in fact the distance from the projection to the 'origin' along that
 direction, so maximizing 
\begin_inset Formula $Z_{1}$
\end_inset

 is equivalent to finding projections that are as 'spread out' as possible
 in that projection direction.
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
With this geometric intuition in mind, we have a clearer picture what PCA
 does -- Once we have identified 
\begin_inset Formula $\phi_{1}$
\end_inset

, we will find the second principal component 
\begin_inset Formula $Z_{2}$
\end_inset

 that has the largest variance, but is uncorrelated to the first PC.
 We will then proceed with finding the thrid principal component 
\begin_inset Formula $Z_{3}$
\end_inset

 that has the largest variance, but is uncorrelated with 
\begin_inset Formula $PC_{1}$
\end_inset

 and 
\begin_inset Formula $PC_{2}$
\end_inset

.
 Subsequently, we find the 
\begin_inset Formula $k^{th}$
\end_inset

 PC that maximize the variance along the direction of 
\begin_inset Formula $\phi_{k}$
\end_inset

, but is uncorrelated with 
\begin_inset Formula $PC_{i},\: i=1,\dots k-1$
\end_inset

, for all remaining 
\begin_inset Formula $k$
\end_inset

.
 By doing so, we ensure that the direction in which the algorithm is variance
 maximizing in subsequent directions are uncorrelated, which leads to them
 being orthogonal.
 The orthogonality allows us to explore remaining information in the data
 that is not yet explored.
 Once we have gone through the exercise for all 
\begin_inset Formula $p$
\end_inset

 dimensions, we now very a new view of the data set, using a very special
 set of 'basis' directions!
\end_layout

\begin_layout Standard
\begin_inset Formula $\:$
\end_inset


\end_layout

\begin_layout Standard
Now that we have an idea of what the algorithm is trying to achieve, given
 a 
\begin_inset Formula $n\times p$
\end_inset

 data set 
\begin_inset Formula $\mathbf{X}$
\end_inset

, how do we compute the (first) principal component? Typically, we assume
 that each of the variables in 
\begin_inset Formula $\mathbf{X}$
\end_inset

 has been centered to have mean zero (since we are only interested in the
 variance).

\color red
 The above problem
\begin_inset Foot
status open

\begin_layout Plain Layout
Optimization problem for subsequent PCs can be defined similarly, except
 we require the new PC to be orthogonal to the previous ones
\end_layout

\end_inset

 turns out can be solved using singular value decomposition (SVD).
 In fact, it can be shown that the PC directions 
\begin_inset Formula $\phi_{1},\phi_{2},\dots\phi_{p}$
\end_inset

 are the ordered sequence of right singular vectors of the matrix 
\begin_inset Formula $\mathbf{X}$
\end_inset

, and the variances of the component are 
\begin_inset Formula $\frac{1}{n}$
\end_inset

 times the squares of the singular values.
\begin_inset Foot
status open

\begin_layout Plain Layout

\color red
Need to fill in the details either from my own notes previously, or there
 is a good document from DataTau
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Another Interpretation of the principal components
\end_layout

\begin_layout Standard
In the previous section, we describe the principal component loading vectors
 as the directions in feature space along which the data vary the most,
 and the principal component scores as projections along these directions.
 However, an alternative interpretation for principal components can also
 be useful: principal components provide low-dimensional linear surfaces
 that are closest to the observations.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/rc8138/Desktop/pca_interpretation_2.png
	scale 50
	rotateOrigin center

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Using this interpretation, together the first 
\begin_inset Formula $M$
\end_inset

 principal component score vectors and the first 
\begin_inset Formula $M$
\end_inset

 principal component loadings vectors provide best 
\begin_inset Formula $M$
\end_inset

-dimensional approximation to the 
\begin_inset Formula $i^{th}$
\end_inset

 observation 
\begin_inset Formula $x_{ij}.$
\end_inset

 This representation can be written as
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{eqnarray*}
x_{ij} & \approx & \sum_{m=1}^{M}z_{im}\phi_{jm}\end{eqnarray*}

\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout

\color red
How does this related to the SVD decomposition, why are we missing the singular
 values, or is it already incorporated into 
\begin_inset Formula $\phi$
\end_inset

's?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In other words, together the 
\begin_inset Formula $M$
\end_inset

 principal component score vectors and laoding vectors can give a good approxima
tion to the data when 
\begin_inset Formula $M$
\end_inset

 is sufficiently large.
 When 
\begin_inset Formula $M=min\left(n-1,p\right)$
\end_inset

, then the representation is exactl: 
\begin_inset Formula $x_{ij}=\sum_{m=1}^{M}z_{im}\phi_{jm}$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
Example
\end_layout

\begin_layout Standard

\end_layout

\end_body
\end_document
